# MarketLens é¡¹ç›®å‡½æ•°è¯´æ˜æ–‡æ¡£

## æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ MarketLens é¡¹ç›®çš„å„ä¸ªåŠŸèƒ½æ¨¡å—å’Œæ ¸å¿ƒå‡½æ•°ï¼ŒåŒ…æ‹¬å‚æ•°è¯´æ˜ã€è¿”å›å€¼ã€å¼‚å¸¸å¤„ç†å’Œä½¿ç”¨ç¤ºä¾‹ç­‰ã€‚

## ç›®å½•

- [é¡¹ç›®æ¶æ„æ¦‚è§ˆ](#é¡¹ç›®æ¶æ„æ¦‚è§ˆ)
- [ç«¯å£æ¥å£å®šä¹‰](#ç«¯å£æ¥å£å®šä¹‰)
  - [1. LLMå®¢æˆ·ç«¯æ¥å£](#1-llmå®¢æˆ·ç«¯æ¥å£)
  - [2. å­˜å‚¨æ¥å£](#2-å­˜å‚¨æ¥å£)
  - [3. å¿«ç…§/è¿è¡Œè®°å½•æ¥å£](#3-å¿«ç…§è¿è¡Œè®°å½•æ¥å£)
  - [4. æ–°é—»æå–æ¥å£](#4-æ–°é—»æå–æ¥å£)
- [æ ¸å¿ƒæ¨¡å—](#æ ¸å¿ƒæ¨¡å—)
  - [1. Pipelineå¼•æ“](#1-pipelineå¼•æ“)
  - [2. æ•°æ®æå–æ¨¡å—](#2-æ•°æ®æå–æ¨¡å—)
  - [3. çŸ¥è¯†å›¾è°±æ“ä½œæ¨¡å—](#3-çŸ¥è¯†å›¾è°±æ“ä½œæ¨¡å—)
  - [4. æ–°é—»å¤„ç†æ¨¡å—](#4-æ–°é—»å¤„ç†æ¨¡å—)
  - [5. LLMé€‚é…å™¨æ¨¡å—](#5-llmé€‚é…å™¨æ¨¡å—)
  - [6. æ•°æ®å­˜å‚¨æ¨¡å—](#6-æ•°æ®å­˜å‚¨æ¨¡å—)
  - [7. åº”ç”¨æœåŠ¡æ¨¡å—](#7-åº”ç”¨æœåŠ¡æ¨¡å—)
  - [8. ä¸šåŠ¡é€»è¾‘æ¨¡å—](#8-ä¸šåŠ¡é€»è¾‘æ¨¡å—)
  - [9. é¢†åŸŸæ¨¡å‹æ¨¡å—](#9-é¢†åŸŸæ¨¡å‹æ¨¡å—)
  - [10. æ•°æ®ç®¡é“æ¨¡å—](#10-æ•°æ®ç®¡é“æ¨¡å—)
  - [11. è¯­ä¹‰åŒ¹é…æ¨¡å—](#11-è¯­ä¹‰åŒ¹é…æ¨¡å—)
  - [12. ä»»åŠ¡é˜Ÿåˆ—æ¨¡å—](#12-ä»»åŠ¡é˜Ÿåˆ—æ¨¡å—)
- [Webå±‚ç»„ä»¶](#webå±‚ç»„ä»¶)
  - [1. é¡µé¢ç»„ä»¶](#1-é¡µé¢ç»„ä»¶)
  - [2. æœåŠ¡ç»„ä»¶](#2-æœåŠ¡ç»„ä»¶)
  - [3. æ¡†æ¶ç»„ä»¶](#3-æ¡†æ¶ç»„ä»¶)
  - [4. ç»„ä»¶ç»„ä»¶](#4-ç»„ä»¶ç»„ä»¶)
- [åŸºç¡€è®¾æ–½ç»„ä»¶](#åŸºç¡€è®¾æ–½ç»„ä»¶)
  - [1. é…ç½®ç®¡ç†](#1-é…ç½®ç®¡ç†)
  - [2. ä¾èµ–æ³¨å…¥](#2-ä¾èµ–æ³¨å…¥)
  - [3. ç¼“å­˜ç®¡ç†](#3-ç¼“å­˜ç®¡ç†)
  - [4. æ–‡ä»¶å·¥å…·](#4-æ–‡ä»¶å·¥å…·)
  - [5. å•ä¾‹æ¨¡å¼](#5-å•ä¾‹æ¨¡å¼)
  - [6. åºåˆ—åŒ–å·¥å…·](#6-åºåˆ—åŒ–å·¥å…·)
  - [7. é‡è¯•ä¸ç†”æ–­](#7-é‡è¯•ä¸ç†”æ–­)
- [é”™è¯¯å¤„ç†ä¸æ—¥å¿—](#é”™è¯¯å¤„ç†ä¸æ—¥å¿—)
- [å·¥å…·å‡½æ•°](#å·¥å…·å‡½æ•°)
  - [1. é€šç”¨å·¥å…·](#1-é€šç”¨å·¥å…·)
  - [2. æ—¶é—´ä¸IDå·¥å…·](#2-æ—¶é—´ä¸idå·¥å…·)
  - [3. å¯†é’¥ç®¡ç†](#3-å¯†é’¥ç®¡ç†)

---

## é¡¹ç›®æ¶æ„æ¦‚è§ˆ

MarketLensé‡‡ç”¨åˆ†å±‚æ¶æ„ï¼ŒåŒ…å«ä»¥ä¸‹å±‚æ¬¡ï¼š

1. **Webå±‚**ï¼šStreamlitç”¨æˆ·ç•Œé¢
2. **åº”ç”¨å±‚**ï¼šæœåŠ¡ç¼–æ’å’Œä¸šåŠ¡æµç¨‹
3. **é¢†åŸŸå±‚**ï¼šæ ¸å¿ƒä¸šåŠ¡é€»è¾‘
4. **åŸºç¡€è®¾æ–½å±‚**ï¼šå¤–éƒ¨ç³»ç»Ÿé›†æˆå’Œå·¥å…·ç±»
5. **ç«¯å£å±‚**ï¼šå®šä¹‰å¤–éƒ¨ä¾èµ–çš„æŠ½è±¡æ¥å£

---

## ç«¯å£æ¥å£å®šä¹‰

### 1. LLMå®¢æˆ·ç«¯æ¥å£

#### `src.ports.llm_client.LLMClient`

**åŠŸèƒ½**: LLMå®¢æˆ·ç«¯æ¥å£ï¼Œå®šä¹‰äº†æ‰€æœ‰LLMä¾›åº”å•†å¿…é¡»å®ç°çš„æ–¹æ³•

**æ–¹æ³•**:
- `call(prompt: str, config: Optional[LLMCallConfig] = None) -> LLMResponse`: è°ƒç”¨LLMæœåŠ¡

**ç¤ºä¾‹**:
```python
from src.ports.llm_client import LLMClient

class OpenAIClient(LLMClient):
    def call(self, prompt: str, config: Optional[LLMCallConfig] = None) -> LLMResponse:
        # å®ç°OpenAIè°ƒç”¨é€»è¾‘
        pass
```

#### `src.ports.llm_client.LLMClientPool`

**åŠŸèƒ½**: LLMå®¢æˆ·ç«¯æ± æ¥å£ï¼Œç®¡ç†å¤šä¸ªLLMå®¢æˆ·ç«¯å®ä¾‹

**æ–¹æ³•**:
- `get_client(provider_type: LLMProviderType) -> LLMClient`: è·å–æŒ‡å®šç±»å‹çš„å®¢æˆ·ç«¯
- `call(prompt: str, config: Optional[LLMCallConfig] = None) -> LLMResponse`: è°ƒç”¨LLMæœåŠ¡

#### `src.ports.llm_client.RateLimiter`

**åŠŸèƒ½**: é™é€Ÿå™¨æ¥å£ï¼Œæ§åˆ¶APIè°ƒç”¨é¢‘ç‡

**æ–¹æ³•**:
- `acquire()`: è·å–ä¸€ä¸ªä»¤ç‰Œ
- `can_acquire() -> bool`: æ£€æŸ¥æ˜¯å¦å¯ä»¥è·å–ä»¤ç‰Œ

### 2. å­˜å‚¨æ¥å£

#### `src.ports.store.UnifiedStore`

**åŠŸèƒ½**: ç»Ÿä¸€å­˜å‚¨æ¥å£ï¼Œå®šä¹‰äº†æ•°æ®å­˜å‚¨çš„åŸºæœ¬æ“ä½œ

**æ–¹æ³•**:
- `save_entity(entity: EntityCanonical) -> bool`: ä¿å­˜å®ä½“
- `get_entity(entity_id: str) -> Optional[EntityCanonical]`: è·å–å®ä½“
- `save_event(event: EventCanonical) -> bool`: ä¿å­˜äº‹ä»¶
- `get_event(event_id: str) -> Optional[EventCanonical]`: è·å–äº‹ä»¶
- `get_entities_by_source(source: str) -> List[EntityCanonical]`: æ ¹æ®æ¥æºè·å–å®ä½“

**ç¤ºä¾‹**:
```python
from src.ports.store import UnifiedStore

class SQLiteStore(UnifiedStore):
    def save_entity(self, entity: EntityCanonical) -> bool:
        # å®ç°SQLiteå­˜å‚¨é€»è¾‘
        pass
    
    def get_entity(self, entity_id: str) -> Optional[EntityCanonical]:
        # å®ç°SQLiteæŸ¥è¯¢é€»è¾‘
        pass
```

#### `src.ports.store.KGReadStore`

**åŠŸèƒ½**: çŸ¥è¯†å›¾è°±åªè¯»å­˜å‚¨æ¥å£

**æ–¹æ³•**:
- `query_entities(keywords: List[str], limit: int = 10) -> List[dict]`: æŸ¥è¯¢å®ä½“
- `get_entity_by_name(name: str) -> Optional[dict]`: æ ¹æ®åç§°è·å–å®ä½“
- `get_related_entities(entity_id: str, depth: int = 1) -> List[dict]`: è·å–ç›¸å…³å®ä½“

#### `src.ports.store.EntityStore`

**åŠŸèƒ½**: å®ä½“å­˜å‚¨æ¥å£

**æ–¹æ³•**:
- `save_mention(mention: EntityMention) -> bool`: ä¿å­˜å®ä½“æåŠ
- `get_mentions_by_entity_id(entity_id: str) -> List[EntityMention]`: æ ¹æ®å®ä½“IDè·å–æåŠ

#### `src.ports.store.EventStore`

**åŠŸèƒ½**: äº‹ä»¶å­˜å‚¨æ¥å£

**æ–¹æ³•**:
- `save_mention(mention: EventMention) -> bool`: ä¿å­˜äº‹ä»¶æåŠ
- `get_events_by_entity(entity_id: str) -> List[EventMention]`: æ ¹æ®å®ä½“è·å–äº‹ä»¶

#### `src.ports.store.ReviewStore`

**åŠŸèƒ½**: å®¡æŸ¥å­˜å‚¨æ¥å£

**æ–¹æ³•**:
- `save_review_task(task: ReviewTask) -> bool`: ä¿å­˜å®¡æŸ¥ä»»åŠ¡
- `get_review_tasks(status: str) -> List[ReviewTask]`: è·å–æŒ‡å®šçŠ¶æ€çš„å®¡æŸ¥ä»»åŠ¡
- `update_review_task(task: ReviewTask) -> bool`: æ›´æ–°å®¡æŸ¥ä»»åŠ¡

### 3. å¿«ç…§/è¿è¡Œè®°å½•æ¥å£

#### `src.ports.snapshot.SnapshotWriter`

**åŠŸèƒ½**: å¿«ç…§å†™å…¥æ¥å£

**æ–¹æ³•**:
- `write(snapshot: Snapshot, params: SnapshotParams) -> bool`: å†™å…¥å¿«ç…§
- `get_available_types() -> List[GraphSnapshotType]`: è·å–å¯ç”¨çš„å¿«ç…§ç±»å‹

#### `src.ports.snapshot.SnapshotReader`

**åŠŸèƒ½**: å¿«ç…§è¯»å–æ¥å£

**æ–¹æ³•**:
- `read(params: SnapshotParams) -> Optional[Snapshot]`: è¯»å–å¿«ç…§
- `get_metadata(params: SnapshotParams) -> Optional[SnapshotMeta]`: è·å–å¿«ç…§å…ƒæ•°æ®

#### `src.ports.snapshot.RunStore`

**åŠŸèƒ½**: è¿è¡Œè®°å½•å­˜å‚¨æ¥å£

**æ–¹æ³•**:
- `save_run_record(record: RunRecord) -> bool`: ä¿å­˜è¿è¡Œè®°å½•
- `get_run_records(limit: int = 10) -> List[RunRecord]`: è·å–è¿è¡Œè®°å½•
- `get_run_record(run_id: str) -> Optional[RunRecord]`: è·å–æŒ‡å®šè¿è¡Œè®°å½•

### 4. æ–°é—»æå–æ¥å£

#### `src.ports.extraction.NewsSource`

**åŠŸèƒ½**: æ–°é—»æºæ¥å£

**æ–¹æ³•**:
- `fetch(config: FetchConfig) -> FetchResult`: è·å–æ–°é—»
- `get_source_info() -> Dict[str, Any]`: è·å–æºä¿¡æ¯

**ç¤ºä¾‹**:
```python
from src.ports.extraction import NewsSource, FetchConfig, FetchResult

class GNewsAdapter(NewsSource):
    def fetch(self, config: FetchConfig) -> FetchResult:
        # å®ç°GNewsè·å–é€»è¾‘
        pass
    
    def get_source_info(self) -> Dict[str, Any]:
        return {
            "name": "GNews",
            "base_url": "https://gnews.io"
        }
```

#### `src.ports.extraction.EntityExtractor`

**åŠŸèƒ½**: å®ä½“æå–æ¥å£

**æ–¹æ³•**:
- `extract(text: str, context: Optional[Dict[str, Any]] = None) -> EntityExtractionResult`: æå–å®ä½“
- `extract_batch(texts: List[str], context: Optional[Dict[str, Any]] = None) -> List[EntityExtractionResult]`: æ‰¹é‡æå–å®ä½“

#### `src.ports.extraction.EventExtractor`

**åŠŸèƒ½**: äº‹ä»¶æå–æ¥å£

**æ–¹æ³•**:
- `extract(text: str, context: Optional[Dict[str, Any]] = None) -> EventExtractionResult`: æå–äº‹ä»¶
- `extract_batch(texts: List[str], context: Optional[Dict[str, Any]] = None) -> List[EventExtractionResult]`: æ‰¹é‡æå–äº‹ä»¶

#### `src.ports.extraction.Deduplicator`

**åŠŸèƒ½**: å»é‡æ¥å£

**æ–¹æ³•**:
- `is_duplicate(text: str) -> bool`: æ£€æŸ¥æ˜¯å¦é‡å¤
- `add_text(text: str) -> None`: æ·»åŠ æ–‡æœ¬åˆ°å»é‡é›†åˆ

---

## æ ¸å¿ƒæ¨¡å—

### 1. Pipelineå¼•æ“

#### `src.app.pipeline.engine.PipelineEngine.run()`

**åŠŸèƒ½**: æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿

**å‚æ•°**:
- `pipeline_def` (dict): æµæ°´çº¿å®šä¹‰ï¼ŒåŒ…å«æ­¥éª¤ã€è¾“å…¥å’Œè¾“å‡ºé…ç½®

**è¿”å›å€¼**:
- `List[AgentResult]`: æ‰§è¡Œç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«æˆåŠŸçŠ¶æ€ã€æ¶ˆæ¯å’Œæ•°æ®

**å¼‚å¸¸**:
- `PipelineExecutionError`: å½“æµæ°´çº¿æ‰§è¡Œå¤±è´¥æ—¶æŠ›å‡º

**ç¤ºä¾‹**:
```python
from src.app.pipeline.engine import PipelineEngine

engine = PipelineEngine()
pipeline_def = {
    "name": "Default Pipeline",
    "steps": [
        {
            "id": "fetch_news",
            "tool": "fetch_news_stream",
            "inputs": {"limit": 10},
            "output": "raw_news_data"
        }
    ]
}

results = engine.run(pipeline_def)
```

#### `src.app.pipeline.context.PipelineContext`

**åŠŸèƒ½**: ç®¡ç†æµæ°´çº¿æ‰§è¡Œä¸Šä¸‹æ–‡å’ŒçŠ¶æ€

**æ–¹æ³•**:
- `set_value(key, value)`: è®¾ç½®ä¸Šä¸‹æ–‡å€¼
- `get_value(key, default=None)`: è·å–ä¸Šä¸‹æ–‡å€¼
- `add_log(message)`: æ·»åŠ æ—¥å¿—æ¶ˆæ¯

**ç¤ºä¾‹**:
```python
from src.app.pipeline.context import PipelineContext

context = PipelineContext()
context.set_value("news_data", news_list)
news_data = context.get_value("news_data")
```

### 2. æ•°æ®æå–æ¨¡å—

#### `src.adapters.extraction.llm_extractor.LLMExtractor.extract_events_from_news()`

**åŠŸèƒ½**: ä»æ–°é—»æ–‡æœ¬ä¸­æå–äº‹ä»¶å’Œå®ä½“ä¿¡æ¯

**å‚æ•°**:
- `text` (str): è¾“å…¥çš„æ–°é—»æ–‡æœ¬
- `source` (str): æ–°é—»æ¥æº
- `published_at` (str): å‘å¸ƒæ—¶é—´

**è¿”å›å€¼**:
- `EventExtractionResult`: åŒ…å«æå–çš„äº‹ä»¶å’Œå®ä½“çš„ç»“æ„åŒ–ç»“æœå¯¹è±¡

**å¼‚å¸¸**:
- `ProcessingError`: å½“æå–è¿‡ç¨‹å¤±è´¥æ—¶æŠ›å‡º

**ç¤ºä¾‹**:
```python
from src.adapters.extraction.llm_extractor import LLMExtractor

extractor = LLMExtractor()
result = extractor.extract_events_from_news(
    text="è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°æ¬¾iPhone",
    source="tech_news",
    published_at="2024-01-01T00:00:00Z"
)

if result.success:
    print(f"æå–åˆ° {len(result.events)} ä¸ªäº‹ä»¶")
```

#### `src.adapters.extraction.entity_merge_llm.EntityMerger.decide_entity_merges()`

**åŠŸèƒ½**: ä½¿ç”¨LLMå†³å®šå“ªäº›å®ä½“éœ€è¦åˆå¹¶

**å‚æ•°**:
- `entities_batch` (List[str]): å®ä½“åˆ—è¡¨
- `evidence_map` (Dict[str, List[str]]): å®ä½“ç›¸å…³è¯æ®æ˜ å°„ï¼ˆå¯é€‰ï¼‰
- `timeout_seconds` (int): LLMè°ƒç”¨è¶…æ—¶æ—¶é—´ï¼Œé»˜è®¤90ç§’

**è¿”å›å€¼**:
- `EntityMergeDecision`: åŒ…å«å®ä½“åˆ†æå’Œåˆå¹¶ç»„çš„å†³ç­–ç»“æœ

**å¼‚å¸¸**:
- `LLMError`: å½“LLMè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º

**ç¤ºä¾‹**:
```python
from src.adapters.extraction.entity_merge_llm import EntityMerger

decider = EntityMerger(llm_client)
decision = decider.decide_entity_merges(
    entities_batch=["è‹¹æœå…¬å¸", "Apple Inc.", "è‹¹æœ"],
    evidence_map={"è‹¹æœå…¬å¸": ["æ–°é—»A", "æ–°é—»B"]},
    timeout_seconds=120
)

for group in decision.merge_groups:
    print(f"ä¸»å®ä½“: {group.primary_entity}")
    print(f"é‡å¤å®ä½“: {group.duplicate_entities}")
```

### 3. çŸ¥è¯†å›¾è°±æ“ä½œæ¨¡å—

#### `src.adapters.sqlite.kg_read_store.KnowledgeGraphStore.add_entities()`

**åŠŸèƒ½**: å‘çŸ¥è¯†å›¾è°±ä¸­æ·»åŠ å®ä½“

**å‚æ•°**:
- `entities` (List[dict]): å®ä½“åˆ—è¡¨
- `source` (str): æ•°æ®æ¥æº
- `published_at` (str): å‘å¸ƒæ—¶é—´

**è¿”å›å€¼**:
- `bool`: æ“ä½œæ˜¯å¦æˆåŠŸ

**ç¤ºä¾‹**:
```python
from src.adapters.sqlite.kg_read_store import KnowledgeGraphStore

kg_store = KnowledgeGraphStore()
success = kg_store.add_entities(entities, "tech_news", "2024-01-01T00:00:00Z")
```

#### `src.app.business.graph_ops.KnowledgeGraph.refresh_graph()`

**åŠŸèƒ½**: åˆ·æ–°çŸ¥è¯†å›¾è°±ï¼ŒåŒ…æ‹¬å®ä½“åˆå¹¶ã€äº‹ä»¶æ¼”åŒ–ç­‰

**å‚æ•°**: æ— 

**è¿”å›å€¼**: æ— 

**å¼‚å¸¸**:
- `StoreError`: å½“å­˜å‚¨æ“ä½œå¤±è´¥æ—¶æŠ›å‡º

**ç¤ºä¾‹**:
```python
from src.app.business.graph_ops import KnowledgeGraph

kg = KnowledgeGraph()
kg.refresh_graph()
```

#### `src.app.business.graph_ops.update_graph_data()`

**åŠŸèƒ½**: æ›´æ–°çŸ¥è¯†å›¾è°±æ•°æ®æ–‡ä»¶

**å‚æ•°**:
- `events_list` (List[Dict]): äº‹ä»¶åˆ—è¡¨
- `default_source` (str): é»˜è®¤æ¥æºæ ‡è¯†ï¼Œé»˜è®¤ä¸º"auto_pipeline"

**è¿”å›å€¼**:
- `Dict[str, Any]`: æ›´æ–°çŠ¶æ€ä¿¡æ¯

**ç¤ºä¾‹**:
```python
from src.app.business.graph_ops import update_graph_data

status = update_graph_data(events_list, default_source="news_pipeline")
print(f"æ›´æ–°äº† {status['updated_count']} ä¸ªäº‹ä»¶")
```

### 4. æ–°é—»å¤„ç†æ¨¡å—

#### `src.adapters.news.api_manager.NewsAPIManager.fetch()`

**åŠŸèƒ½**: æ ¹æ®é…ç½®è·å–æ–°é—»æ•°æ®

**å‚æ•°**:
- `config` (FetchConfig): è·å–é…ç½®

**è¿”å›å€¼**:
- `FetchResult`: åŒ…å«è·å–ç»“æœçš„ç»“æ„åŒ–å¯¹è±¡

**ç¤ºä¾‹**:
```python
from src.adapters.news.api_manager import NewsAPIManager
from src.ports.extraction import FetchConfig

api_manager = NewsAPIManager()
config = FetchConfig(
    max_items=10,
    keywords=["ç§‘æŠ€", "äººå·¥æ™ºèƒ½"],
    from_date=datetime.now() - timedelta(days=7)
)

result = api_manager.fetch(config)
if result.success:
    print(f"è·å–åˆ° {len(result.items)} æ¡æ–°é—»")
```

#### `src.app.business.extraction.process_news_pipeline()`

**åŠŸèƒ½**: å¤„ç†æ–°é—»æ•°æ®çš„å®Œæ•´æµç¨‹

**å‚æ•°**:
- `max_workers` (int): æœ€å¤§å¹¶å‘æ•°ï¼Œé»˜è®¤3
- `rate_limit_per_sec` (float): æ¯ç§’é€Ÿç‡é™åˆ¶ï¼Œé»˜è®¤1.0

**è¿”å›å€¼**:
- `Dict[str, Any]`: å¤„ç†ç»Ÿè®¡ä¿¡æ¯

**ç¤ºä¾‹**:
```python
from src.app.business.extraction import process_news_pipeline

result = await process_news_pipeline(max_workers=5, rate_limit_per_sec=2.0)
print(f"å¤„ç†äº† {result['processed_count']} ä¸ªæ–°é—»æ–‡ä»¶")
```

### 5. LLMé€‚é…å™¨æ¨¡å—

#### `src.adapters.llm.pool.DefaultLLMPool.call()`

**åŠŸèƒ½**: è°ƒç”¨LLMæœåŠ¡

**å‚æ•°**:
- `prompt` (str): æç¤ºæ–‡æœ¬
- `config` (LLMCallConfig): è°ƒç”¨é…ç½®ï¼ˆå¯é€‰ï¼‰

**è¿”å›å€¼**:
- `LLMResponse`: åŒ…å«å“åº”å†…å®¹å’ŒçŠ¶æ€çš„ç»“æ„åŒ–å¯¹è±¡

**å¼‚å¸¸**:
- `LLMError`: å½“LLMè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º

**ç¤ºä¾‹**:
```python
from src.adapters.llm.pool import get_llm_pool
from src.ports.llm_client import LLMCallConfig

llm_pool = get_llm_pool()
config = LLMCallConfig(max_tokens=1000, temperature=0.7)

response = llm_pool.call("æ€»ç»“ä»¥ä¸‹æ–°é—»å†…å®¹ï¼š...", config)
if response.success:
    print(response.content)
```

#### `src.adapters.llm.providers` (é€šç”¨æ¥å£)

**åŠŸèƒ½**: LLMä¾›åº”å•†é€‚é…å™¨æ¥å£

**è¯´æ˜**: å®šä¹‰äº†æ‰€æœ‰LLMä¾›åº”å•†å¿…é¡»å®ç°çš„æ¥å£

### 6. æ•°æ®å­˜å‚¨æ¨¡å—

#### `src.adapters.sqlite.store.SQLiteStore.update_entities()`

**åŠŸèƒ½**: æ›´æ–°å®ä½“ä¿¡æ¯

**å‚æ•°**:
- `entities` (List[dict]): å®ä½“åˆ—è¡¨
- `entities_original` (List[dict]): åŸå§‹å®ä½“åˆ—è¡¨
- `source` (str): æ•°æ®æ¥æº
- `published_at` (str): å‘å¸ƒæ—¶é—´

**è¿”å›å€¼**: æ— 

**ç¤ºä¾‹**:
```python
from src.adapters.sqlite.store import get_store

store = get_store()
store.update_entities(entities, entities_original, "news_source", "2024-01-01T00:00:00Z")
```

#### `src.adapters.sqlite.store.get_store()`

**åŠŸèƒ½**: è·å–SQLiteå­˜å‚¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

**å‚æ•°**: æ— 

**è¿”å›å€¼**:
- `SQLiteStore`: å­˜å‚¨å®ä¾‹

**ç¤ºä¾‹**:
```python
from src.adapters.sqlite.store import get_store

store = get_store()
entities = store.get_all_entities()
```

### 7. åº”ç”¨æœåŠ¡æ¨¡å—

#### `src.app.services.IngestionService.ingest_news()`

**åŠŸèƒ½**: æ‰§è¡Œæ–°é—»å…¥åº“æµç¨‹

**å‚æ•°**:
- `news_items` (List[Dict]): æ–°é—»é¡¹ç›®åˆ—è¡¨

**è¿”å›å€¼**:
- `IngestionResult`: å…¥åº“ç»“æœå¯¹è±¡

**ç¤ºä¾‹**:
```python
from src.app.services_impl import get_ingestion_service

ingestion_service = get_ingestion_service()
result = ingestion_service.ingest_news(news_items)

if result.success:
    print(f"åˆ›å»ºäº† {result.mentions_created} ä¸ªæåŠ")
```

#### `src.app.services.ReviewService.run_end_to_end()`

**åŠŸèƒ½**: æ‰§è¡Œå®Œæ•´çš„å®¡æŸ¥æµç¨‹

**å‚æ•°**: æ— 

**è¿”å›å€¼**:
- `ReviewResult`: å®¡æŸ¥ç»“æœå¯¹è±¡

**ç¤ºä¾‹**:
```python
from src.app.services_impl import get_review_service

review_service = get_review_service()
result = review_service.run_end_to_end()

print(f"å®¡æŸ¥äº† {result.tasks_reviewed} ä¸ªä»»åŠ¡")
print(f"åº”ç”¨äº† {result.merges_applied} ä¸ªåˆå¹¶")
```

#### `src.app.services.KnowledgeGraphService.refresh()`

**åŠŸèƒ½**: åˆ·æ–°çŸ¥è¯†å›¾è°±

**å‚æ•°**: æ— 

**è¿”å›å€¼**:
- `ServiceResult`: æœåŠ¡ç»“æœå¯¹è±¡

**ç¤ºä¾‹**:
```python
from src.app.services_impl import get_kg_service

kg_service = get_kg_service()
result = kg_service.refresh()

if result.success:
    print(f"å›¾è°±åŒ…å« {result.data['entities_count']} ä¸ªå®ä½“")
```

### 8. ä¸šåŠ¡é€»è¾‘æ¨¡å—

#### `src.app.business.data_fetch.fetch_news_multi_source()`

**åŠŸèƒ½**: ä»å¤šä¸ªæ–°é—»æºè·å–æ–°é—»

**å‚æ•°**:
- `sources` (List[str]): æ•°æ®æºåˆ—è¡¨
- `limit` (int): é™åˆ¶æ•°é‡
- `keywords` (List[str]): æœç´¢å…³é”®è¯

**è¿”å›å€¼**:
- `List[Dict]`: æ–°é—»æ•°æ®åˆ—è¡¨

**ç¤ºä¾‹**:
```python
from src.app.business.data_fetch import fetch_news_multi_source

news = await fetch_news_multi_source(
    sources=["gnews_cn", "gnews_us"],
    limit=50,
    keywords=["ç§‘æŠ€", "åˆ›æ–°"]
)
```

#### `src.app.business.review_ops.generate_entity_merge_candidates()`

**åŠŸèƒ½**: ç”Ÿæˆå®ä½“åˆå¹¶å€™é€‰

**å‚æ•°**: æ— 

**è¿”å›å€¼**:
- `List[Dict]`: åˆå¹¶å€™é€‰åˆ—è¡¨

**ç¤ºä¾‹**:
```python
from src.app.business.review_ops import generate_entity_merge_candidates

candidates = generate_entity_merge_candidates()
print(f"ç”Ÿæˆäº† {len(candidates)} ä¸ªåˆå¹¶å€™é€‰")
```

### 9. é¢†åŸŸæ¨¡å‹æ¨¡å—

#### `src.domain.models.EntityCanonical`

**åŠŸèƒ½**: æ ‡å‡†å®ä½“æ¨¡å‹

**å±æ€§**:
- `id` (str): å®ä½“å”¯ä¸€ID
- `name` (str): å®ä½“åç§°
- `type` (str): å®ä½“ç±»å‹
- `first_seen` (str): é¦–æ¬¡å‡ºç°æ—¶é—´
- `last_seen` (str): æœ€åå‡ºç°æ—¶é—´
- `sources` (List[str]): æ¥æºåˆ—è¡¨
- `original_forms` (List[str]): åŸå§‹å½¢å¼åˆ—è¡¨

**ç¤ºä¾‹**:
```python
from src.domain.models import EntityCanonical

entity = EntityCanonical(
    id="entity_001",
    name="è‹¹æœå…¬å¸",
    type="organization",
    first_seen="2024-01-01T00:00:00Z",
    last_seen="2024-01-01T00:00:00Z",
    sources=["news_source_1"],
    original_forms=["è‹¹æœå…¬å¸", "Apple Inc."]
)
```

#### `src.domain.models.EventCanonical`

**åŠŸèƒ½**: æ ‡å‡†äº‹ä»¶æ¨¡å‹

**å±æ€§**:
- `id` (str): äº‹ä»¶å”¯ä¸€ID
- `abstract` (str): äº‹ä»¶æ‘˜è¦
- `event_summary` (str): äº‹ä»¶æ€»ç»“
- `timestamp` (str): æ—¶é—´æˆ³
- `entities` (List[str]): ç›¸å…³å®ä½“åˆ—è¡¨
- `first_seen` (str): é¦–æ¬¡å‡ºç°æ—¶é—´
- `last_seen` (str): æœ€åå‡ºç°æ—¶é—´
- `sources` (List[str]): æ¥æºåˆ—è¡¨

### 10. æ•°æ®ç®¡é“æ¨¡å—

#### `src.domain.data_pipeline.StandardEventPipeline.execute()`

**åŠŸèƒ½**: æ‰§è¡Œæ ‡å‡†äº‹ä»¶æ•°æ®ç®¡é“å¤„ç†

**å‚æ•°**:
- `data` (Any): è¾“å…¥æ•°æ®

**è¿”å›å€¼**:
- `PipelineResult`: ç®¡é“æ‰§è¡Œç»“æœ

**ç¤ºä¾‹**:
```python
from src.domain.data_pipeline import StandardEventPipeline

pipeline = StandardEventPipeline()
result = await pipeline.execute(events_list)
processed_data = result.get("serialization", [])
```

#### `src.domain.data_pipeline.DataNormalizer.normalize()`

**åŠŸèƒ½**: è§„èŒƒåŒ–æ•°æ®

**å‚æ•°**:
- `data` (Dict[str, Any]): å¾…è§„èŒƒåŒ–çš„æ•°æ®

**è¿”å›å€¼**:
- `Dict[str, Any]`: è§„èŒƒåŒ–åçš„æ•°æ®

### 11. è¯­ä¹‰åŒ¹é…æ¨¡å—

#### `src.infra.semantic_matcher.SemanticMatcher.encode()`

**åŠŸèƒ½**: å°†æ–‡æœ¬ç¼–ç ä¸ºè¯­ä¹‰å‘é‡

**å‚æ•°**:
- `texts` (List[str]): æ–‡æœ¬åˆ—è¡¨

**è¿”å›å€¼**:
- `Optional[np.ndarray]`: åµŒå…¥å‘é‡çŸ©é˜µï¼Œshape: (len(texts), embedding_dim)

**ç¤ºä¾‹**:
```python
from src.infra.semantic_matcher import SemanticMatcher

matcher = SemanticMatcher()
embeddings = matcher.encode(["è‹¹æœå…¬å¸", "Apple Inc."])
```

### 12. ä»»åŠ¡é˜Ÿåˆ—æ¨¡å—

#### `src.infra.task_queue.TaskQueue.add_task()`

**åŠŸèƒ½**: æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—

**å‚æ•°**:
- `func` (Callable): ä»»åŠ¡å‡½æ•°
- `*args`: ä½ç½®å‚æ•°
- `**kwargs`: å…³é”®å­—å‚æ•°
- `priority` (int): ä¼˜å…ˆçº§
- `callback` (Callable): å›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰

**è¿”å›å€¼**:
- `str`: ä»»åŠ¡ID

**ç¤ºä¾‹**:
```python
from src.infra.task_queue import TaskQueue

queue = TaskQueue(max_workers=5)
task_id = queue.add_task(print, "Hello World", priority=1)
```

---

## Webå±‚ç»„ä»¶

### 1. é¡µé¢ç»„ä»¶

#### `src.web.pages_impl.run.render()`

**åŠŸèƒ½**: æ¸²æŸ“è¿è¡Œæµç¨‹é¡µé¢

**å‚æ•°**: æ— 

**è¿”å›å€¼**: æ— 

**ç¤ºä¾‹**:
```python
from src.web.pages_impl.run import render

render()  # åœ¨Streamlité¡µé¢ä¸­è°ƒç”¨
```

#### `src.web.pages_impl.graph.render()`

**åŠŸèƒ½**: æ¸²æŸ“çŸ¥è¯†å›¾è°±é¡µé¢

**å‚æ•°**: æ— 

**è¿”å›å€¼**: æ— 

**ç¤ºä¾‹**:
```python
from src.web.pages_impl.graph import render

render()  # åœ¨Streamlité¡µé¢ä¸­è°ƒç”¨
```

### 2. æœåŠ¡ç»„ä»¶

#### `src.web.services.pipeline_runner.PipelineRunner.start()`

**åŠŸèƒ½**: å¯åŠ¨æµæ°´çº¿æ‰§è¡Œ

**å‚æ•°**:
- `pipeline_def` (dict): æµæ°´çº¿å®šä¹‰
- `history_idx` (int): å†å²ç´¢å¼•
- `run_id` (str): è¿è¡ŒID
- `project_id` (str): é¡¹ç›®ID

**è¿”å›å€¼**:
- `bool`: æ˜¯å¦å¯åŠ¨æˆåŠŸ

**ç¤ºä¾‹**:
```python
from src.web.services.pipeline_runner import get_global_pipeline_runner

runner = get_global_pipeline_runner()
success = runner.start(pipeline_def, history_idx=0, run_id="run_001", project_id="project_001")
```

#### `src.web.services.run_store.save_run_change_pack()`

**åŠŸèƒ½**: ä¿å­˜è¿è¡Œå˜æ›´åŒ…

**å‚æ•°**:
- `project_id` (str): é¡¹ç›®ID
- `pack` (RunChangePack): è¿è¡Œå˜æ›´åŒ…

**è¿”å›å€¼**: æ— 

### 3. æ¡†æ¶ç»„ä»¶

#### `src.web.framework.page.init_page()`

**åŠŸèƒ½**: åˆå§‹åŒ–é¡µé¢

**å‚æ•°**:
- `page_spec` (PageSpec): é¡µé¢è§„æ ¼

**è¿”å›å€¼**: æ— 

**ç¤ºä¾‹**:
```python
from src.web.framework.page import init_page, PageSpec

init_page(PageSpec(title="æ–°é—»æ™ºèƒ½ä½“ç³»ç»Ÿ", icon="ğŸ“°"))
```

#### `src.web.framework.user_context.get_user_context()`

**åŠŸèƒ½**: è·å–ç”¨æˆ·ä¸Šä¸‹æ–‡

**å‚æ•°**: æ— 

**è¿”å›å€¼**:
- `UserContext`: ç”¨æˆ·ä¸Šä¸‹æ–‡å¯¹è±¡

**ç¤ºä¾‹**:
```python
from src.web.framework.user_context import get_user_context

ctx = get_user_context()
project_id = ctx.project_id
```

### 4. ç»„ä»¶ç»„ä»¶

#### `src.web.components.task_monitor.render_task_monitor()`

**åŠŸèƒ½**: æ¸²æŸ“ä»»åŠ¡ç›‘æ§ç»„ä»¶

**å‚æ•°**:
- `pipeline_runner` (PipelineRunner): æµæ°´çº¿è¿è¡Œå™¨å®ä¾‹

**è¿”å›å€¼**: æ— 

**ç¤ºä¾‹**:
```python
from src.web.components.task_monitor import render_task_monitor
from src.web.services.pipeline_runner import get_global_pipeline_runner

runner = get_global_pipeline_runner()
render_task_monitor(runner)
```

---

## åŸºç¡€è®¾æ–½ç»„ä»¶

### 1. é…ç½®ç®¡ç†

#### `src.infra.config.ConfigManager.get_config_value()`

**åŠŸèƒ½**: è·å–é…ç½®å€¼

**å‚æ•°**:
- `key` (str): é…ç½®é”®
- `default` (Any): é»˜è®¤å€¼

**è¿”å›å€¼**:
- `Any`: é…ç½®å€¼

**ç¤ºä¾‹**:
```python
from src.infra.config import get_config_manager

config = get_config_manager()
rate_limit = config.get_config_value("rate_limit_per_sec", 1.0)
```

#### `src.infra.config.ConfigManager.set_config_value()`

**åŠŸèƒ½**: è®¾ç½®é…ç½®å€¼

**å‚æ•°**:
- `key` (str): é…ç½®é”®
- `value` (Any): é…ç½®å€¼

**è¿”å›å€¼**: æ— 

### 2. ä¾èµ–æ³¨å…¥

#### `src.infra.di_container.get_container()`

**åŠŸèƒ½**: è·å–å…¨å±€ä¾èµ–æ³¨å…¥å®¹å™¨

**è¿”å›å€¼**:
- `DependencyContainer`: ä¾èµ–æ³¨å…¥å®¹å™¨å®ä¾‹

**ç¤ºä¾‹**:
```python
from src.infra.di_container import get_container

container = get_container()
config_manager = container.resolve(ConfigManager)
```

#### `src.infra.di_container.register_service()`

**åŠŸèƒ½**: æ³¨å†ŒæœåŠ¡

**å‚æ•°**:
- `service_type` (Type[T]): æœåŠ¡ç±»å‹
- `implementation_type` (Optional[Type[T]]): å®ç°ç±»å‹ï¼ˆå¯é€‰ï¼‰
- `lifetime` (ServiceLifetime): ç”Ÿå‘½å‘¨æœŸ

**ç¤ºä¾‹**:
```python
from src.infra.di_container import register_service
from src.ports.store import UnifiedStore
from src.adapters.sqlite.store import SQLiteStore

register_service(UnifiedStore, SQLiteStore)
```

### 3. ç¼“å­˜ç®¡ç†

#### `src.infra.cache.MemoryCache.get()`

**åŠŸèƒ½**: ä»å†…å­˜ç¼“å­˜è·å–å€¼

**å‚æ•°**:
- `key` (str): ç¼“å­˜é”®
- `default` (Any): é»˜è®¤å€¼

**è¿”å›å€¼**:
- `Any`: ç¼“å­˜å€¼æˆ–é»˜è®¤å€¼

#### `src.infra.cache.MemoryCache.set()`

**åŠŸèƒ½**: è®¾ç½®ç¼“å­˜å€¼

**å‚æ•°**:
- `key` (str): ç¼“å­˜é”®
- `value` (Any): ç¼“å­˜å€¼
- `ttl` (Optional[int]): è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰

**è¿”å›å€¼**: æ— 

### 4. æ–‡ä»¶å·¥å…·

#### `src.infra.file_utils.ensure_dir()`

**åŠŸèƒ½**: ç¡®ä¿ç›®å½•å­˜åœ¨

**å‚æ•°**:
- `path` (Path): ç›®å½•è·¯å¾„

**è¿”å›å€¼**: æ— 

**ç¤ºä¾‹**:
```python
from src.infra.file_utils import ensure_dir
from pathlib import Path

ensure_dir(Path("data/tmp"))
```

#### `src.infra.file_utils.read_json_sync()`

**åŠŸèƒ½**: åŒæ­¥è¯»å–JSONæ–‡ä»¶

**å‚æ•°**:
- `file_path` (Path): æ–‡ä»¶è·¯å¾„

**è¿”å›å€¼**:
- `Any`: JSONå†…å®¹

### 5. å•ä¾‹æ¨¡å¼

#### `src.infra.singleton.SingletonBase`

**åŠŸèƒ½**: å•ä¾‹æ¨¡å¼åŸºç±»

**ç¤ºä¾‹**:
```python
from src.infra.singleton import SingletonBase

class MySingleton(SingletonBase):
    def __init__(self):
        self.value = 0

instance = MySingleton()  # ç¬¬ä¸€æ¬¡åˆ›å»ºå®ä¾‹
instance2 = MySingleton()  # è¿”å›ç›¸åŒå®ä¾‹
assert instance is instance2
```

### 6. åºåˆ—åŒ–å·¥å…·

#### `src.infra.serialization.Serializer.safe_json_dumps()`

**åŠŸèƒ½**: å®‰å…¨JSONåºåˆ—åŒ–

**å‚æ•°**:
- `obj` (Any): è¦åºåˆ—åŒ–çš„å¯¹è±¡
- `**kwargs`: å…¶ä»–JSONå‚æ•°

**è¿”å›å€¼**:
- `str`: JSONå­—ç¬¦ä¸²

**ç¤ºä¾‹**:
```python
from src.infra.serialization import Serializer

data = {"name": "test", "value": 123}
json_str = Serializer.safe_json_dumps(data)
```

### 7. é‡è¯•ä¸ç†”æ–­

#### `src.infra.common.retry_with_backoff()`

**åŠŸèƒ½**: å¸¦é€€é¿ç­–ç•¥çš„é‡è¯•è£…é¥°å™¨

**å‚æ•°**:
- `retries` (int): é‡è¯•æ¬¡æ•°
- `backoff_factor` (float): é€€é¿å› å­
- `exceptions` (Tuple): è¦æ•è·çš„å¼‚å¸¸ç±»å‹

**ç¤ºä¾‹**:
```python
from src.infra.common import retry_with_backoff

@retry_with_backoff(retries=3, backoff_factor=1.5)
def unreliable_function():
    # å¯èƒ½å¤±è´¥çš„å‡½æ•°
    pass
```

#### `src.infra.common.SimpleCircuitBreaker`

**åŠŸèƒ½**: ç®€å•ç†”æ–­å™¨

**æ–¹æ³•**:
- `can_call() -> bool`: æ£€æŸ¥æ˜¯å¦å¯ä»¥è°ƒç”¨
- `record_success()`: è®°å½•æˆåŠŸ
- `record_failure()`: è®°å½•å¤±è´¥

---

## é”™è¯¯å¤„ç†ä¸æ—¥å¿—

### å¼‚å¸¸å¤„ç†

#### `src.infra.exceptions.handle_errors()`

**åŠŸèƒ½**: ç»Ÿä¸€é”™è¯¯å¤„ç†è£…é¥°å™¨

**å‚æ•°**:
- `logger` (logging.Logger): æ—¥å¿—è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰

**ç¤ºä¾‹**:
```python
from src.infra.exceptions import handle_errors

@handle_errors()
def risky_function():
    # å¯èƒ½å‡ºé”™çš„å‡½æ•°
    pass
```

### æ—¥å¿—è®°å½•

#### `src.infra.logging.get_logger()`

**åŠŸèƒ½**: è·å–æ—¥å¿—è®°å½•å™¨

**å‚æ•°**:
- `name` (str): è®°å½•å™¨åç§°

**è¿”å›å€¼**:
- `logging.Logger`: æ—¥å¿—è®°å½•å™¨å®ä¾‹

**ç¤ºä¾‹**:
```python
from src.infra.logging import get_logger

logger = get_logger(__name__)
logger.info("è¿™æ˜¯ä¸€ä¸ªä¿¡æ¯æ—¥å¿—")
```

---

## å·¥å…·å‡½æ•°

### 1. é€šç”¨å·¥å…·

#### `src.infra.registry.register_tool()`

**åŠŸèƒ½**: è£…é¥°å™¨ï¼Œç”¨äºæ³¨å†Œå¯ç”±Pipelineè°ƒç”¨çš„å·¥å…·å‡½æ•°

**å‚æ•°**:
- `name` (str): å·¥å…·åç§°
- `description` (str): å·¥å…·æè¿°
- `category` (str): å·¥å…·ç±»åˆ«

**ç¤ºä¾‹**:
```python
from src.infra.registry import register_tool

@register_tool(
    name="custom_extraction",
    description="è‡ªå®šä¹‰å®ä½“æå–å·¥å…·",
    category="Extraction"
)
def custom_extraction(text: str) -> List[Dict]:
    # å®ç°è‡ªå®šä¹‰æå–é€»è¾‘
    return [{"entity": "example", "type": "misc"}]
```

### 2. æ—¶é—´ä¸IDå·¥å…·

#### `src.infra.common.utc_now()`

**åŠŸèƒ½**: è·å–å½“å‰UTCæ—¶é—´

**è¿”å›å€¼**:
- `datetime`: å½“å‰UTCæ—¶é—´

**ç¤ºä¾‹**:
```python
from src.infra.common import utc_now

now = utc_now()
print(now.isoformat())
```

#### `src.infra.common.IdFactory.generate()`

**åŠŸèƒ½**: ç”Ÿæˆå”¯ä¸€ID

**è¿”å›å€¼**:
- `str`: å”¯ä¸€IDå­—ç¬¦ä¸²

**ç¤ºä¾‹**:
```python
from src.infra.common import IdFactory

factory = IdFactory()
unique_id = factory.generate()
print(unique_id)
```

### 3. å¯†é’¥ç®¡ç†

#### `src.infra.key_manager.get_key_manager()`

**åŠŸèƒ½**: è·å–å¯†é’¥ç®¡ç†å™¨å®ä¾‹

**è¿”å›å€¼**:
- `KeyManager`: å¯†é’¥ç®¡ç†å™¨å®ä¾‹

**ç¤ºä¾‹**:
```python
from src.infra.key_manager import get_key_manager

key_manager = get_key_manager()
key_manager.store_api_key("openai", "your-api-key")
api_key = key_manager.get_api_key("openai")
```

#### `src.infra.key_manager.KeyManager.store_api_key()`

**åŠŸèƒ½**: å­˜å‚¨APIå¯†é’¥

**å‚æ•°**:
- `service_name` (str): æœåŠ¡åç§°
- `api_key` (str): APIå¯†é’¥

**è¿”å›å€¼**:
- `bool`: æ˜¯å¦å­˜å‚¨æˆåŠŸ

#### `src.infra.key_manager.KeyManager.get_api_key()`

**åŠŸèƒ½**: è·å–APIå¯†é’¥

**å‚æ•°**:
- `service_name` (str): æœåŠ¡åç§°

**è¿”å›å€¼**:
- `Optional[str]`: APIå¯†é’¥æˆ–None

---

## æ–‡æ¡£ç»´æŠ¤è¯´æ˜

### å¦‚ä½•æ›´æ–°æ–‡æ¡£

1. å½“æ·»åŠ æ–°åŠŸèƒ½æˆ–ä¿®æ”¹ç°æœ‰åŠŸèƒ½æ—¶ï¼Œè¯·åŒæ­¥æ›´æ–°æœ¬æ–‡æ¡£
2. éµå¾ªç»Ÿä¸€çš„æ ¼å¼ï¼šåŠŸèƒ½æè¿° â†’ å‚æ•° â†’ è¿”å›å€¼ â†’ å¼‚å¸¸ â†’ ç¤ºä¾‹
3. ç¤ºä¾‹ä»£ç åº”å…·æœ‰å¯æ‰§è¡Œæ€§
4. ä¿æŒæ–‡æ¡£ä¸ä»£ç çš„ä¸€è‡´æ€§

### æ–‡æ¡£æ ¼å¼è§„èŒƒ

- ä½¿ç”¨markdownæ ¼å¼
- å‡½æ•°åä½¿ç”¨å®Œæ•´è·¯å¾„
- å‚æ•°å’Œè¿”å›å€¼ç±»å‹ä½¿ç”¨Pythonç±»å‹æ³¨è§£æ ¼å¼
- ç¤ºä¾‹ä»£ç éœ€è¦åŒ…å«å®é™…çš„å¯¼å…¥è¯­å¥
- æŒ‰åŠŸèƒ½æ¨¡å—ç»„ç»‡ï¼Œä¾¿äºæŸ¥æ‰¾
- ä¸ºæ¥å£å’Œå®ç°ç±»åˆ†åˆ«æä¾›è¯´æ˜
- åŒ…å«æ‰€æœ‰é‡è¦ä¸šåŠ¡é€»è¾‘å‡½æ•°
- è¦†ç›–ä»åŸºç±»æ¥å£åˆ°æ‰€æœ‰é‡å†™å®ç°çš„å®Œæ•´ç»§æ‰¿é“¾
- åŒ…å«é¢†åŸŸæ¨¡å‹ã€æ•°æ®ç®¡é“ã€Webå±‚ç»„ä»¶ç­‰å…³é”®æ¨¡å—
