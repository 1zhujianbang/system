import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Normal
import numpy as np
import pandas as pd
import gym
from gym import spaces
from collections import deque
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class TorchTradingEnvironment(gym.Env):
    def __init__(self, df, initial_balance=10000, transaction_cost=0.001, lookback_window=50):
        super(TorchTradingEnvironment, self).__init__()
        
        # æ•°æ®é¢„å¤„ç† - å¡«å……NaNå€¼
        self.df = df.reset_index(drop=True).fillna(method='bfill').fillna(method='ffill')
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost
        self.lookback_window = lookback_window
        
        # åŠ¨ä½œç©ºé—´: [-1, 1] è¿ç»­åŠ¨ä½œ
        self.action_space = spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)
        
        # çŠ¶æ€ç©ºé—´ç»´åº¦
        self.state_dim = self._get_state_dim()
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(self.state_dim,), 
            dtype=np.float32
        )
        
        self.reset()
    
    def _get_state_dim(self):
        """è®¡ç®—çŠ¶æ€ç»´åº¦"""
        base_features = 4  # OHLC
        technical_features = 17
        account_features = 4
        history_features = self.lookback_window * 2
        
        return base_features + technical_features + account_features + history_features
    
    def _get_features(self, step):
        """è·å–å®Œæ•´çŠ¶æ€ç‰¹å¾"""
        if step < self.lookback_window:
            step = self.lookback_window
        
        row = self.df.iloc[step]
        
        # åŸºç¡€ä»·æ ¼ç‰¹å¾ - ä½¿ç”¨å¯¹æ•°æ”¶ç›Šç‡å½’ä¸€åŒ–
        price_features = [
            np.log(row['open'] / row['close']),
            np.log(row['high'] / row['close']), 
            np.log(row['low'] / row['close']),
            0.0  # closeç›¸å¯¹äºè‡ªå·±ä¸º0
        ]
        
        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ - è¿›è¡Œå½’ä¸€åŒ–
        technical_features = [
            (row['ma_5'] - row['close']) / row['close'],
            (row['ma_10'] - row['close']) / row['close'],
            (row['ma_20'] - row['close']) / row['close'],
            (row['ma_50'] - row['close']) / row['close'],
            (row['ma_200'] - row['close']) / row['close'],
            (row['ema_12'] - row['close']) / row['close'],
            (row['ema_26'] - row['close']) / row['close'],
            (row['rsi'] - 50) / 50,  # RSIå½’ä¸€åŒ–åˆ°[-1,1]
            row['macd'] / (abs(row['close']) + 1e-8),
            row['macd_signal'] / (abs(row['close']) + 1e-8),
            (row['bollinger_upper'] - row['close']) / row['close'],
            (row['bollinger_middle'] - row['close']) / row['close'],
            (row['bollinger_lower'] - row['close']) / row['close'],
            row['atr'] / row['close'],
            np.log(row['volume'] + 1),
            np.log(row['volume_ma_5'] + 1),
            row['volume_ratio'] - 1
        ]
        
        # è´¦æˆ·çŠ¶æ€ç‰¹å¾
        account_features = [
            self.balance / self.initial_balance,
            self.position,  # ç›´æ¥ä½¿ç”¨æŒä»“æ¯”ä¾‹ï¼Œä¸ç”¨ä»·æ ¼
            self.total_value / self.initial_balance,
            np.clip(self.returns, -1, 10)  # é™åˆ¶æ”¶ç›Šç‡èŒƒå›´
        ]
        
        # å†å²ä»·æ ¼åºåˆ—ç‰¹å¾ - ä½¿ç”¨å¯¹æ•°æ”¶ç›Šç‡
        history_features = []
        current_price = row['close']
        for i in range(step - self.lookback_window, step):
            if i >= 0:
                hist_row = self.df.iloc[i]
                price_return = np.log(hist_row['close'] / current_price)
                volume_ratio = np.log(hist_row['volume'] / (row['volume'] + 1e-8) + 1)
                history_features.extend([price_return, volume_ratio])
            else:
                history_features.extend([0, 0])
        
        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        features = np.array(price_features + technical_features + account_features + history_features, 
                           dtype=np.float32)
        
        # å¤„ç†å¼‚å¸¸å€¼
        features = np.nan_to_num(features, nan=0.0, posinf=1.0, neginf=-1.0)
        features = np.clip(features, -10, 10)  # é™åˆ¶ç‰¹å¾èŒƒå›´
        
        return features
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = self.lookback_window
        self.balance = self.initial_balance
        self.position = 0.0  # æŒä»“æ•°é‡
        self.position_value = 0.0  # æŒä»“ä»·å€¼
        self.total_value = self.initial_balance
        self.returns = 0.0
        self.trades = []
        self.done = False
        self.max_drawdown = 0.0
        self.peak_value = self.initial_balance
        
        return self._get_features(self.current_step)
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        if self.done:
            return self._get_features(self.current_step), 0, True, {}
        
        current_price = self.df.iloc[self.current_step]['close']
        
        # ä¿®å¤åŠ¨ä½œç»´åº¦é—®é¢˜
        if isinstance(action, np.ndarray):
            action = action[0]  # ä»æ•°ç»„ä¸­æå–æ ‡é‡å€¼
        else:
            action = float(action)

        action = np.clip(action, -1, 1)  # ç¡®ä¿åŠ¨ä½œåœ¨æœ‰æ•ˆèŒƒå›´å†…
        
        # è®¡ç®—ç›®æ ‡æŒä»“ä»·å€¼
        target_value = action * self.total_value
        current_position_value = self.position * current_price
        
        # æ‰§è¡Œäº¤æ˜“
        trade_value = target_value - current_position_value
        
        # äº¤æ˜“æˆæœ¬å’Œæ»‘ç‚¹
        transaction_cost = abs(trade_value) * self.transaction_cost
        
        if abs(trade_value) > self.total_value * 0.01:  # æœ€å°äº¤æ˜“é˜ˆå€¼
            # æ›´æ–°æŒä»“å’Œä½™é¢
            if trade_value > 0:  # ä¹°å…¥
                shares_to_buy = trade_value / current_price
                self.position += shares_to_buy
                self.balance -= trade_value + transaction_cost
            else:  # å–å‡º
                shares_to_sell = abs(trade_value) / current_price
                self.position = max(0, self.position - shares_to_sell)  # ä¸èƒ½å–ç©º
                self.balance += abs(trade_value) - transaction_cost
            
            self.trades.append({
                'step': self.current_step,
                'action': action,
                'price': current_price,
                'value': trade_value
            })
        
        # æ›´æ–°æ€»èµ„äº§å’Œæ”¶ç›Š
        self.total_value = self.balance + self.position * current_price
        self.returns = (self.total_value - self.initial_balance) / self.initial_balance
        
        # æ›´æ–°æœ€å¤§å›æ’¤
        if self.total_value > self.peak_value:
            self.peak_value = self.total_value
        current_drawdown = (self.peak_value - self.total_value) / self.peak_value
        self.max_drawdown = max(self.max_drawdown, current_drawdown)
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥
        self.current_step += 1
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        if self.current_step >= len(self.df) - 1:
            self.done = True
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward(action, current_price)
        
        # è·å–æ–°çŠ¶æ€
        next_state = self._get_features(self.current_step)
        
        return next_state, reward, self.done, {
            'total_value': self.total_value,
            'returns': self.returns,
            'max_drawdown': self.max_drawdown
        }
    
    def _calculate_reward(self, action, price):
        """å¹³è¡¡å¤šç©ºç­–ç•¥çš„å¥–åŠ±å‡½æ•°"""
        row = self.df.iloc[self.current_step]
        
        action_scalar = float(action)
        
        # 1. åŸºç¡€æ”¶ç›Šå¥–åŠ±
        portfolio_return = (self.total_value - self.initial_balance) / self.initial_balance
        returns_reward = portfolio_return * 0.05
        
        # 2. æ–¹å‘æ€§å¥–åŠ±
        directional_reward = 0
        
        # æŠ€æœ¯æŒ‡æ ‡åˆ¤æ–­
        rsi = row['rsi'] if not pd.isna(row['rsi']) else 50
        bb_position = row['bollinger_position'] if not pd.isna(row['bollinger_position']) else 0.5
        macd = row['macd'] if not pd.isna(row['macd']) else 0
        macd_signal = row['macd_signal'] if not pd.isna(row['macd_signal']) else 0
        
        # ä¹°å…¥ä¿¡å·å¥–åŠ±
        buy_signals = 0
        if rsi < 35:
            buy_signals += 0.05
        if bb_position < 0.2:
            buy_signals += 0.05
        if macd > macd_signal:
            buy_signals += 0.05
        
        # å–å‡ºä¿¡å·å¥–åŠ±  
        sell_signals = 0
        if rsi > 65:
            sell_signals += 0.05
        if bb_position > 0.8:
            sell_signals += 0.05
        if macd < macd_signal:
            sell_signals += 0.05
        
        # æ–¹å‘ä¸€è‡´æ€§å¥–åŠ±
        if buy_signals >= 2 and action_scalar > 0.2:
            directional_reward += 0.1
            # print(f"âœ… æ­£ç¡®ä¹°å…¥! RSI: {rsi:.1f}, å¸ƒæ—å¸¦: {bb_position:.2f}, åŠ¨ä½œ: {action_scalar:.2f}")
        elif sell_signals >= 2 and action_scalar < -0.2:
            directional_reward += 0.1
            # print(f"âœ… æ­£ç¡®å–å‡º! RSI: {rsi:.1f}, å¸ƒæ—å¸¦: {bb_position:.2f}, åŠ¨ä½œ: {action_scalar:.2f}")
        elif buy_signals >= 2 and action_scalar < -0.2:
            directional_reward -= 0.09  # é€†åŠ¿å–å‡ºæƒ©ç½š
            # print(f"âŒ é”™è¯¯å–å‡º! RSI: {rsi:.1f}, åŠ¨ä½œ: {action_scalar:.2f}")
        elif sell_signals >= 2 and action_scalar > 0.2:
            directional_reward -= 0.09  # é€†åŠ¿ä¹°å…¥æƒ©ç½š
            # print(f"âŒ é”™è¯¯ä¹°å…¥! RSI: {rsi:.1f}, åŠ¨ä½œ: {action_scalar:.2f}")
        
        # 3. æŒä»“å¹³è¡¡å¥–åŠ±
        position_value = self.position * price
        position_ratio = position_value / self.total_value
        
        if abs(action_scalar) > 0.5:  # å¤§å¹…åŠ¨ä½œ
            if action_scalar > 0.5 and position_ratio < 0.8:  # å¤§å¹…ä¹°å…¥ä¸”æœªè¶…ä¹°
                balance_reward = 0.1
            elif action_scalar < -0.5 and position_ratio > -0.8:  # å¤§å¹…å–å‡ºä¸”æœªè¶…å–
                balance_reward = 0.1
            else:
                balance_reward = -0.15  # è¿‡åº¦æŒä»“æƒ©ç½š
        else:
            balance_reward = 0
        
        # 4. ç©ºå¤´ç›ˆåˆ©å¥–åŠ± - ç‰¹åˆ«å¥–åŠ±å–å‡ºç›ˆåˆ©
        if action_scalar < -0.3 and len(self.trades) > 0:
            last_trade = self.trades[-1]
            if last_trade['value'] < 0:  # å–å‡ºäº¤æ˜“
                # æ£€æŸ¥ä»·æ ¼æ˜¯å¦ä¸‹è·Œ
                if self.current_step > 0:
                    prev_price = self.df.iloc[self.current_step-1]['close']
                    price_change = (price - prev_price) / prev_price
                    if price_change < -0.01:  # ä»·æ ¼ä¸‹è·Œ1%
                        short_profit_reward = 0.1
                        print(f"ğŸ¯ ç©ºå¤´ç›ˆåˆ©! ä»·æ ¼ä¸‹è·Œ: {price_change*100:.1f}%")
                    else:
                        short_profit_reward = 0
                else:
                    short_profit_reward = 0
            else:
                short_profit_reward = 0
        else:
            short_profit_reward = 0
        
        # 5. äº¤æ˜“é¢‘ç‡æƒ©ç½š
        if len(self.trades) > 50:  # è¿‡å¤šäº¤æ˜“
            frequency_penalty = -0.01 * len(self.trades)
        else:
            frequency_penalty = 0
        
        # ç»„åˆå¥–åŠ±
        total_reward = (
            returns_reward * 0.3 +
            directional_reward * 0.4 +
            balance_reward * 0.1 +
            short_profit_reward * 0.3 +
            frequency_penalty * 0.1
        )
        
        return np.clip(total_reward, -10, 10)

    
    def _technical_consistency_reward(self, action):
        """æŠ€æœ¯æŒ‡æ ‡ä¸€è‡´æ€§å¥–åŠ±"""
        row = self.df.iloc[self.current_step]
        reward = 0
        
        # RSIä¿¡å·
        if row['rsi'] < 30 and action > 0:  # è¶…å–ä¹°å…¥
            reward += 0.1
        elif row['rsi'] > 70 and action < 0:  # è¶…ä¹°å–å‡º
            reward += 0.1
        
        # MACDä¿¡å·
        if (row['macd'] > row['macd_signal'] and action > 0):
            reward += 0.05
        elif (row['macd'] < row['macd_signal'] and action < 0):
            reward += 0.05
        
        return reward

class ActorNetwork(nn.Module):
    def __init__(self, state_dim, hidden_dim=256):  # å‡å°ç½‘ç»œè§„æ¨¡
        super(ActorNetwork, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.Tanh(),  # ä½¿ç”¨Tanhé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.Tanh(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.Tanh(),
        )
        
        self.mu = nn.Linear(hidden_dim // 2, 1)
        self.sigma = nn.Linear(hidden_dim // 2, 1)
        
        # åˆå§‹åŒ–
        nn.init.orthogonal_(self.mu.weight, gain=0.01)
        nn.init.constant_(self.mu.bias, 0.0)
        nn.init.orthogonal_(self.sigma.weight, gain=0.01)
        nn.init.constant_(self.sigma.bias, -1.0)  # åˆå§‹è¾ƒå°çš„æ–¹å·®
        
    def forward(self, state):
        features = self.network(state)
        mu = torch.tanh(self.mu(features))  # [-1, 1]
        sigma = F.softplus(self.sigma(features)) + 1e-6  # ç¡®ä¿æ­£å€¼
        
        return mu, sigma

class CriticNetwork(nn.Module):
    def __init__(self, state_dim, hidden_dim=256):
        super(CriticNetwork, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.Tanh(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.Tanh(),
            nn.Dropout(0.1),
            
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.Tanh(),
            
            nn.Linear(hidden_dim // 2, 1)
        )
        
    def forward(self, state):
        return self.network(state)

class PPOAgent:
    def __init__(self, state_dim, lr_actor=1e-5, lr_critic=3e-5, gamma=0.99, 
             gae_lambda=0.95, clip_epsilon=0.1, ppo_epochs=4, batch_size=64, entropy_coef=0.02):
        
        self.state_dim = state_dim
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_epsilon = clip_epsilon
        self.ppo_epochs = ppo_epochs
        self.batch_size = batch_size
        self.entropy_coef = entropy_coef
        
        # ç½‘ç»œ
        self.actor = ActorNetwork(state_dim).to(device)
        self.critic = CriticNetwork(state_dim).to(device)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor, eps=1e-6)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic, eps=1e-6)
        
        # ç»éªŒç¼“å†²åŒº
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.values = []
        self.dones = []
        
        # ç­–ç•¥å‚æ•°
        self.confidence_threshold = 0.7  # ä¿¡å¿ƒé˜ˆå€¼ï¼Œç”¨äºå¤§èƒ†ä¸‹å•
        self.exploration_decay = 0.995   # æ¢ç´¢è¡°å‡


    
    def get_action(self, state, training=True):
        """åŠ¨ä½œé€‰æ‹©"""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        
        with torch.no_grad():
            mu, sigma = self.actor(state_tensor)
            
            if (torch.isnan(mu).any() or torch.isnan(sigma).any()):
                mu = torch.zeros_like(mu)
                sigma = torch.ones_like(sigma) * 0.5
            
            # åŸºäºå¸‚åœºçŠ¶æ€è°ƒæ•´æ¢ç´¢
            if training:
                # æ£€æŸ¥å¸‚åœºæ˜¯å¦å¤„äºå–å‡ºä¿¡å·åŒºåŸŸ
                rsi = state[7] if len(state) > 7 else 50
                bb_pos = state[14] if len(state) > 14 else 0.5
                
                if rsi > 60 or bb_pos > 0.7:  # å–å‡ºä¿¡å·åŒºåŸŸ
                    # é¼“åŠ±å–å‡ºæ¢ç´¢
                    if np.random.random() < 0.3:  # 30%æ¦‚ç‡å¼ºåˆ¶å–å‡ºæ¢ç´¢
                        mu = torch.clamp(mu, max=-0.3)  # åå‘å–å‡º
                        sigma = sigma * 0.8  # å‡å°‘æ¢ç´¢
                
                sigma = torch.clamp(sigma, min=0.2, max=0.8)
            
            dist = Normal(mu, sigma)
            action = dist.sample()
            action_log_prob = dist.log_prob(action)
            value = self.critic(state_tensor)
            
        # ç¡®ä¿è¿”å›æ­£ç¡®çš„æ ¼å¼
        action_value = float(action.cpu().numpy()[0])
        action_log_prob_value = float(action_log_prob.cpu().numpy()[0])
        value_value = float(value.cpu().numpy()[0])
        
        # å¼ºåˆ¶å¤šç©ºå¹³è¡¡æ¢ç´¢
        if training:
            current_position = state[21] if len(state) > 21 else 0  # æŒä»“æ¯”ä¾‹
            
            if current_position > 0.5 and np.random.random() < 0.2:
                # æŒä»“è¿‡é«˜æ—¶ï¼Œé¼“åŠ±å–å‡º
                action_value = min(action_value, -0.3)  # åå‘å–å‡º
            elif current_position < -0.5 and np.random.random() < 0.2:
                # ç©ºå¤´è¿‡é«˜æ—¶ï¼Œé¼“åŠ±ä¹°å…¥
                action_value = max(action_value, 0.3)  # åå‘ä¹°å…¥
        
        action_value = np.clip(action_value, -1, 1)
        
        # è¿”å›æ ‡é‡åŠ¨ä½œå€¼
        return action_value, action_log_prob_value, value_value
    
    def store_transition(self, state, action, log_prob, reward, value, done):
        self.states.append(state)
        self.actions.append(float(action))
        self.log_probs.append(float(log_prob))
        self.rewards.append(float(reward))
        self.values.append(float(value))
        self.dones.append(bool(done))
    
    def compute_advantages_and_returns(self, last_value=0):
        """ä¼˜åŠ¿è®¡ç®—"""
        if len(self.rewards) == 0:
            return np.array([]), np.array([])
        
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ ‡é‡
        rewards_clean = [float(r) for r in self.rewards]
        values_clean = [float(v) for v in self.values]
        dones_clean = [bool(d) for d in self.dones]
        
        # å¢å¼ºå–å‡ºäº¤æ˜“çš„å¥–åŠ±
        enhanced_rewards = []
        for i, (reward, action) in enumerate(zip(rewards_clean, self.actions)):
            action_val = float(action)
            if action_val < -0.1:  # å–å‡ºåŠ¨ä½œ
                if reward > 0:
                    enhanced_reward = reward * 3.0  # å¤§å¹…å¥–åŠ±æˆåŠŸå–å‡º
                else:
                    enhanced_reward = reward * 0.5  # å‡è½»å¤±è´¥å–å‡ºçš„æƒ©ç½š
            else:
                enhanced_reward = reward
                
            enhanced_rewards.append(enhanced_reward)
        
        # æ ‡å‡†åŒ–å¥–åŠ±
        scaled_rewards = np.array(enhanced_rewards, dtype=np.float32)
        if len(scaled_rewards) > 1:
            reward_std = scaled_rewards.std()
            if reward_std > 0:
                scaled_rewards = scaled_rewards / (reward_std + 1e-8)
        
        # è·å–æœ€åçŠ¶æ€çš„ä»·å€¼ä¼°è®¡
        if len(self.states) > 0:
            states_tensor = torch.FloatTensor(np.array(self.states)).to(device)
            with torch.no_grad():
                last_value_tensor = self.critic(states_tensor[-1:])
                last_value = float(last_value_tensor.cpu().numpy()[0])
        else:
            last_value = 0.0
        
        advantages = []
        returns = []
        gae = 0
        
        # ç¡®ä¿valuesæ•°ç»„æ˜¯æ ‡é‡
        values = np.array(values_clean + [last_value], dtype=np.float32)
        rewards = np.array(scaled_rewards, dtype=np.float32)
        dones = np.array(dones_clean, dtype=bool)
        
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_non_terminal = 1.0 - float(dones[t])
                next_value = float(last_value)
            else:
                next_non_terminal = 1.0 - float(dones[t])
                next_value = float(values[t + 1])
            
            delta = float(rewards[t]) + self.gamma * next_value * next_non_terminal - float(values[t])
            gae = delta + self.gamma * self.gae_lambda * next_non_terminal * gae
            advantages.insert(0, float(gae))
            returns.insert(0, float(gae + values[t]))
        
        advantages = np.array(advantages, dtype=np.float32)
        returns = np.array(returns, dtype=np.float32)
        
        if len(advantages) > 1:
            adv_mean = advantages.mean()
            adv_std = advantages.std()
            if adv_std > 0:
                advantages = (advantages - adv_mean) / (adv_std + 1e-8)
        
        return advantages, returns
    
    def update(self):
        """é’ˆå¯¹å¤šç©ºå¹³è¡¡çš„PPOæ›´æ–°"""
        if len(self.states) < self.batch_size:
            return
        
        # åˆ†æå¤šç©ºè¡Œä¸º
        buy_actions = 0
        sell_actions = 0
        profitable_buys = 0
        profitable_sells = 0
        
        for i, (action, reward) in enumerate(zip(self.actions, self.rewards)):
            action = float(action)
            if action > 0.1:
                buy_actions += 1
                if reward > 0.5:
                    profitable_buys += 1
            elif action < -0.1:
                sell_actions += 1
                if reward > 0.5:
                    profitable_sells += 1
        
        total_actions = len(self.actions)
        buy_ratio = buy_actions / total_actions if total_actions > 0 else 0
        sell_ratio = sell_actions / total_actions if total_actions > 0 else 0
        buy_success_rate = profitable_buys / buy_actions if buy_actions > 0 else 0
        sell_success_rate = profitable_sells / sell_actions if sell_actions > 0 else 0
        
        print(f"ä¹°å…¥æ¯”ä¾‹: {buy_ratio:.2f}, å–å‡ºæ¯”ä¾‹: {sell_ratio:.2f}")
        print(f"ä¹°å…¥æˆåŠŸç‡: {buy_success_rate:.2f}, å–å‡ºæˆåŠŸç‡: {sell_success_rate:.2f}")
        
        # è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥
        advantages, returns = self.compute_advantages_and_returns()
        
        if len(advantages) == 0:
            return
        
        # å¯¹ä¸å¹³è¡¡ç­–ç•¥è¿›è¡Œè°ƒæ•´
        if sell_ratio < 0.1:  # å–å‡ºåŠ¨ä½œè¿‡å°‘
            print("âš ï¸ å–å‡ºåŠ¨ä½œä¸è¶³ï¼ŒåŠ å¼ºå–å‡ºå¥–åŠ±")
            # æ”¾å¤§å–å‡ºåŠ¨ä½œçš„å¥–åŠ±
            for i in range(len(self.actions)):
                if self.actions[i] < -0.1:
                    advantages[i] = advantages[i] * 2.0
        
        states = torch.FloatTensor(np.array(self.states)).to(device)
        actions = torch.FloatTensor(np.array([float(a) for a in self.actions])).to(device)
        old_log_probs = torch.FloatTensor(np.array([float(lp) for lp in self.log_probs])).to(device)
        returns = torch.FloatTensor(returns).to(device)
        advantages = torch.FloatTensor(advantages).to(device)
        
        # PPOæ›´æ–°
        for epoch in range(self.ppo_epochs):
            indices = torch.randperm(len(states))
            
            for start in range(0, len(states), self.batch_size):
                end = start + self.batch_size
                batch_indices = indices[start:end]
                
                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]
                
                # æ¼”å‘˜ç½‘ç»œæ›´æ–°
                self.actor_optimizer.zero_grad()
                
                mu, sigma = self.actor(batch_states)
                
                if (torch.isnan(mu).any() or torch.isnan(sigma).any()):
                    continue
                    
                dist = Normal(mu, sigma)
                new_log_probs = dist.log_prob(batch_actions)
                entropy = dist.entropy().mean()
                
                # å¯¹å–å‡ºåŠ¨ä½œç»™äºˆé¢å¤–å…³æ³¨
                sell_mask = (batch_actions < -0.1).float()
                sell_bonus = sell_mask.mean() * 0.2  # é¼“åŠ±å–å‡º
                
                log_ratio = new_log_probs - batch_old_log_probs
                ratio = torch.exp(torch.clamp(log_ratio, -5, 5))
                
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages
                
                actor_loss = -torch.min(surr1, surr2).mean() - self.entropy_coef * entropy + sell_bonus
                
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.3)
                self.actor_optimizer.step()
                
                # è¯„è®ºå®¶ç½‘ç»œæ›´æ–°
                self.critic_optimizer.zero_grad()
                current_values = self.critic(batch_states)
                critic_loss = F.mse_loss(current_values, batch_returns)
                
                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.3)
                self.critic_optimizer.step()
        
        self._clear_buffer()
    
    def _clear_buffer(self):
        self.states = []
        self.actions = []
        self.log_probs = []
        self.rewards = []
        self.values = []
        self.dones = []
    
    def save_model(self, path):
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
        }, path)
    
    def load_model(self, path):
        checkpoint = torch.load(path)
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic.load_state_dict(checkpoint['critic_state_dict'])

class PPOTrainer:
    def __init__(self, env, agent, max_episodes=1000, max_steps=500,  # å‡å°‘æ­¥æ•°
                 update_interval=512, save_interval=50):  # å‡å°‘æ›´æ–°é—´éš”
        self.env = env
        self.agent = agent
        self.max_episodes = max_episodes
        self.max_steps = max_steps
        self.update_interval = update_interval
        self.save_interval = save_interval
        
        self.episode_returns = []
        self.episode_lengths = []
        
    def train(self):
        """è®­ç»ƒå¾ªç¯"""
        print("å¼€å§‹è®­ç»ƒPPOäº¤æ˜“æ™ºèƒ½ä½“...")
        
        for episode in range(self.max_episodes):
            state = self.env.reset()
            episode_reward = 0
            episode_length = 0
            
            for step in range(self.max_steps):
                # è·å–åŠ¨ä½œ
                action, log_prob, value = self.agent.get_action(state)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = self.env.step(action)
                
                self.agent.store_transition(state, action, log_prob, reward, value, done)
                
                state = next_state
                episode_reward += reward
                episode_length += 1
                
                # å®šæœŸæ›´æ–°
                if len(self.agent.states) >= self.update_interval:
                    self.agent.update()
                
                if done:
                    break
            
            # æœ€ç»ˆæ›´æ–°
            if len(self.agent.states) > 0:
                self.agent.update()
            
            self.episode_returns.append(episode_reward)
            self.episode_lengths.append(episode_length)
            
            if episode % 10 == 0:
                avg_return = np.mean(self.episode_returns[-10:]) if len(self.episode_returns) >= 10 else episode_reward
                print(f"Episode {episode}, Return: {episode_reward:.2f}, "
                    f"Avg Return: {avg_return:.2f}, Length: {episode_length}, Total Value: {info.get('total_value', 0):.2f}")
            
            if episode % self.save_interval == 0 and episode > 0:
                self.agent.save_model(f"models/pth/ppo_trading_agent_{episode}.pth")
                print(f"æ¨¡å‹å·²ä¿å­˜: models/pth/ppo_trading_agent_{episode}.pth")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.agent.save_model("models/pth/ppo_trading_agent_final.pth")
        print("æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: models/pth/ppo_trading_agent_final.pth")
        
        return self.episode_returns, self.episode_lengths
    
    def plot_training_progress(self):
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(self.episode_returns)
        plt.title('Episode Returns')
        plt.xlabel('Episode')
        plt.ylabel('Return')
        
        plt.subplot(1, 2, 2)
        plt.plot(self.episode_lengths)
        plt.title('Episode Lengths')
        plt.xlabel('Episode')
        plt.ylabel('Length')
        
        plt.tight_layout()
        plt.show()

def main():
    # åŠ è½½æ•°æ®
    df = pd.read_csv('models/data/1D/BTC_USDT_1D_5years_20251130_193559.csv')
    
    # åˆ›å»ºç¯å¢ƒ
    env = TorchTradingEnvironment(df, lookback_window=30, initial_balance=10000)
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = PPOAgent(state_dim=env.state_dim)
    
    # è®­ç»ƒ
    trainer = PPOTrainer(env, agent, max_episodes=500, max_steps=200)  # å‡å°‘è®­ç»ƒè§„æ¨¡
    returns, lengths = trainer.train()
    
    # ç»˜åˆ¶è®­ç»ƒè¿›åº¦
    trainer.plot_training_progress()
    
    return agent, returns

if __name__ == "__main__":
    agent, returns = main()