import streamlit as st
import yaml
import asyncio
import sys
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import time
import threading
from datetime import timezone
from dotenv import dotenv_values
from typing import Dict, Any  

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
ROOT_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(ROOT_DIR))
ENV_PATH = ROOT_DIR / "config" / ".env.local"

from src.core.registry import FunctionRegistry
from src.core.engine import PipelineEngine
from src.core.context import PipelineContext
from src.data.api_client import DataAPIPool, get_apis_config
from src.web import utils
import src.functions.data_fetch
import src.functions.extraction
import src.functions.graph_ops
import src.functions.reporting

st.set_page_config(page_title="Pipeline Builder - Market Lens", page_icon="â›“ï¸", layout="wide")

# --- å…¨å±€ä»»åŠ¡ç®¡ç†å™¨ ---

class GlobalTaskManager:
    def __init__(self):
        self.is_running = False
        self.logs = []
        self.status_info = {"label": "Idle", "state": "idle", "expanded": False}
        self.current_step_idx = 0
        self.total_steps = 0
        self.final_report = None
        self._lock = threading.Lock()
        
    def start_task(self, pipeline_def):
        if self.is_running:
            return False
            
        self.is_running = True
        self.logs = []
        self.status_info = {"label": "Starting...", "state": "running", "expanded": True}
        self.final_report = None
        self.current_step_idx = 0
        steps = pipeline_def.get("steps", [])
        self.total_steps = len(steps)
        
        def _worker():
            asyncio.run(self._async_runner(pipeline_def))
            
        thread = threading.Thread(target=_worker, daemon=True)
        thread.start()
        return True
        
    async def _async_runner(self, pipeline_def):
        def log_callback(entry):
            with self._lock:
                ts = entry['timestamp'].split('T')[1][:8]
                msg = f"[{ts}] [{entry['level']}] {entry['message']}"
                self.logs.append(msg)
                # ä¿ç•™æœ€è¿‘ 1000 æ¡æ—¥å¿—
                if len(self.logs) > 1000:
                    self.logs.pop(0)

        context = PipelineContext(log_callback=log_callback)
        engine = PipelineEngine(context)
        
        steps = pipeline_def.get("steps", [])
        
        try:
            for i, step in enumerate(steps):
                step_id = step.get('id')
                self.current_step_idx = i + 1
                
                # æ›´æ–°çŠ¶æ€
                with self._lock:
                    self.status_info = {
                        "label": f"Executing Step {self.current_step_idx}/{self.total_steps}: **{step_id}**", 
                        "state": "running", 
                        "expanded": True
                    }
                
                # æ‰§è¡Œä»»åŠ¡
                await engine.run_task(step)
                
            # å®Œæˆ
            with self._lock:
                self.status_info = {"label": "âœ… Pipeline Execution Completed!", "state": "complete", "expanded": False}
                self.final_report = context.get("final_report_md")
                
        except Exception as e:
            with self._lock:
                self.status_info = {"label": f"âŒ Execution Failed: {str(e)}", "state": "error", "expanded": True}
                self.logs.append(f"[System] Error: {str(e)}")
        finally:
            self.is_running = False

@st.cache_resource
def get_task_manager():
    return GlobalTaskManager()

task_manager = get_task_manager()

# --- UI ç»„ä»¶ ---

st.title("Task Center & Pipeline Builder")

# ä»»åŠ¡ç›‘æ§åŒº (å§‹ç»ˆæ˜¾ç¤ºåœ¨é¡¶éƒ¨)
def render_task_monitor():
    if task_manager.is_running or task_manager.status_info["state"] != "idle":
        with st.container(border=True):
            col_status, col_ctrl = st.columns([4, 1])
            
            with col_status:
                # ä½¿ç”¨ st.status å±•ç¤ºçŠ¶æ€
                state = task_manager.status_info["state"]
                label = task_manager.status_info["label"]
                expanded = task_manager.status_info["expanded"]
                
                status_container = st.status(label, expanded=expanded, state=state)
                
                # æ˜¾ç¤ºæœ€åå‡ æ¡æ—¥å¿—
                with status_container:
                    st.write("Recent Logs:")
                    with task_manager._lock:
                        recent_logs = task_manager.logs[-10:]
                    st.code("\n".join(recent_logs) if recent_logs else "Initializing...", language="text")
            
            with col_ctrl:
                if task_manager.is_running:
                    st.caption("Running in background...")
                    if st.button("ğŸ”„ Refresh View"):
                        st.rerun()
                    # è‡ªåŠ¨åˆ·æ–°é€»è¾‘ (å®éªŒæ€§)
                    time.sleep(2)
                    st.rerun()
                else:
                    if st.button("Clear Status"):
                        task_manager.status_info["state"] = "idle"
                        st.rerun()

        # ç»“æœå±•ç¤º
        if not task_manager.is_running and task_manager.final_report:
            with st.expander("ğŸ“„ Final Report Result", expanded=True):
                st.markdown(task_manager.final_report)
                st.download_button(
                    "Download Report", 
                    task_manager.final_report, 
                    file_name=f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                )

render_task_monitor()

# åˆå§‹åŒ– Session State
if "pipeline_steps" not in st.session_state:
    st.session_state.pipeline_steps = []

# åˆå§‹åŒ– API é…ç½® (ä»…è¿è¡Œä¸€æ¬¡)
if "ingestion_apis" not in st.session_state:
    st.session_state.ingestion_apis = utils.get_default_api_sources_df()

if "expansion_tasks" not in st.session_state:
    # åˆå§‹åŒ– expansion_tasks (ç©º) â€”â€” ä½¿ç”¨å½“å‰æ”¯æŒçš„å­—æ®µ
    st.session_state.expansion_tasks = pd.DataFrame(
        columns=["enabled", "keyword", "limit", "category", "from", "to", "sortby"]
    ).astype({
        "enabled": "bool",
        "keyword": "str",
        "limit": "int",
        "category": "str",
        "from": "str",
        "to": "str",
        "sortby": "str",
    })

# åˆå§‹åŒ– .env.local Key-Value
def load_env_df():
    kv = dotenv_values(ENV_PATH) if ENV_PATH.exists() else {}
    rows = [{"key": k, "value": v or ""} for k, v in kv.items()]
    return pd.DataFrame(rows, columns=["key", "value"])

if "env_kv" not in st.session_state:
    st.session_state.env_kv = load_env_df()

# --- è¾…åŠ©å‡½æ•° ---

def execute_pipeline(pipeline_def):
    """æäº¤ä»»åŠ¡åˆ°åå°ç®¡ç†å™¨"""
    if task_manager.is_running:
        st.warning("âš ï¸ A task is already running. Please wait for it to finish.")
        return

    success = task_manager.start_task(pipeline_def)
    if success:
        st.toast("ğŸš€ Task started in background!")
        st.rerun()
    else:
        st.error("Failed to start task.")

def render_input_field(step_idx, p_name, p_info, current_inputs, step):
    """
    æ™ºèƒ½æ¸²æŸ“è¾“å…¥ç»„ä»¶
    """
    p_type = p_info.get('type', 'Any')
    p_required = p_info.get('required', False)
    default_val = p_info.get('default')
    
    label = f"{p_name}{' *' if p_required else ''}"
    help_text = f"Type: {p_type}" + (f", Default: {default_val}" if default_val else "")
    key = f"in_{step_idx}_{p_name}"
    
    current_val = current_inputs.get(p_name, default_val)
    is_ref = isinstance(current_val, str) and current_val.startswith("$")
    
    if is_ref:
         new_val = st.text_input(label + " (Variable)", value=current_val, key=key, help=help_text)
         step["inputs"][p_name] = new_val
         return

    if "bool" in p_type.lower():
        val = st.checkbox(label, value=bool(current_val) if current_val is not None else False, key=key, help=help_text)
        step["inputs"][p_name] = val
        
    elif "int" in p_type.lower():
        val = st.number_input(label, value=int(current_val) if current_val is not None else 0, step=1, key=key, help=help_text)
        step["inputs"][p_name] = int(val)
        
    elif "list" in p_type.lower() or "dict" in p_type.lower():
        val_str = str(current_val) if current_val is not None else "[]"
        new_val_str = st.text_area(label + " (JSON/List)", value=val_str, height=100, key=key, help="Enter valid JSON or $variable")
        if new_val_str.startswith("$"):
            step["inputs"][p_name] = new_val_str
        else:
            try:
                import ast
                if new_val_str.strip():
                    parsed = ast.literal_eval(new_val_str)
                    step["inputs"][p_name] = parsed
            except:
                step["inputs"][p_name] = new_val_str
    else:
        val = st.text_input(label, value=str(current_val) if current_val is not None else "", key=key, help=help_text)
        if val:
            step["inputs"][p_name] = val

# --- åœºæ™¯åŒ–æ¨¡å—æ¸²æŸ“ ---
def render_configuration_tab():
    st.header("ğŸ“š Configuration")
    source_config, source_api_pool= st.tabs(["Source Configuration","Source API Pool"])
    with source_config:
        st.subheader("Source Configuration")
        st.write("Source Configuration")
        edited_df = st.data_editor(
            st.session_state.ingestion_apis,
            num_rows="dynamic",
            use_container_width=True,
            column_config={
                "enabled": st.column_config.CheckboxColumn("Enabled"),
                "name": st.column_config.TextColumn("Source Name", required=True),
                "type": st.column_config.SelectboxColumn("API Type", options=["gnews"], required=True),
                "language": st.column_config.SelectboxColumn("Language", options=["ar", "zh", "nl", "en", "fr", "de", "el", "he", "hi", "id", "it", "ja", "ml", "mr", "no", "pt", "pa", "ro", "ru", "es", "sv", "ta", "te", "tr", "uk"]),
                "timeout": st.column_config.NumberColumn("Timeout (s)"),
                "country": st.column_config.SelectboxColumn("Country", options=["ar", "au", "br", "ca", "cn", "co", "eg", "fr", "de", "gr", "hk", "in", "id", "ie", "il", "it", "jp", "my", "mx", "nl", "no", "pk", "pe", "ph", "pt", "ro", "ru", "sg", "es", "se", "ch", "tw", "tr", "ua", "gb", "us"]),
            },
            key="ingestion_editor_main"
        )
        st.session_state.ingestion_apis = edited_df
        
        # å®æ—¶æ˜¾ç¤ºé€‰ä¸­çš„æºæ•°é‡ï¼Œå¹¶ä¿å­˜å½“å‰é€‰æ‹©
        selected_apis = edited_df[edited_df["enabled"] == True]["name"].tolist()
        st.session_state["ingestion_selected_apis"] = selected_apis
        st.caption(f"âœ… Selected Sources: {len(selected_apis)}")

        # GNews å¯é€‰å‚æ•°é…ç½®
        gnews_params = st.session_state.get("gnews_params", {})
        with st.expander("GNews å¯é€‰å‚æ•°", expanded=False):
            category = st.selectbox(
                "Category",
                ["", "general", "world", "business", "technology", "sports", "science", "health", "entertainment"],
                index=0,
                help="ç•™ç©ºåˆ™ä¸æŒ‡å®šåˆ†ç±»"
            )
            query = st.text_input("Query (å…³é”®è¯æœç´¢ï¼Œå¯ç©º)")
            col_from, col_to = st.columns(2)
            min_date = datetime(2020, 1, 1).date()
            max_date = datetime.now().date()
            with col_from:
                d_from = st.date_input("From æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_from_date")
                t_from = st.time_input("From æ—¶é—´", value=None, key="gnews_from_time")
            with col_to:
                d_to = st.date_input("To æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_to_date")
                t_to = st.time_input("To æ—¶é—´", value=None, key="gnews_to_time")

            def combine(dt, tm):
                if dt is None:
                    return None
                tm = tm or datetime.min.time()
                # ç»Ÿä¸€ä½¿ç”¨ UTC è¾“å‡º ISO8601
                return datetime.combine(dt, tm, tzinfo=timezone.utc).isoformat().replace("+00:00", "Z")

            from_iso = combine(d_from, t_from)
            to_iso = combine(d_to, t_to)
            nullable = st.text_input("Nullable", value=gnews_params.get("nullable", ""), help="å¦‚ description,content")
            truncate = st.text_input("Truncate", value=gnews_params.get("truncate", ""), help="å¦‚ content")
            sortby = st.selectbox("Sortby", ["", "publishedAt", "relevance"], index=0)
            in_fields = st.text_input("In fields", value=gnews_params.get("in_fields", ""), help="å¦‚ title,description")
            page = st.number_input("Page", min_value=1, value=gnews_params.get("page", 1), step=1)

            # ä¿å­˜åˆ° session_state
            st.session_state["gnews_params"] = {
                "category": category or None,
                "query": query or None,
                "from_": from_iso,
                "to": to_iso,
                "nullable": nullable or None,
                "truncate": truncate or None,
                "sortby": sortby or None,
                "in_fields": in_fields or None,
                "page": int(page) if page else None,
            }

    with source_api_pool:
        st.subheader("Source API Pool")
        st.caption("ç¼–è¾‘å¹¶ä¿å­˜åˆ° config/.env.localï¼ˆè¦†ç›–å†™å…¥ï¼Œä¸ä¿ç•™æ³¨é‡Š/ç©ºè¡Œï¼‰ã€‚ä¸Šæ–¹â€œä¿å­˜è¯¥è¡Œ/è¡¨æ ¼ä¿®æ”¹â€ä»…æ›´æ–°å†…å­˜ï¼Œéœ€åœ¨ä¸‹æ–¹ç‚¹å‡»ä¿å­˜æ‰å†™å…¥æ–‡ä»¶ã€‚")
        edited_env = st.session_state.env_kv
        if st.checkbox("æ˜¾ç¤º Key/Value è¡¨æ ¼ç¼–è¾‘å™¨", value=False, key="env_editor_toggle"):
            edited_env = st.data_editor(
                st.session_state.env_kv,
                num_rows="dynamic",
                use_container_width=True,
                column_config={
                    "key": st.column_config.TextColumn("Key", required=True),
                    "value": st.column_config.TextColumn("Value", required=False, help="å¯è¾“å…¥å ä½ç¬¦ï¼Œæ³¨æ„é¿å…æ³„éœ²æ•æ„Ÿå€¼")
                },
                key="env_editor"
            )
            st.session_state.env_kv = edited_env
        def try_parse_json(val: str):
            if not isinstance(val, str):
                return None, "éå­—ç¬¦ä¸²"
            txt = val.strip()
            if not txt:
                return None, "ç©ºå€¼"
            try:
                obj = json.loads(txt)
                return obj, ""
            except Exception as e:
                return None, str(e)

        for _, row in edited_env.iterrows():
            idx = row.name
            k = str(row.get("key", "")).strip()
            v = str(row.get("value", "")).strip()
            if not k:
                continue
            parsed, err = try_parse_json(v)
            with st.expander(f"{k}", expanded=False):
                if parsed is not None:
                    pretty_txt = json.dumps(parsed, ensure_ascii=False, indent=2)
                    new_txt = st.text_area(
                        "JSON ç¼–è¾‘ï¼ˆä¿å­˜å³å†™å› valueï¼‰",
                        value=pretty_txt,
                        key=f"json_edit_{idx}",
                        height=200
                    )
                    if st.button("ä¿å­˜è¯¥è¡Œ", key=f"save_json_{idx}", use_container_width=True):
                        try:
                            parsed_new = json.loads(new_txt)
                            # å†™å›è¡¨æ ¼ç¼“å­˜ï¼Œä¿æŒç´§å‡‘å­˜å‚¨
                            edited_env.at[idx, "value"] = json.dumps(parsed_new, ensure_ascii=False)
                            st.session_state.env_kv = edited_env
                            st.success("å·²æ›´æ–°è¯¥è¡Œçš„ value")
                        except Exception as e:
                            st.error(f"JSON è§£æå¤±è´¥: {e}")
                    # è¡¨æ ¼åŒ–å±•ç¤ºï¼ˆä¼˜å…ˆ list[dict] æˆ– dict -> DataFrameï¼‰ï¼Œå¦åˆ™ç”¨ json
                    def to_table(obj):
                        if isinstance(obj, list) and obj and all(isinstance(x, dict) for x in obj):
                            return pd.DataFrame(obj), "list"
                        if isinstance(obj, dict):
                            return pd.DataFrame([obj]), "dict"
                        return None, ""
                    df_preview, kind = to_table(parsed)
                    if df_preview is not None and not df_preview.empty:
                        edited_df = st.data_editor(
                            df_preview,
                            num_rows="dynamic",
                            use_container_width=True,
                            key=f"env_table_{idx}"
                        )
                        if st.button("ä¿å­˜è¡¨æ ¼ä¿®æ”¹", key=f"save_table_{idx}", use_container_width=True):
                            try:
                                if kind == "list":
                                    new_obj = edited_df.to_dict(orient="records")
                                else:
                                    new_obj = edited_df.to_dict(orient="records")[0] if not edited_df.empty else {}
                                edited_env.at[idx, "value"] = json.dumps(new_obj, ensure_ascii=False)
                                st.session_state.env_kv = edited_env
                                st.success("å·²æ ¹æ®è¡¨æ ¼ä¿®æ”¹æ›´æ–° value")
                            except Exception as e:
                                st.error(f"å†™å› JSON å¤±è´¥: {e}")
                    else:
                        st.json(parsed)
                else:
                    st.warning(f"æ— æ³•è§£æä¸º JSONï¼š{err}")

        if st.button("ğŸ’¾ ä¿å­˜åˆ° .env.local", type="primary", use_container_width=True):
            lines = []
            for _, row in edited_env.iterrows():
                k = str(row.get("key", "")).strip()
                if not k:
                    continue
                v = str(row.get("value", "")).strip()
                lines.append(f"{k}={v}")
            ENV_PATH.parent.mkdir(parents=True, exist_ok=True)
            ENV_PATH.write_text("\n".join(lines) + "\n", encoding="utf-8")
            st.success(f"å·²å†™å…¥ {ENV_PATH.name} ï¼Œå…± {len(lines)} æ¡è®°å½•")



def render_ingestion_tab():
    st.header("ğŸ“¥ Data Ingestion")
    st.caption("Fetch news from sources (Feed/Search) and extract events.")
    
    st.subheader("Processing Parameters")
    
    col_p1, col_p2 = st.columns(2)
    with col_p1:
            st.markdown("##### ğŸ“¥ Fetch Settings")
            news_limit = st.number_input("Limit (per source)", 1, 10, 5, 1, help="Max news items to fetch per source.")
            
    with col_p2:
            st.markdown("##### âš™ï¸ Pipeline Actions")
            auto_update_kg = st.checkbox("Auto Update Knowledge Graph", True, help="Automatically extract entities and update the graph.")
            enable_report = st.checkbox("Generate Summary Report", True, help="Create a markdown report after processing.")

    st.subheader("ğŸš€ Ready to Start?")
    
    # æ±‡æ€»é…ç½®
    current_df = st.session_state.ingestion_apis
    selected_sources = current_df[current_df["enabled"] == True]["name"].tolist()
    
    st.write("Summary:")
    c1, c2, c3 = st.columns(3)
    c1.metric("Sources Selected", len(selected_sources))
    c2.metric("Max Items", news_limit)
    c3.metric("Auto-Update KG", "Yes" if auto_update_kg else "No")
    gnews_params = st.session_state.get("gnews_params", {})

    st.subheader("å›¾è°±æ›´æ–°æ¨¡å¼")
    col_mode_ing, col_forms_ing = st.columns(2)
    with col_mode_ing:
        append_only_ing = st.checkbox("ä»…è¿½åŠ ï¼ˆä¸æ”¹æ—§æ•°æ®ï¼‰- Ingestion", value=True, help="ä¸ä¿®æ”¹å·²æœ‰å®ä½“/äº‹ä»¶ï¼Œåªæ–°å¢ä¸å­˜åœ¨çš„è®°å½•")
    with col_forms_ing:
        allow_append_forms_ing = st.checkbox("è¿½åŠ æ—§å®ä½“çš„ original_forms - Ingestion", value=True, help="ä»…åœ¨ä»…è¿½åŠ æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼›å…³é—­åˆ™å®Œå…¨ä¸æ”¹æ—§å®ä½“å­—æ®µ")
    
    if not selected_sources:
        st.error("âŒ No sources selected. Please go back to 'Data Sources' tab.")
        btn_disabled = True
    else:
        btn_disabled = False
        
    if st.button("Start Ingestion Task", type="primary", disabled=btn_disabled, use_container_width=True):
        pipeline_def = {
            "name": "Data Ingestion Task",
            "steps": [
                {
                    "id": "fetch_news",
                    "tool": "fetch_news_stream",
                    "inputs": {
                        "limit": news_limit, 
                        "sources": selected_sources,
                        # GNews å¯é€‰å‚æ•°é€ä¼ ï¼ˆä»…å½“æœ‰å€¼ï¼‰
                        **{k: v for k, v in gnews_params.items() if v}
                    },
                    "output": "raw_news_data"
                },
                {
                    "id": "process_news",
                    "tool": "batch_process_news",
                    "inputs": {"news_list": "$raw_news_data"},
                    "output": "extracted_events"
                },
                {
                    "id": "save_events_tmp",
                    "tool": "save_extracted_events_tmp",
                    "inputs": {"events": "$extracted_events"},
                    "output": "events_path"
                },
                {
                    "id": "update_graph_from_ingestion" if not append_only_ing else "append_graph_from_ingestion",
                    "tool": "update_graph_data" if not append_only_ing else "append_only_update_graph",
                    "inputs": {"events_list": "$extracted_events", "allow_append_original_forms": allow_append_forms_ing} if append_only_ing else {"events_list": "$extracted_events"},
                    "output": "kg_update_result_ingestion"
                },
                {
                    "id": "refresh_kg_after_ingestion",
                    "tool": "refresh_knowledge_graph",
                    "inputs": {},
                    "output": "kg_refresh_result_ingestion"
                },
                {
                    "id": "report_ingestion",
                    "tool": "generate_markdown_report",
                    "inputs": {"events_list": "$extracted_events", "title": "Ingestion Extracted Events Report"},
                    "output": "ingestion_report_md"
                }
            ]
        }
        
        if auto_update_kg:
            pipeline_def["steps"].append({
                "id": "update_kg",
                "tool": "update_graph_data",
                "inputs": {"events_list": "$extracted_events"},
                "output": "update_status"
            })
            
        if enable_report:
            pipeline_def["steps"].append({
                "id": "generate_report",
                "tool": "generate_markdown_report",
                "inputs": {"events_list": "$extracted_events", "title": f"Ingestion Report {datetime.now().strftime('%Y-%m-%d')}"},
                "output": "final_report_md"
            })
            
        execute_pipeline(pipeline_def)

def render_expansion_tab():
    st.header("ğŸ” Knowledge Expansion")
    st.caption("Search for news based on keywords to discover new entities.")
    
    st.subheader("Define Search Tasks")
    st.info("Manage search keywords. You can add entities from the Knowledge Graph or manually type new keywords in the table.")
    # é€‰æ‹©å¯ç”¨çš„æœç´¢ APIï¼ˆä¸ Configuration å…±ç”¨åŒä¸€é…ç½®ï¼‰
    selected_apis = st.session_state.ingestion_apis[st.session_state.ingestion_apis["enabled"] == True]["name"].tolist()
    
    # å·¥å…·æ ï¼šä»ä¸‹æ‹‰åˆ—è¡¨æ·»åŠ 
    entities = utils.load_entities()
    if entities:
        all_entity_names = sorted(list(entities.keys()))
        
        c_add_sel, c_add_btn = st.columns([3, 1])
        with c_add_sel:
            selected_entities = st.multiselect(
                "Select Entities from Graph", 
                options=all_entity_names,
                placeholder="Choose entities to add..."
            )
        with c_add_btn:
            st.write("") # Spacer
            st.write("") 
            if st.button("â• Add Selected", use_container_width=True):
                if selected_entities:
                    new_rows = []
                    # è·å–ç°æœ‰å…³é”®è¯ä»¥é¿å…é‡å¤
                    existing_kws = set()
                    if not st.session_state.expansion_tasks.empty:
                        existing_kws = set(st.session_state.expansion_tasks["keyword"].tolist())
                        
                    count = 0
                    for ent in selected_entities:
                        if ent not in existing_kws:
                            new_rows.append({
                                "enabled": True,
                                "keyword": ent,
                                "limit": 5,
                                "category": "general",
                                "from": "",
                                "to": "",
                                "sortby": ""
                            })
                            count += 1
                    
                    if new_rows:
                        new_df = pd.DataFrame(new_rows)
                        st.session_state.expansion_tasks = pd.concat(
                            [st.session_state.expansion_tasks, new_df], 
                            ignore_index=True
                        )
                        st.success(f"Added {count} new tasks!")
                        st.rerun()
                    else:
                        st.warning("Selected entities are already in the list.")
                else:
                    st.warning("Please select entities first.")
    
    # æ—¥æœŸæ—¶é—´å¿«æ·å¡«å……ï¼ˆå¯é€‰ï¼Œä½¿ç”¨æ—¥å†é€‰æ‹©å™¨ç”Ÿæˆ ISO8601 å­—ç¬¦ä¸²å¹¶æ‰¹é‡å¡«å……ï¼‰
    with st.expander("æ—¥æœŸæ—¶é—´å¿«æ·å¡«å……ï¼ˆå¯é€‰ï¼‰", expanded=False):
        col_from, col_to = st.columns(2)
        min_date = datetime(2020, 1, 1).date()
        max_date = datetime.now().date()
        with col_from:
            d_from = st.date_input("From æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_from_date_expansion_picker")
            t_from = st.time_input("From æ—¶é—´", value=None, key="gnews_from_time_expansion_picker")
        with col_to:
            d_to = st.date_input("To æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_to_date_expansion_picker")
            t_to = st.time_input("To æ—¶é—´", value=None, key="gnews_to_time_expansion_picker")

        def combine(dt, tm):
            if dt is None:
                return None
            tm = tm or datetime.min.time()
            return datetime.combine(dt, tm, tzinfo=timezone.utc).isoformat().replace("+00:00", "Z")

        from_iso = combine(d_from, t_from)
        to_iso = combine(d_to, t_to)
        st.caption(f"From (ISO8601): {from_iso or 'æœªè®¾ç½®'}")
        st.caption(f"To   (ISO8601): {to_iso or 'æœªè®¾ç½®'}")

        apply_from_all = st.checkbox("å°† From å¡«å……åˆ°æ‰€æœ‰è¡Œï¼ˆè‹¥è®¾ç½®ï¼‰", value=False, key="apply_from_all_expansion")
        apply_to_all = st.checkbox("å°† To å¡«å……åˆ°æ‰€æœ‰è¡Œï¼ˆè‹¥è®¾ç½®ï¼‰", value=False, key="apply_to_all_expansion")

        if st.button("åº”ç”¨åˆ°ä»»åŠ¡è¡¨", type="primary", key="apply_datetime_expansion"):
            df = st.session_state.expansion_tasks.copy()
            if from_iso:
                if apply_from_all:
                    df["from"] = from_iso
                else:
                    df.loc[(df["from"].isna()) | (df["from"] == ""), "from"] = from_iso
            if to_iso:
                if apply_to_all:
                    df["to"] = to_iso
                else:
                    df.loc[(df["to"].isna()) | (df["to"] == ""), "to"] = to_iso
            st.session_state.expansion_tasks = df
            st.success("å·²åº”ç”¨åˆ°ä»»åŠ¡è¡¨ï¼Œè¯·åœ¨ä¸‹æ–¹è¡¨æ ¼ç¡®è®¤ã€‚")
            st.rerun()

    # ä»»åŠ¡è¡¨æ ¼ç¼–è¾‘å™¨
    edited_tasks = st.data_editor(
        st.session_state.expansion_tasks,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "enabled": st.column_config.CheckboxColumn("Enabled"),
            "keyword": st.column_config.TextColumn("Keyword", required=True, help="Type manually or added from dropdown"),
            "limit": st.column_config.NumberColumn("Limit", min_value=1, max_value=10, default=5),
            "category": st.column_config.TextColumn("Category", help="GNews categoryï¼Œå¦‚ general/business/...ï¼Œå¯ç©º"),
            "from": st.column_config.TextColumn("From (ISO8601)", help="å¯ç©ºï¼Œå¦‚ 2025-12-01T00:00:00Z"),
            "to": st.column_config.TextColumn("To (ISO8601)", help="å¯ç©ºï¼Œå¦‚ 2025-12-31T23:59:59Z"),
            "sortby": st.column_config.SelectboxColumn("Sortby", options=["", "publishedAt", "relevance"]),
        },
        key="expansion_tasks_editor"
    )
    st.session_state.expansion_tasks = edited_tasks

    st.subheader("ğŸš€ Run Expansion")
    
    # è¿‡æ»¤å‡ºå¯ç”¨çš„ä»»åŠ¡
    active_tasks = st.session_state.expansion_tasks[st.session_state.expansion_tasks["enabled"] == True]
    
    c1, c2 = st.columns(2)
    c1.metric("Selected APIs", len(selected_apis))
    c2.metric("Active Tasks", len(active_tasks))

    st.subheader("å›¾è°±æ›´æ–°æ¨¡å¼")
    col_mode, col_forms = st.columns(2)
    with col_mode:
        append_only_mode = st.checkbox("ä»…è¿½åŠ ï¼ˆä¸æ”¹æ—§æ•°æ®ï¼‰", value=True, help="ä¸ä¿®æ”¹å·²æœ‰å®ä½“/äº‹ä»¶ï¼Œåªæ–°å¢ä¸å­˜åœ¨çš„è®°å½•")
    with col_forms:
        allow_append_forms = st.checkbox("è¿½åŠ æ—§å®ä½“çš„ original_forms", value=True, help="ä»…åœ¨ä»…è¿½åŠ æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼›å…³é—­åˆ™å®Œå…¨ä¸æ”¹æ—§å®ä½“å­—æ®µ")
    
    if st.button("Start Expansion Task", type="primary", use_container_width=True):
        if not selected_apis:
            st.error("Please select at least one Search API.")
            return
        if active_tasks.empty:
            st.error("Please define and enable at least one Search Task.")
            return
            
        # æ„å»º Pipelineï¼šä¸ºæ¯ä¸ªå¯ç”¨ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªæ­¥éª¤
        pipeline_steps = []
        for idx, row in active_tasks.iterrows():
            kw = row["keyword"]
            step_id = f"search_{kw.replace(' ', '_')}_{idx}"
            
            pipeline_steps.append({
                    "id": step_id,
                    "tool": "search_news_by_keywords", 
                    "inputs": {
                        "keywords": [kw], # å·¥å…·æœŸæœ›åˆ—è¡¨
                        "apis": selected_apis,
                        "limit": int(row.get("limit", 50)),
                        "category": row.get("category") or None,
                        "from": row.get("from") or None,
                        "to": row.get("to") or None,
                        "sortby": row.get("sortby") or None
                    },
                    "output": f"results_{idx}"
            })
            # å…ˆå¯¹æ‹“å±•ç»“æœåšäº‹ä»¶æå–
            pipeline_steps.append({
                "id": f"extract_{kw.replace(' ', '_')}_{idx}",
                "tool": "batch_process_news",
                "inputs": {"news_list": f"$results_{idx}"},
                "output": f"extracted_events_{idx}"
            })
            # å°†æå–çš„äº‹ä»¶æš‚å­˜åˆ° tmpï¼Œæ–¹ä¾¿åç»­é¢„è§ˆ
            pipeline_steps.append({
                "id": f"save_events_{kw.replace(' ', '_')}_{idx}",
                "tool": "save_extracted_events_tmp",
                "inputs": {"events": f"$extracted_events_{idx}"},
                "output": f"events_path_{idx}"
            })
            pipeline_steps.append({
                "id": f"persist_{kw.replace(' ', '_')}_{idx}",
                "tool": "persist_expanded_news_tmp",
                "inputs": {
                    "expanded_news": f"$results_{idx}"
                },
                "output": f"persist_result_{idx}"
            })
        # æ±‡æ€»æ‰€æœ‰ä»»åŠ¡çš„æå–ç»“æœç”¨äºåç»­æ›´æ–°/æŠ¥å‘Š
        all_extracted_keys = [f"$extracted_events_{i}" for i in range(len(active_tasks))]
        if append_only_mode:
            pipeline_steps.append({
                "id": "append_graph_from_expansion",
                "tool": "append_only_update_graph",
                "inputs": {
                    "events_list": all_extracted_keys,
                    "allow_append_original_forms": allow_append_forms
                },
                "output": "kg_update_result"
            })
        else:
            pipeline_steps.append({
                "id": "update_graph_from_expansion",
                "tool": "update_graph_data",
                "inputs": {"events_list": all_extracted_keys},
                "output": "kg_update_result"
            })
        pipeline_steps.append({
            "id": "refresh_kg_after_expansion",
            "tool": "refresh_knowledge_graph",
            "inputs": {},
            "output": "kg_refresh_result"
        })
        pipeline_steps.append({
            "id": "report_expansion",
            "tool": "generate_markdown_report",
            "inputs": {
                "events_list": all_extracted_keys,
                "title": "Expansion Extracted Events Report"
            },
            "output": "expansion_report_md"
        })
        
        pipeline_def = {
            "name": "Knowledge Expansion Batch",
            "steps": pipeline_steps
        }
        execute_pipeline(pipeline_def)

def render_maintenance_tab():
    st.header("ğŸ•¸ï¸ Graph Maintenance")
    
    # å½“å‰ä¸´æ—¶æ•°æ®å¿«ç…§ï¼ˆå®ä½“/äº‹ä»¶ï¼‰
    data_dir = ROOT_DIR / "data"
    entities_tmp_file = data_dir / "tmp" / "entities_tmp.json"
    events_tmp_file = data_dir / "tmp" / "abstract_to_event_map_tmp.json"
    extracted_dir = data_dir / "tmp"
    deduped_dir = data_dir / "tmp" / "deduped_news"
    raw_dir = data_dir / "tmp" / "raw_news"

    @st.cache_data(ttl=60)
    def load_json_cached(path: Path):
        try:
            if path.exists():
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            st.warning(f"{path.name} è¯»å–å¤±è´¥: {e}")
        return {}

    entities_tmp = load_json_cached(entities_tmp_file)
    events_tmp = load_json_cached(events_tmp_file)
    @st.cache_data(ttl=60)
    def list_extracted_files(base: Path):
        files = sorted(base.glob("extracted_events_*.jsonl"), key=lambda x: x.stat().st_mtime, reverse=True)
        return [str(f) for f in files]
    extracted_files = list_extracted_files(extracted_dir)

    @st.cache_data(ttl=60)
    def list_news_files(base: Path, pattern: str):
        files = sorted(base.glob(pattern), key=lambda x: x.stat().st_mtime, reverse=True)
        return [str(f) for f in files]
    deduped_files = list_news_files(deduped_dir, "*.jsonl")
    raw_files = list_news_files(raw_dir, "*.jsonl")

    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("ä¸´æ—¶å®ä½“ï¼ˆç¼“å­˜æ¡æ•°ï¼‰", len(entities_tmp))
    c2.metric("ä¸´æ—¶äº‹ä»¶ï¼ˆç¼“å­˜æ¡æ•°ï¼‰", len(events_tmp))
    c3.metric("æå–ç»“æœæ–‡ä»¶æ•°", len(extracted_files))
    c4.metric("å»é‡æ–°é—»æ–‡ä»¶æ•°", len(deduped_files))
    c5.metric("åŸå§‹æ–°é—»æ–‡ä»¶æ•°", len(raw_files))

    with st.expander("æŸ¥çœ‹ä¸´æ—¶å®ä½“ / äº‹ä»¶ç¤ºä¾‹", expanded=False):
        if entities_tmp:
            df_ent = pd.DataFrame(
                [
                    {
                        "name": k,
                        "first_seen": v.get("first_seen", ""),
                        "sources": ",".join(
                            [
                                (
                                    s.get("name")
                                    or s.get("id")
                                    or s.get("url")
                                    or str(s)
                                )
                                if isinstance(s, dict)
                                else str(s)
                                for s in v.get("sources", [])
                            ]
                        )[:80],
                    }
                    for k, v in list(entities_tmp.items())[:50]
                ]
            )
            st.write("ä¸´æ—¶å®ä½“ï¼ˆæœ€å¤š50æ¡é¢„è§ˆï¼‰")
            st.dataframe(df_ent, use_container_width=True)
        else:
            st.info("æš‚æ— ä¸´æ—¶å®ä½“æ•°æ®")

        if events_tmp:
            df_evt = pd.DataFrame(
                [{"abstract": k, "first_seen": v.get("first_seen", ""), "entities": ",".join(v.get("entities", []))[:80]} for k, v in list(events_tmp.items())[:50]]
            )
            st.write("ä¸´æ—¶äº‹ä»¶ï¼ˆæœ€å¤š50æ¡é¢„è§ˆï¼‰")
            st.dataframe(df_evt, use_container_width=True)
        else:
            st.info("æš‚æ— ä¸´æ—¶äº‹ä»¶æ•°æ®")
        
        if extracted_files:
            st.write("æå–ç»“æœæ–‡ä»¶ï¼ˆæœ€æ–°5ä¸ªï¼‰")
            st.table({"path": extracted_files[:5]})
        else:
            st.info("æš‚æ— æå–ç»“æœæ–‡ä»¶")

        if deduped_files:
            st.write("å»é‡æ–°é—»æ–‡ä»¶ï¼ˆæœ€æ–°5ä¸ªï¼‰")
            st.table({"path": deduped_files[:5]})
        else:
            st.info("æš‚æ— å»é‡æ–°é—»æ–‡ä»¶")

        if raw_files:
            st.write("åŸå§‹æ–°é—»æ–‡ä»¶ï¼ˆæœ€æ–°5ä¸ªï¼‰")
            st.table({"path": raw_files[:5]})
        else:
            st.info("æš‚æ— åŸå§‹æ–°é—»æ–‡ä»¶")
    
    with st.form("maintenance_form"):
        c1, c2 = st.columns(2)
        with c1:
            st.subheader("Deduplication")
            strict = st.checkbox("Strict Mode", True)
            thresh = st.slider("Similarity", 0.5, 1.0, 0.9)
        with c2:
            st.subheader("Cleaning")
            rm_iso = st.checkbox("Remove Isolated Nodes")
        
        st.subheader("å¯¼å…¥ tmp æŠ½å–ç»“æœ")
        use_tmp_events = st.checkbox("åˆ·æ–°å‰å…ˆè¿½åŠ  tmp/extracted_events_*.jsonl", value=True)
        max_tmp_files = st.number_input("æœ€å¤šè¯»å–æ–‡ä»¶æ•°ï¼ˆ0=å…¨éƒ¨ï¼‰", min_value=0, value=0, step=1)
        allow_forms_tmp = st.checkbox("è¿½åŠ æ—§å®ä½“ original_formsï¼ˆè¿½åŠ æ¨¡å¼ï¼‰", value=True)
            
        submitted = st.form_submit_button("ğŸš€ Run Maintenance", type="primary", use_container_width=True)
        
    if submitted:
        pipeline_def = {
            "name": "Graph Maintenance",
            # è°ƒç”¨ Agent3 åˆ·æ–°å‹ç¼©çŸ¥è¯†å›¾è°±ï¼ˆåŸºäºç°æœ‰å®ä½“/äº‹ä»¶æ–‡ä»¶ï¼‰
            "steps": [
                *(
                    [
                        {
                            "id": "append_tmp_events",
                            "tool": "append_tmp_extracted_events",
                            "inputs": {
                                "max_files": int(max_tmp_files),
                                "allow_append_original_forms": allow_forms_tmp
                            },
                            "output": "tmp_append_result"
                        }
                    ]
                    if use_tmp_events else []
                ),
                {
                    "id": "refresh_kg",
                    "tool": "refresh_knowledge_graph",
                    "inputs": {},
                    "output": "status"
                }
            ]
        }
        execute_pipeline(pipeline_def)
        # æ¸…ç†ä¸´æ—¶ç¼“å­˜æ–‡ä»¶å¹¶åˆ·æ–°ç¼“å­˜
        try:
            for p in [entities_tmp_file, events_tmp_file]:
                if p.exists():
                    p.unlink()
            st.cache_data.clear()
            st.success("å·²æ¸…ç†ä¸´æ—¶ç¼“å­˜æ–‡ä»¶")
        except Exception as e:
            st.warning(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

def render_custom_builder():
    st.header("ğŸ› ï¸ Custom Pipeline Builder")
    
    col_builder, col_preview = st.columns([1.5, 1])
    
    with col_builder:
        # å·¥å…·æ 
        c_add, c_save, c_load = st.columns([2, 1, 1])
        with c_add:
            tools = FunctionRegistry.get_all_tools()
            selected_tool = st.selectbox("Select Tool", list(tools.keys()), label_visibility="collapsed")
            if st.button("Add Step", use_container_width=True):
                 st.session_state.pipeline_steps.append({
                    "id": f"step_{len(st.session_state.pipeline_steps) + 1}",
                    "tool": selected_tool,
                    "inputs": {}
                })
                 st.rerun()

        # æ­¥éª¤ç¼–è¾‘
        if not st.session_state.pipeline_steps:
            st.info("No steps added. Select a tool to start.")
        else:
            for i, step in enumerate(st.session_state.pipeline_steps):
                tool_name = step["tool"]
                tool_meta = tools.get(tool_name, {})
                
                with st.expander(f"Step {i+1}: {tool_name}", expanded=False):
                    c_id, c_del = st.columns([4, 1])
                    step["id"] = c_id.text_input("ID", step["id"], key=f"id_{i}")
                    if c_del.button("ğŸ—‘ï¸", key=f"del_{i}"):
                        st.session_state.pipeline_steps.pop(i)
                        st.rerun()
                        
                    st.caption(tool_meta.get("description", ""))
                    
                    # å‚æ•°ç¼–è¾‘åŒº
                    params = tool_meta.get("parameters", {})
                    if params:
                        for p_name, p_info in params.items():
                            render_input_field(i, p_name, p_info, step.get("inputs", {}), step)
                    
                    step["output"] = st.text_input("Output to ($var)", step.get("output", ""), key=f"out_{i}")

    with col_preview:
        st.subheader("Preview")
        pipeline_def = {"name": "Custom Pipeline", "steps": st.session_state.pipeline_steps}
        st.code(yaml.dump(pipeline_def, sort_keys=False), language="yaml")
        
        if st.button("ğŸš€ Run Pipeline", type="primary", use_container_width=True):
            execute_pipeline(pipeline_def)

def render_snapshots_tab():
    st.header("ğŸ“¸ Knowledge Graph Snapshots")
    st.caption("ç”Ÿæˆ/æŸ¥çœ‹å¯è§†åŒ–å¿«ç…§ï¼ˆkg_visual.json / kg_visual_timeline.jsonï¼‰")
    if st.button("ç”Ÿæˆå¿«ç…§", type="primary"):
        try:
            from src.functions.graph_ops import generate_kg_visual_snapshots
            res = generate_kg_visual_snapshots()
            st.success(f"ç”Ÿæˆå®Œæˆ: {res}")
        except Exception as e:
            st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
    data_root = ROOT_DIR / "data"
    vis_path = data_root / "kg_visual.json"
    tl_path = data_root / "kg_visual_timeline.json"
    st.write("å¿«ç…§æ–‡ä»¶è·¯å¾„ï¼š")
    st.write(f"- å›¾è°±å¿«ç…§: {vis_path}")
    st.write(f"- æ—¶é—´çº¿å¿«ç…§: {tl_path}")
    for p in [vis_path, tl_path]:
        if p.exists():
            ts = datetime.fromtimestamp(p.stat().st_mtime).strftime("%Y-%m-%d %H:%M:%S")
            st.info(f"{p.name} å·²å­˜åœ¨ï¼Œå¤§å° {p.stat().st_size} å­—èŠ‚ï¼Œä¿®æ”¹æ—¶é—´ {ts}")
        else:
            st.warning(f"{p.name} å°šæœªç”Ÿæˆ")


def render_tool_explorer_tab():
    st.header("Tool Explorer")
    st.caption("è‡ªåŠ¨å‘ç°æ‰€æœ‰æ³¨å†Œå·¥å…· Â· æ”¯æŒæœç´¢ã€é¢„è§ˆã€å¤åˆ¶ã€ä¸€é”®æ‰§è¡Œ")

    # 1. è‡ªåŠ¨åŠ è½½æ‰€æœ‰çœŸå®å·¥å…·ï¼ˆæ ¸å¿ƒï¼ï¼‰
    all_tools = FunctionRegistry.get_all_tools()  # <-- ä½ çš„çœŸå®æ³¨å†Œè¡¨
    if not all_tools:
        st.warning("æœªæ£€æµ‹åˆ°å·²æ³¨å†Œçš„å·¥å…·ï¼Œè¯·æ£€æŸ¥ FunctionRegistry")
        return

    # 2. å¯é€‰ï¼šç»™å·¥å…·æ‰“ tag åˆ†ç»„ï¼ˆæ¨èåœ¨å‡½æ•°ä¸ŠåŠ  @tool è£…é¥°å™¨æ—¶é¡ºä¾¿åŠ  categoryï¼‰
    #    å¦‚æœä½ è¿˜æ²¡åŠ ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªé»˜è®¤åˆ†ç»„é€»è¾‘
    CATEGORY_ORDER = {
        "Data Fetch": ["fetch", "search", "scrape", "crawl"],
        "Extraction": ["extract", "process", "parse", "llm"],
        "Graph Ops": ["graph", "update", "refresh", "merge", "kg", "node", "edge"],
        "Reporting": ["report", "markdown", "summary", "export"],
        "Utility": ["save", "load", "tmp", "debug", "test"],
    }

    def get_category(tool_name: str) -> str:
        name_lower = tool_name.lower()
        for cat, keywords in CATEGORY_ORDER.items():
            if any(k in name_lower for k in keywords):
                return cat
        return "Other"

    # æ·»åŠ åˆ†ç±»
    categorized = {}
    for name, meta in all_tools.items():
        cat = meta.get("category") or get_category(name)  # æ”¯æŒæ‰‹åŠ¨ category
        categorized.setdefault(cat, []).append((name, meta))

    # 3. æœç´¢æ¡†
    search = st.text_input("Search Tools", placeholder="è¾“å…¥å·¥å…·åæˆ–æè¿°å…³é”®è¯...", key="tool_search")
    
    # è¿‡æ»¤
    if search:
        filtered = {}
        for cat, tools in categorized.items():
            matched = []
            for name, meta in tools:
                if (search.lower() in name.lower() or 
                    (meta.get("description") and search.lower() in meta.get("description", "").lower())):
                    matched.append((name, meta))
            if matched:
                filtered[cat] = matched
        categorized = filtered

    # 4. ä¸»æ¸²æŸ“åŒº - å“åº”å¼å¡ç‰‡æµ
    for category, tools in categorized.items():
        with st.expander(f"**{category}** Â· {len(tools)} tools", expanded=True):
            cols = st.columns(3, gap="medium")  # æ¯è¡Œ3ä¸ªå¡ç‰‡ï¼Œå¯æ”¹æˆ2æˆ–4
            for idx, (tool_name, meta) in enumerate(tools):
                with cols[idx % 3]:
                    with st.container(border=True):
                        st.markdown(f"**`{tool_name}`**")
                        
                        desc = meta.get("description") or "No description"
                        st.caption(desc)

                        # å‚æ•°è¡¨å•ï¼ˆå¯ç¼–è¾‘ï¼‰
                        params = meta.get("parameters", {})
                        if params:
                            with st.form(key=f"form_{tool_name}_{idx}", clear_on_submit=False, border=False):
                                inputs = {}
                                for p_name, p_info in params.items():
                                    p_type = p_info.get("type", "str")
                                    default = p_info.get("default")
                                    desc = p_info.get("description", "")
                                    
                                    label = f"{p_name}{' *' if p_info.get('required') else ''}"
                                    
                                    if p_type == "bool":
                                        val = st.checkbox(label, value=bool(default), help=desc)
                                    elif p_type in ("int", "integer"):
                                        val = st.number_input(label, value=int(default) if default is not None else 0, step=1, help=desc)
                                    elif p_type == "float":
                                        val = st.number_input(label, value=float(default) if default is not None else 0.0, step=0.1, help=desc)
                                    elif p_type in ("list", "dict", "json"):
                                        val_str = json.dumps(default, ensure_ascii=False) if default is not None else "[]"
                                        val_input = st.text_area(label, value=val_str, height=80, help=desc + "\næ”¯æŒ JSON æˆ– $å˜é‡å¼•ç”¨")
                                        if val_input.strip().startswith("$"):
                                            inputs[p_name] = val_input.strip()
                                        else:
                                            try:
                                                inputs[p_name] = json.loads(val_input) if val_input.strip() else None
                                            except:
                                                inputs[p_name] = val_input  # å…è®¸å­—ç¬¦ä¸²
                                    else:
                                        val = st.text_input(label, value=str(default) if default is not None else "", help=desc)
                                        inputs[p_name] = val if val else None

                                col_run, col_copy = st.columns([1, 2])
                                with col_run:
                                    run_now = st.form_submit_button("Run", type="primary", use_container_width=True)
                                with col_copy:
                                    copy_step = st.form_submit_button("Copy as Step", use_container_width=True)

                                # ä¸€é”®è¿è¡Œï¼ˆè°ƒè¯•ç¥å™¨ï¼ï¼‰
                                if run_now:
                                    with st.spinner(f"Running {tool_name}..."):
                                        try:
                                            context = PipelineContext()
                                            engine = PipelineEngine(context)
                                            result = asyncio.run(engine.run_task({
                                                "id": f"debug_{tool_name}",
                                                "tool": tool_name,
                                                "inputs": inputs
                                            }))
                                            st.success("Success!")
                                            st.json(result, expanded=False)
                                        except Exception as e:
                                            st.error(f"Failed: {e}")

                                # ä¸€é”®å¤åˆ¶ä¸º Pipeline Step
                                if copy_step:
                                    step_yaml = {
                                        "id": tool_name,
                                        "tool": tool_name,
                                        "inputs": inputs,
                                        "output": f"{tool_name}_result"
                                    }
                                    yaml_str = yaml.dump(step_yaml, sort_keys=False, allow_unicode=True)
                                    st.code(yaml_str, language="yaml")
                                    st.toast("å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼ˆæ¨¡æ‹Ÿï¼‰", icon="clipboard")
                                    # çœŸå®å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼ˆStreamlit 1.30+ï¼‰
                                    try:
                                        st.code(yaml_str, language="yaml")
                                        st.success("Step YAML å·²ç”Ÿæˆï¼Œå¯ç›´æ¥ç²˜è´´åˆ° Custom Builder")
                                    except:
                                        pass
                        else:
                            st.info("No parameters")

                        # åº•éƒ¨æ ‡ç­¾
                        tags = []
                        if meta.get("async"): tags.append("async")
                        if "llm" in tool_name.lower(): tags.append("LLM")
                        if tags:
                            st.caption(" Â· ".join(tags))

# --- ä¸»å¯¼èˆª ---
tabs = st.tabs(["Configuration","Ingestion", "Expansion", "Maintenance", "Snapshots", "Tools", "Custom Builder"])

with tabs[0]: render_configuration_tab()
with tabs[1]: render_ingestion_tab()
with tabs[2]: render_expansion_tab()
with tabs[3]: render_maintenance_tab()
with tabs[4]: render_snapshots_tab()
with tabs[5]: render_tool_explorer_tab()
with tabs[6]: render_custom_builder()

