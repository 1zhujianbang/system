import streamlit as st
import yaml
import asyncio
import sys
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import time
import threading
from datetime import timezone
from dotenv import dotenv_values
from typing import Dict, Any  
from src.utils.tool_function import tools
tools = tools()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
ROOT_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(ROOT_DIR))
ENV_PATH = ROOT_DIR / "config" / ".env.local"

from src.core.registry import FunctionRegistry
from src.core.engine import PipelineEngine
from src.core.context import PipelineContext
from src.web import utils
import src.functions.data_fetch
import src.functions.extraction
import src.functions.graph_ops
import src.functions.reporting

st.set_page_config(page_title="æ–°é—»æ™ºèƒ½ä½“ç³»ç»Ÿ - æµæ°´çº¿æ„å»ºå™¨", page_icon="â›“ï¸", layout="wide")

# --- å…¨å±€ä»»åŠ¡ç®¡ç†å™¨ ---

class GlobalTaskManager:
    def __init__(self):
        self.is_running = False
        self.logs = []
        self.status_info = {"label": "Idle", "state": "idle", "expanded": False}
        self.current_step_idx = 0
        self.total_steps = 0
        self.final_report = None
        self._lock = threading.Lock()
        
    def start_task(self, pipeline_def):
        if self.is_running:
            return False
            
        self.is_running = True
        self.logs = []
        self.status_info = {"label": "Starting...", "state": "running", "expanded": True}
        self.final_report = None
        self.current_step_idx = 0
        steps = pipeline_def.get("steps", [])
        self.total_steps = len(steps)
        
        def _worker():
            asyncio.run(self._async_runner(pipeline_def))
            
        thread = threading.Thread(target=_worker, daemon=True)
        thread.start()
        return True
        
    async def _async_runner(self, pipeline_def):
        def log_callback(entry):
            with self._lock:
                ts = entry['timestamp'].split('T')[1][:8]
                msg = f"[{ts}] [{entry['level']}] {entry['message']}"
                self.logs.append(msg)
                # ä¿ç•™æœ€è¿‘ 1000 æ¡æ—¥å¿—
                if len(self.logs) > 1000:
                    self.logs.pop(0)

        context = PipelineContext(log_callback=log_callback)
        engine = PipelineEngine(context)
        
        steps = pipeline_def.get("steps", [])
        
        try:
            for i, step in enumerate(steps):
                step_id = step.get('id')
                self.current_step_idx = i + 1
                
                # æ›´æ–°çŠ¶æ€
                with self._lock:
                    self.status_info = {
                        "label": f"Executing Step {self.current_step_idx}/{self.total_steps}: **{step_id}**", 
                        "state": "running", 
                        "expanded": True
                    }
                
                # æ‰§è¡Œä»»åŠ¡
                await engine.run_task(step)
                
            # å®Œæˆ - æ›´æ–°å†å²è®°å½•çŠ¶æ€
            with self._lock:
                self.status_info = {"label": "âœ… Pipeline Execution Completed!", "state": "complete", "expanded": False}
                self.final_report = context.get("final_report_md")

                # æ›´æ–°å†å²è®°å½•
                import streamlit as st
                if "pipeline_history" in st.session_state and st.session_state.pipeline_history:
                    # æ›´æ–°æœ€æ–°è®°å½•çš„çŠ¶æ€
                    st.session_state.pipeline_history[-1]["status"] = "success"
                
        except Exception as e:
            with self._lock:
                self.status_info = {"label": f"âŒ Execution Failed: {str(e)}", "state": "error", "expanded": True}
                self.logs.append(f"[System] Error: {str(e)}")

                # æ›´æ–°å†å²è®°å½•
                import streamlit as st
                if "pipeline_history" in st.session_state and st.session_state.pipeline_history:
                    # æ›´æ–°æœ€æ–°è®°å½•çš„çŠ¶æ€
                    st.session_state.pipeline_history[-1]["status"] = "failed"
        finally:
            self.is_running = False

@st.cache_resource
def get_task_manager():
    return GlobalTaskManager()

task_manager = get_task_manager()

# --- UI ç»„ä»¶ ---

st.title("Task Center & Pipeline Builder")

# ä»»åŠ¡ç›‘æ§åŒº (å§‹ç»ˆæ˜¾ç¤ºåœ¨é¡¶éƒ¨)
def render_task_monitor():
    if task_manager.is_running or task_manager.status_info["state"] != "idle":
        with st.container(border=True):
            col_status, col_ctrl = st.columns([4, 1])
            
            with col_status:
                # ä½¿ç”¨ st.status å±•ç¤ºçŠ¶æ€
                state = task_manager.status_info["state"]
                label = task_manager.status_info["label"]
                expanded = task_manager.status_info["expanded"]
                
                status_container = st.status(label, expanded=expanded, state=state)
                
                # æ˜¾ç¤ºæœ€åå‡ æ¡æ—¥å¿—
                with status_container:
                    st.write("Recent Logs:")
                    with task_manager._lock:
                        recent_logs = task_manager.logs[-10:]
                    st.code("\n".join(recent_logs) if recent_logs else "Initializing...", language="text")
            
            with col_ctrl:
                if task_manager.is_running:
                    st.caption("Running in background...")
                    if st.button("ğŸ”„ Refresh View", key="pipeline_refresh_view_monitor"):
                        st.rerun()
                    # è‡ªåŠ¨åˆ·æ–°é€»è¾‘ (å®éªŒæ€§)
                    time.sleep(2)
                    st.rerun()
                else:
                    if st.button("Clear Status", key="pipeline_clear_status_monitor"):
                        task_manager.status_info["state"] = "idle"
                        st.rerun()

        # ç»“æœå±•ç¤º
        if not task_manager.is_running and task_manager.final_report:
            with st.expander("ğŸ“„ Final Report Result", expanded=True):
                st.markdown(task_manager.final_report)
                st.download_button(
                    "Download Report", 
                    task_manager.final_report, 
                    file_name=f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                )

render_task_monitor()

# åˆå§‹åŒ– Session State
if "pipeline_steps" not in st.session_state:
    st.session_state.pipeline_steps = []

# åˆå§‹åŒ– API é…ç½® (ä»…è¿è¡Œä¸€æ¬¡)
if "ingestion_apis" not in st.session_state:
    st.session_state.ingestion_apis = utils.get_default_api_sources_df()

if "expansion_tasks" not in st.session_state:
    # åˆå§‹åŒ– expansion_tasks (ç©º) â€”â€” ä½¿ç”¨å½“å‰æ”¯æŒçš„å­—æ®µ
    st.session_state.expansion_tasks = pd.DataFrame(
        columns=["enabled", "keyword", "limit", "category", "from", "to", "sortby"]
    ).astype({
        "enabled": "bool",
        "keyword": "str",
        "limit": "int",
        "category": "str",
        "from": "str",
        "to": "str",
        "sortby": "str",
    })

# åˆå§‹åŒ– .env.local Key-Value
def load_env_df():
    kv = dotenv_values(ENV_PATH) if ENV_PATH.exists() else {}
    rows = [{"key": k, "value": v or ""} for k, v in kv.items()]
    return pd.DataFrame(rows, columns=["key", "value"])

if "env_kv" not in st.session_state:
    st.session_state.env_kv = load_env_df()

# --- è¾…åŠ©å‡½æ•° ---

def execute_pipeline(pipeline_def):
    """æäº¤ä»»åŠ¡åˆ°åå°ç®¡ç†å™¨"""
    if task_manager.is_running:
        st.warning("âš ï¸ å·²æœ‰ä»»åŠ¡æ­£åœ¨è¿è¡Œã€‚è¯·ç­‰å¾…å®Œæˆåå†è¯•ã€‚")
        return

    success = task_manager.start_task(pipeline_def)
    if success:
        # è®°å½•æ‰§è¡Œå†å²
        if "pipeline_history" not in st.session_state:
            st.session_state.pipeline_history = []

        history_entry = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "name": pipeline_def.get("name", "Unknown Pipeline"),
            "steps": len(pipeline_def.get("steps", [])),
            "status": "running"  # åˆå§‹çŠ¶æ€
        }
        st.session_state.pipeline_history.append(history_entry)

        st.toast("ğŸš€ Task started in background!")
        st.rerun()
    else:
        st.error("å¯åŠ¨ä»»åŠ¡å¤±è´¥ã€‚")

class InputRenderer:
    """è¾“å…¥å­—æ®µæ¸²æŸ“å™¨"""

    def __init__(self, step_idx, p_name, p_info, current_inputs, step):
        self.step_idx = step_idx
        self.p_name = p_name
        self.p_info = p_info
        self.current_inputs = current_inputs
        self.step = step

        self.p_type = p_info.get('type', 'Any')
        self.p_required = p_info.get('required', False)
        self.default_val = p_info.get('default')

    def render(self):
        """æ¸²æŸ“è¾“å…¥å­—æ®µ"""
        current_val = self.current_inputs.get(self.p_name, self.default_val)
        is_ref = isinstance(current_val, str) and current_val.startswith("$")

        if is_ref:
            self._render_variable_input(current_val)
            return

        # æ ¹æ®ç±»å‹æ¸²æŸ“ä¸åŒçš„è¾“å…¥ç»„ä»¶
        if "bool" in self.p_type.lower():
            self._render_bool_input(current_val)
        elif "int" in self.p_type.lower():
            self._render_int_input(current_val)
        elif "list" in self.p_type.lower() or "dict" in self.p_type.lower():
            self._render_json_input(current_val)
        else:
            self._render_text_input(current_val)

    def _get_label_and_help(self):
        """è·å–æ ‡ç­¾å’Œå¸®åŠ©æ–‡æœ¬"""
        label = f"{self.p_name}{' *' if self.p_required else ''}"
        help_text = f"Type: {self.p_type}"
        if self.default_val is not None:
            help_text += f", Default: {self.default_val}"
        return label, help_text

    def _get_key(self):
        """è·å–ç»„ä»¶å”¯ä¸€é”®"""
        return f"in_{self.step_idx}_{self.p_name}"

    def _render_variable_input(self, current_val):
        """æ¸²æŸ“å˜é‡å¼•ç”¨è¾“å…¥"""
        label, help_text = self._get_label_and_help()
        key = self._get_key()

        new_val = st.text_input(label + " (Variable)", value=current_val, key=key, help=help_text)
        self.step["inputs"][self.p_name] = new_val

    def _render_bool_input(self, current_val):
        """æ¸²æŸ“å¸ƒå°”è¾“å…¥"""
        label, help_text = self._get_label_and_help()
        key = self._get_key()

        val = st.checkbox(label, value=bool(current_val) if current_val is not None else False, key=key, help=help_text)
        self.step["inputs"][self.p_name] = val

    def _render_int_input(self, current_val):
        """æ¸²æŸ“æ•´æ•°è¾“å…¥"""
        label, help_text = self._get_label_and_help()
        key = self._get_key()

        val = st.number_input(label, value=int(current_val) if current_val is not None else 0, step=1, key=key, help=help_text)
        self.step["inputs"][self.p_name] = int(val)

    def _render_json_input(self, current_val):
        """æ¸²æŸ“JSONè¾“å…¥"""
        label, help_text = self._get_label_and_help()
        key = self._get_key()

        val_str = str(current_val) if current_val is not None else "[]"
        new_val_str = st.text_area(label + " (JSON/List)", value=val_str, height=100, key=key, help="Enter valid JSON or $variable")
        if new_val_str.startswith("$"):
            self.step["inputs"][self.p_name] = new_val_str
        else:
            self._parse_json_value(new_val_str)

    def _render_text_input(self, current_val):
        """æ¸²æŸ“æ–‡æœ¬è¾“å…¥"""
        label, help_text = self._get_label_and_help()
        key = self._get_key()

        val = st.text_input(label, value=str(current_val) if current_val is not None else "", key=key, help=help_text)
        if val:
            self.step["inputs"][self.p_name] = val

    def _parse_json_value(self, value_str):
        """å®‰å…¨è§£æJSONå€¼"""
        try:
            import json
            if value_str.strip():
                # ä½¿ç”¨å®‰å…¨çš„JSONè§£æ
                parsed = json.loads(value_str)
                self.step["inputs"][self.p_name] = parsed
        except json.JSONDecodeError:
            # å¦‚æœä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œå°è¯•åŸºç¡€çš„Pythonå­—é¢é‡è§£æ
            self._parse_literal_value(value_str)

    def _parse_literal_value(self, value_str):
        """è§£æåŸºç¡€å­—é¢é‡å€¼"""
        try:
            stripped = value_str.strip()
            if stripped in ('True', 'False'):
                self.step["inputs"][self.p_name] = stripped == 'True'
            elif stripped == 'None':
                self.step["inputs"][self.p_name] = None
            elif self._is_numeric(stripped):
                self.step["inputs"][self.p_name] = float(stripped) if '.' in stripped else int(stripped)
            else:
                # å¯¹äºå¤æ‚ç±»å‹æˆ–å­—ç¬¦ä¸²ï¼Œç›´æ¥å­˜å‚¨åŸå€¼
                self.step["inputs"][self.p_name] = value_str
        except:
            self.step["inputs"][self.p_name] = value_str

    @staticmethod
    def _is_numeric(value):
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼"""
        test_val = value.replace('.', '').replace('-', '')
        return test_val.isdigit() or (value.startswith('-') and value[1:].replace('.', '').isdigit())

def render_input_field(step_idx, p_name, p_info, current_inputs, step):
    """æ™ºèƒ½æ¸²æŸ“è¾“å…¥ç»„ä»¶ï¼ˆå…¼å®¹æ€§æ¥å£ï¼‰"""
    renderer = InputRenderer(step_idx, p_name, p_info, current_inputs, step)
    renderer.render()

# --- åœºæ™¯åŒ–æ¨¡å—æ¸²æŸ“ ---
def render_source_configuration():
    """æ¸²æŸ“æ•°æ®æºé…ç½®"""
    st.subheader("Source Configuration")
    st.write("Source Configuration")

    # æ¸²æŸ“æ•°æ®æºç¼–è¾‘å™¨
    edited_df = render_source_editor()
    st.session_state.ingestion_apis = edited_df

    # æ˜¾ç¤ºé€‰ä¸­æºç»Ÿè®¡
    selected_apis = edited_df[edited_df["enabled"] == True]["name"].tolist()
    st.session_state["ingestion_selected_apis"] = selected_apis
    st.caption(f"âœ… Selected Sources: {len(selected_apis)}")

    # GNewså‚æ•°é…ç½®
    render_gnews_params()

def render_source_editor():
    """æ¸²æŸ“æ•°æ®æºç¼–è¾‘å™¨"""
    return st.data_editor(
        st.session_state.ingestion_apis,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "enabled": st.column_config.CheckboxColumn("Enabled"),
            "name": st.column_config.TextColumn("Source Name", required=True),
            "type": st.column_config.SelectboxColumn("API Type", options=["gnews"], required=True),
            "language": st.column_config.SelectboxColumn("Language", options=["ar", "zh", "nl", "en", "fr", "de", "el", "he", "hi", "id", "it", "ja", "ml", "mr", "no", "pt", "pa", "ro", "ru", "es", "sv", "ta", "te", "tr", "uk"]),
            "timeout": st.column_config.NumberColumn("Timeout (s)"),
            "country": st.column_config.SelectboxColumn("Country", options=["ar", "au", "br", "ca", "cn", "co", "eg", "fr", "de", "gr", "hk", "in", "id", "ie", "il", "it", "jp", "my", "mx", "nl", "no", "pk", "pe", "ph", "pt", "ro", "ru", "sg", "es", "se", "ch", "tw", "tr", "ua", "gb", "us"]),
        },
        key="ingestion_editor_main"
    )

def render_gnews_params():
    """æ¸²æŸ“GNewså‚æ•°é…ç½®"""
    gnews_params = st.session_state.get("gnews_params", {})
    with st.expander("GNews å¯é€‰å‚æ•°", expanded=False):
        # åŸºæœ¬å‚æ•°
        category = st.selectbox(
            "Category",
            ["", "general", "world", "business", "technology", "sports", "science", "health", "entertainment"],
            index=0,
            help="ç•™ç©ºåˆ™ä¸æŒ‡å®šåˆ†ç±»",
            key="gnews_category_source_config"
        )
        query = st.text_input("Query (å…³é”®è¯æœç´¢ï¼Œå¯ç©º)", key="gnews_query_source_config")

        # æ—¥æœŸæ—¶é—´é€‰æ‹©
        from_iso, to_iso = render_datetime_range()

        # é«˜çº§å‚æ•°
        nullable = st.text_input("Nullable", value=gnews_params.get("nullable", ""), help="å¦‚ description,content", key="gnews_nullable_source_config")
        truncate = st.text_input("Truncate", value=gnews_params.get("truncate", ""), help="å¦‚ content", key="gnews_truncate_source_config")
        sortby = st.selectbox("Sortby", ["", "publishedAt", "relevance"], index=0, key="gnews_sortby_source_config")
        in_fields = st.text_input("In fields", value=gnews_params.get("in_fields", ""), help="å¦‚ title,description", key="gnews_infields_source_config")
        page = st.number_input("Page", min_value=1, value=gnews_params.get("page", 1), step=1, key="gnews_page_source_config")

        # ä¿å­˜åˆ° session_state
        st.session_state["gnews_params"] = {
            "category": category or None,
            "query": query or None,
            "from_": from_iso,
            "to": to_iso,
            "nullable": nullable or None,
            "truncate": truncate or None,
            "sortby": sortby or None,
            "in_fields": in_fields or None,
            "page": int(page) if page else None,
        }

def render_datetime_range():
    """æ¸²æŸ“æ—¥æœŸæ—¶é—´èŒƒå›´é€‰æ‹©å™¨"""
    col_from, col_to = st.columns(2)
    min_date = datetime(2020, 1, 1).date()
    max_date = datetime.now().date()

    with col_from:
        d_from = st.date_input("From æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_from_date_source_config")
        t_from = st.time_input("From æ—¶é—´", value=None, key="gnews_from_time_source_config")

    with col_to:
        d_to = st.date_input("To æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_to_date_source_config")
        t_to = st.time_input("To æ—¶é—´", value=None, key="gnews_to_time_source_config")

    def combine(dt, tm):
        if dt is None:
            return None
        tm = tm or datetime.min.time()
        # ç»Ÿä¸€ä½¿ç”¨ UTC è¾“å‡º ISO8601
        return datetime.combine(dt, tm, tzinfo=timezone.utc).isoformat().replace("+00:00", "Z")

    from_iso = combine(d_from, t_from)
    to_iso = combine(d_to, t_to)

    return from_iso, to_iso

def render_source_api_pool():
    """æ¸²æŸ“æ•°æ®æºAPIæ± é…ç½®"""
    st.subheader("Source API Pool")
    st.caption("ç¼–è¾‘å¹¶ä¿å­˜åˆ° config/.env.localï¼ˆè¦†ç›–å†™å…¥ï¼Œä¸ä¿ç•™æ³¨é‡Š/ç©ºè¡Œï¼‰ã€‚ä¸Šæ–¹\"ä¿å­˜è¯¥è¡Œ/è¡¨æ ¼ä¿®æ”¹\"ä»…æ›´æ–°å†…å­˜ï¼Œéœ€åœ¨ä¸‹æ–¹ç‚¹å‡»ä¿å­˜æ‰å†™å…¥æ–‡ä»¶ã€‚")

    # ç¯å¢ƒå˜é‡ç¼–è¾‘å™¨
    edited_env = render_env_editor()

    # æ¸²æŸ“æ¯ä¸ªç¯å¢ƒå˜é‡çš„è¯¦ç»†ç¼–è¾‘å™¨
    render_env_detail_editors(edited_env)

    # ä¿å­˜æŒ‰é’®
    render_env_save_button(edited_env)

def render_env_editor():
    """æ¸²æŸ“ç¯å¢ƒå˜é‡ç¼–è¾‘å™¨"""
    edited_env = st.session_state.env_kv
    if st.checkbox("æ˜¾ç¤º Key/Value è¡¨æ ¼ç¼–è¾‘å™¨", value=False, key="env_editor_toggle_source_api"):
        edited_env = st.data_editor(
            st.session_state.env_kv,
            num_rows="dynamic",
            use_container_width=True,
                    column_config={
                        "key": st.column_config.TextColumn("Key", required=True),
                        "value": st.column_config.TextColumn("Value", required=False, help="å¯è¾“å…¥å ä½ç¬¦ï¼Œæ³¨æ„é¿å…æ³„éœ²æ•æ„Ÿå€¼")
                    },
                    key="env_editor_config_tab"
        )
        st.session_state.env_kv = edited_env
    return edited_env

def render_env_detail_editors(edited_env):
    """æ¸²æŸ“ç¯å¢ƒå˜é‡è¯¦ç»†ç¼–è¾‘å™¨"""
    for _, row in edited_env.iterrows():
        idx = row.name
        k = str(row.get("key", "")).strip()
        v = str(row.get("value", "")).strip()
        if not k:
            continue

        parsed, err = try_parse_json(v)
        with st.expander(f"{k}", expanded=False):
            if parsed is not None:
                render_json_editor(idx, parsed)
            else:
                st.warning(f"æ— æ³•è§£æä¸º JSONï¼š{err}")

def render_json_editor(idx, parsed):
    """æ¸²æŸ“JSONç¼–è¾‘å™¨"""
    pretty_txt = json.dumps(parsed, ensure_ascii=False, indent=2)
    new_txt = st.text_area(
        "JSON ç¼–è¾‘ï¼ˆä¿å­˜å³å†™å› valueï¼‰",
        value=pretty_txt,
        key=f"json_edit_{idx}",
        height=200
    )
    if st.button("ä¿å­˜è¯¥è¡Œ", key=f"save_json_source_api_{idx}", use_container_width=True):
        save_json_row(idx, new_txt)

    # è¡¨æ ¼åŒ–å±•ç¤º
    render_table_editor(idx, parsed)

def save_json_row(idx, new_txt):
    """ä¿å­˜JSONè¡Œ"""
    try:
        parsed_new = json.loads(new_txt)
        # å†™å›è¡¨æ ¼ç¼“å­˜ï¼Œä¿æŒç´§å‡‘å­˜å‚¨
        st.session_state.env_kv.at[idx, "value"] = json.dumps(parsed_new, ensure_ascii=False)
        st.success("å·²æ›´æ–°è¯¥è¡Œçš„ value")
    except Exception as e:
        st.error(f"JSON è§£æå¤±è´¥: {e}")

def render_table_editor(idx, parsed):
    """æ¸²æŸ“è¡¨æ ¼ç¼–è¾‘å™¨"""
    df_preview, kind = to_table(parsed)
    if df_preview is not None and not df_preview.empty:
        edited_df = st.data_editor(
            df_preview,
            num_rows="dynamic",
            use_container_width=True,
            key=f"env_table_{idx}"
        )
        if st.button("ä¿å­˜è¡¨æ ¼ä¿®æ”¹", key=f"save_table_{idx}", use_container_width=True):
            save_table_modification(idx, edited_df, kind)

def save_table_modification(idx, edited_df, kind):
    """ä¿å­˜è¡¨æ ¼ä¿®æ”¹"""
    try:
        if kind == "list":
            new_obj = edited_df.to_dict(orient="records")
        else:
            new_obj = edited_df.to_dict(orient="records")[0] if not edited_df.empty else {}
        st.session_state.env_kv.at[idx, "value"] = json.dumps(new_obj, ensure_ascii=False)
        st.success("å·²æ ¹æ®è¡¨æ ¼ä¿®æ”¹æ›´æ–° value")
    except Exception as e:
        st.error(f"å†™å› JSON å¤±è´¥: {e}")

def to_table(obj):
    """å°†å¯¹è±¡è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼"""
    if isinstance(obj, list) and obj and all(isinstance(x, dict) for x in obj):
        return pd.DataFrame(obj), "list"
    if isinstance(obj, dict):
        return pd.DataFrame([obj]), "dict"
    return None, ""

def render_env_save_button(edited_env):
    """æ¸²æŸ“ç¯å¢ƒå˜é‡ä¿å­˜æŒ‰é’®"""
    if st.button("ğŸ’¾ ä¿å­˜åˆ° .env.local", type="primary", use_container_width=True, key="system_save_env_local_source_api"):
        save_env_file(edited_env)

def save_env_file(edited_env):
    """ä¿å­˜ç¯å¢ƒå˜é‡æ–‡ä»¶"""
    lines = []
    for _, row in edited_env.iterrows():
        k = str(row.get("key", "")).strip()
        if not k:
            continue
        v = str(row.get("value", "")).strip()
        lines.append(f"{k}={v}")

    ENV_PATH.parent.mkdir(parents=True, exist_ok=True)
    ENV_PATH.write_text("\n".join(lines) + "\n", encoding="utf-8")
    st.success(f"å·²å†™å…¥ {ENV_PATH.name} ï¼Œå…± {len(lines)} æ¡è®°å½•")

def try_parse_json(val: str):
    """å°è¯•è§£æJSONå­—ç¬¦ä¸²"""
    if not isinstance(val, str):
        return None, "éå­—ç¬¦ä¸²"
    txt = val.strip()
    if not txt:
        return None, "ç©ºå€¼"
    try:
        obj = json.loads(txt)
        return obj, ""
    except Exception as e:
        return None, str(e)

def render_configuration_tab():
    """æ¸²æŸ“é…ç½®æ ‡ç­¾é¡µ"""
    st.header("ğŸ“š Configuration")
    source_config, source_api_pool = st.tabs(["Source Configuration", "Source API Pool"])

    with source_config:
        render_source_configuration()

    with source_api_pool:
        render_source_api_pool()

        # GNews å¯é€‰å‚æ•°é…ç½®
        gnews_params = st.session_state.get("gnews_params", {})
        with st.expander("GNews å¯é€‰å‚æ•°", expanded=False):
            category = st.selectbox(
                "Category",
                ["", "general", "world", "business", "technology", "sports", "science", "health", "entertainment"],
                index=0,
                help="ç•™ç©ºåˆ™ä¸æŒ‡å®šåˆ†ç±»",
                key="gnews_category_config_tab"
            )
            query = st.text_input("Query (å…³é”®è¯æœç´¢ï¼Œå¯ç©º)", key="gnews_query_config_tab")
            col_from, col_to = st.columns(2)
            min_date = datetime(2020, 1, 1).date()
            max_date = datetime.now().date()
            with col_from:
                d_from = st.date_input("From æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_from_date_config_tab")
                t_from = st.time_input("From æ—¶é—´", value=None, key="gnews_from_time_config_tab")
            with col_to:
                d_to = st.date_input("To æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_to_date_config_tab")
                t_to = st.time_input("To æ—¶é—´", value=None, key="gnews_to_time_config_tab")

            def combine(dt, tm):
                if dt is None:
                    return None
                tm = tm or datetime.min.time()
                # ç»Ÿä¸€ä½¿ç”¨ UTC è¾“å‡º ISO8601
                return datetime.combine(dt, tm, tzinfo=timezone.utc).isoformat().replace("+00:00", "Z")

            from_iso = combine(d_from, t_from)
            to_iso = combine(d_to, t_to)
            nullable = st.text_input("Nullable", value=gnews_params.get("nullable", ""), help="å¦‚ description,content", key="gnews_nullable_config_tab")
            truncate = st.text_input("Truncate", value=gnews_params.get("truncate", ""), help="å¦‚ content", key="gnews_truncate_config_tab")
            sortby = st.selectbox("Sortby", ["", "publishedAt", "relevance"], index=0, key="gnews_sortby_config_tab")
            in_fields = st.text_input("In fields", value=gnews_params.get("in_fields", ""), help="å¦‚ title,description", key="gnews_infields_config_tab")
            page = st.number_input("Page", min_value=1, value=gnews_params.get("page", 1), step=1, key="gnews_page_config_tab")

            # ä¿å­˜åˆ° session_state
            st.session_state["gnews_params"] = {
                "category": category or None,
                "query": query or None,
                "from_": from_iso,
                "to": to_iso,
                "nullable": nullable or None,
                "truncate": truncate or None,
                "sortby": sortby or None,
                "in_fields": in_fields or None,
                "page": int(page) if page else None,
            }

    with source_api_pool:
        st.subheader("Source API Pool")
        st.caption("ç¼–è¾‘å¹¶ä¿å­˜åˆ° config/.env.localï¼ˆè¦†ç›–å†™å…¥ï¼Œä¸ä¿ç•™æ³¨é‡Š/ç©ºè¡Œï¼‰ã€‚ä¸Šæ–¹â€œä¿å­˜è¯¥è¡Œ/è¡¨æ ¼ä¿®æ”¹â€ä»…æ›´æ–°å†…å­˜ï¼Œéœ€åœ¨ä¸‹æ–¹ç‚¹å‡»ä¿å­˜æ‰å†™å…¥æ–‡ä»¶ã€‚")
        edited_env = st.session_state.env_kv
        if st.checkbox("æ˜¾ç¤º Key/Value è¡¨æ ¼ç¼–è¾‘å™¨", value=False, key="env_editor_toggle_config_tab"):
            edited_env = st.data_editor(
                st.session_state.env_kv,
                num_rows="dynamic",
                use_container_width=True,
                column_config={
                    "key": st.column_config.TextColumn("Key", required=True),
                    "value": st.column_config.TextColumn("Value", required=False, help="å¯è¾“å…¥å ä½ç¬¦ï¼Œæ³¨æ„é¿å…æ³„éœ²æ•æ„Ÿå€¼")
                },
                key="env_editor_source_api"
            )
            st.session_state.env_kv = edited_env

        for _, row in edited_env.iterrows():
            idx = row.name
            k = str(row.get("key", "")).strip()
            v = str(row.get("value", "")).strip()
            if not k:
                continue
            parsed, err = try_parse_json(v)
            with st.expander(f"{k}", expanded=False):
                if parsed is not None:
                    pretty_txt = json.dumps(parsed, ensure_ascii=False, indent=2)
                    new_txt = st.text_area(
                        "JSON ç¼–è¾‘ï¼ˆä¿å­˜å³å†™å› valueï¼‰",
                        value=pretty_txt,
                        key=f"json_edit_config_tab_{idx}",
                        height=200
                    )
                    if st.button("ä¿å­˜è¯¥è¡Œ", key=f"save_json_config_tab_{idx}", use_container_width=True):
                        try:
                            parsed_new = json.loads(new_txt)
                            # å†™å›è¡¨æ ¼ç¼“å­˜ï¼Œä¿æŒç´§å‡‘å­˜å‚¨
                            edited_env.at[idx, "value"] = json.dumps(parsed_new, ensure_ascii=False)
                            st.session_state.env_kv = edited_env
                            st.success("å·²æ›´æ–°è¯¥è¡Œçš„ value")
                        except Exception as e:
                            st.error(f"JSON è§£æå¤±è´¥: {e}")
                    # è¡¨æ ¼åŒ–å±•ç¤ºï¼ˆä¼˜å…ˆ list[dict] æˆ– dict -> DataFrameï¼‰ï¼Œå¦åˆ™ç”¨ json
                    def to_table(obj):
                        if isinstance(obj, list) and obj and all(isinstance(x, dict) for x in obj):
                            return pd.DataFrame(obj), "list"
                        if isinstance(obj, dict):
                            return pd.DataFrame([obj]), "dict"
                        return None, ""
                    df_preview, kind = to_table(parsed)
                    if df_preview is not None and not df_preview.empty:
                        edited_df = st.data_editor(
                            df_preview,
                            num_rows="dynamic",
                            use_container_width=True,
                            key=f"env_table_config_tab_{idx}"
                        )
                        if st.button("ä¿å­˜è¡¨æ ¼ä¿®æ”¹", key=f"save_table_config_tab_{idx}", use_container_width=True):
                            try:
                                if kind == "list":
                                    new_obj = edited_df.to_dict(orient="records")
                                else:
                                    new_obj = edited_df.to_dict(orient="records")[0] if not edited_df.empty else {}
                                edited_env.at[idx, "value"] = json.dumps(new_obj, ensure_ascii=False)
                                st.session_state.env_kv = edited_env
                                st.success("å·²æ ¹æ®è¡¨æ ¼ä¿®æ”¹æ›´æ–° value")
                            except Exception as e:
                                st.error(f"å†™å› JSON å¤±è´¥: {e}")
                    else:
                        st.json(parsed)
                else:
                    st.warning(f"æ— æ³•è§£æä¸º JSONï¼š{err}")

        if st.button("ğŸ’¾ ä¿å­˜åˆ° .env.local", type="primary", use_container_width=True, key="system_save_env_local_config_tab"):
            lines = []
            for _, row in edited_env.iterrows():
                k = str(row.get("key", "")).strip()
                if not k:
                    continue
                v = str(row.get("value", "")).strip()
                lines.append(f"{k}={v}")
            ENV_PATH.parent.mkdir(parents=True, exist_ok=True)
            ENV_PATH.write_text("\n".join(lines) + "\n", encoding="utf-8")
            st.success(f"å·²å†™å…¥ {ENV_PATH.name} ï¼Œå…± {len(lines)} æ¡è®°å½•")



def render_ingestion_tab():
    """æ¸²æŸ“æ•°æ®æ‘„å…¥æ ‡ç­¾é¡µ"""
    st.header("ğŸ“¥ Data Ingestion")
    st.caption("Fetch news from sources (Feed/Search) and extract events.")

    # å¤„ç†å‚æ•°é…ç½®
    params = render_ingestion_parameters()

    # æ¸²æŸ“é…ç½®æ‘˜è¦
    render_ingestion_summary(params)

    # å›¾è°±æ›´æ–°æ¨¡å¼é…ç½®
    update_config = render_graph_update_mode()

    # æ‰§è¡ŒæŒ‰é’®
    if render_ingestion_execution_button(params, update_config):
        execute_ingestion_pipeline(params, update_config)

def render_ingestion_parameters():
    """æ¸²æŸ“æ‘„å…¥å‚æ•°é…ç½®"""
    st.subheader("Processing Parameters")

    col_p1, col_p2 = st.columns(2)
    with col_p1:
        st.markdown("##### ğŸ“¥ Fetch Settings")
        news_limit = st.number_input("Limit (per source)", 1, 10, 5, 1, help="Max news items to fetch per source.")
    with col_p2:
        st.markdown("##### âš™ï¸ Pipeline Actions")
        auto_update_kg = st.checkbox("Auto Update Knowledge Graph", True, help="Automatically extract entities and update the graph.")
        enable_report = st.checkbox("Generate Summary Report", True, help="Create a markdown report after processing.")
    return {
        'news_limit': news_limit,
        'auto_update_kg': auto_update_kg,
        'enable_report': enable_report
    }

def render_ingestion_summary(params):
    """æ¸²æŸ“æ‘„å…¥é…ç½®æ‘˜è¦"""
    st.subheader("ğŸš€ Ready to Start?")

    # æ±‡æ€»é…ç½®
    current_df = st.session_state.ingestion_apis
    selected_sources = current_df[current_df["enabled"] == True]["name"].tolist()

    st.write("Summary:")
    c1, c2, c3 = st.columns(3)
    c1.metric("Sources Selected", len(selected_sources))
    c2.metric("Max Items", params['news_limit'])
    c3.metric("Auto-Update KG", "Yes" if params['auto_update_kg'] else "No")

    return selected_sources

def render_graph_update_mode():
    """æ¸²æŸ“å›¾è°±æ›´æ–°æ¨¡å¼é…ç½®"""
    st.subheader("å›¾è°±æ›´æ–°æ¨¡å¼")
    col_mode_ing, col_forms_ing = st.columns(2)
    with col_mode_ing:
        append_only_ing = st.checkbox("ä»…è¿½åŠ ï¼ˆä¸æ”¹æ—§æ•°æ®ï¼‰- Ingestion", value=True, help="ä¸ä¿®æ”¹å·²æœ‰å®ä½“/äº‹ä»¶ï¼Œåªæ–°å¢ä¸å­˜åœ¨çš„è®°å½•")
    with col_forms_ing:
        allow_append_forms_ing = st.checkbox("è¿½åŠ æ—§å®ä½“çš„ original_forms - Ingestion", value=True, help="ä»…åœ¨ä»…è¿½åŠ æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼›å…³é—­åˆ™å®Œå…¨ä¸æ”¹æ—§å®ä½“å­—æ®µ")
    return {
        'append_only': append_only_ing,
        'allow_append_forms': allow_append_forms_ing
    }

def render_ingestion_execution_button(params, update_config):
    """æ¸²æŸ“æ‘„å…¥æ‰§è¡ŒæŒ‰é’®"""
    selected_sources = render_ingestion_summary(params)

    if not selected_sources:
        st.error("âŒ æœªé€‰æ‹©æ•°æ®æºã€‚è¯·è¿”å›'Data Sources'æ ‡ç­¾é¡µè¿›è¡Œé€‰æ‹©ã€‚")
        return False

    return st.button("Start Ingestion Task", type="primary", use_container_width=True, key="pipeline_start_ingestion_task")

def execute_ingestion_pipeline(params, update_config):
    """æ‰§è¡Œæ•°æ®æ‘„å…¥æµæ°´çº¿"""
    current_df = st.session_state.ingestion_apis
    selected_sources = current_df[current_df["enabled"] == True]["name"].tolist()
    gnews_params = st.session_state.get("gnews_params", {})

    pipeline_def = {
        "name": "Data Ingestion Task",
        "steps": [
            {
                "id": "fetch_news",
                "tool": "fetch_news_stream",
                "inputs": {
                    "limit": params['news_limit'],
                    "sources": selected_sources,
                    # GNews å¯é€‰å‚æ•°é€ä¼ ï¼ˆä»…å½“æœ‰å€¼ï¼‰
                    **{k: v for k, v in gnews_params.items() if v}
                },
                "output": "raw_news_data"
            },
            {
                "id": "process_news",
                "tool": "batch_process_news",
                "inputs": {"news_list": "$raw_news_data"},
                "output": "extracted_events"
            },
            {
                "id": "save_events_tmp",
                "tool": "save_extracted_events_tmp",
                "inputs": {"events": "$extracted_events"},
                "output": "events_path"
            },
            {
                "id": "update_graph_from_ingestion" if not update_config['append_only'] else "append_graph_from_ingestion",
                "tool": "update_graph_data" if not update_config['append_only'] else "append_only_update_graph",
                "inputs": {"events_list": "$extracted_events", "allow_append_original_forms": update_config['allow_append_forms']} if update_config['append_only'] else {"events_list": "$extracted_events"},
                "output": "kg_update_result_ingestion"
            },
            {
                "id": "refresh_kg_after_ingestion",
                "tool": "refresh_knowledge_graph",
                "inputs": {},
                "output": "kg_refresh_result_ingestion"
            },
            {
                "id": "report_ingestion",
                "tool": "generate_markdown_report",
                "inputs": {"events_list": "$extracted_events", "title": "Ingestion Extracted Events Report"},
                "output": "ingestion_report_md"
            }
        ]
    }

    if params['auto_update_kg']:
        pipeline_def["steps"].append({
            "id": "update_kg",
            "tool": "update_graph_data",
            "inputs": {"events_list": "$extracted_events"},
            "output": "update_status"
        })

    if params['enable_report']:
        pipeline_def["steps"].append({
            "id": "generate_report",
            "tool": "generate_markdown_report",
            "inputs": {"events_list": "$extracted_events", "title": f"Ingestion Report {datetime.now().strftime('%Y-%m-%d')}"},
            "output": "final_report_md"
        })

    execute_pipeline(pipeline_def)

def render_entity_selector():
    """æ¸²æŸ“å®ä½“é€‰æ‹©å™¨ç»„ä»¶"""
    entities = utils.load_entities()
    if not entities:
        return

    all_entity_names = sorted(list(entities.keys()))

    c_add_sel, c_add_btn = st.columns([3, 1])
    with c_add_sel:
        selected_entities = st.multiselect(
            "Select Entities from Graph",
            options=all_entity_names,
            placeholder="Choose entities to add..."
        )
    with c_add_btn:
        st.write("")  # Spacer
        st.write("")
        if st.button("â• Add Selected", use_container_width=True, key="expansion_add_selected_entities"):
            add_selected_entities(selected_entities)

def add_selected_entities(selected_entities):
    """æ·»åŠ é€‰ä¸­çš„å®ä½“åˆ°ä»»åŠ¡åˆ—è¡¨"""
    if not selected_entities:
        st.warning("è¯·å…ˆé€‰æ‹©å®ä½“ã€‚")
        return

    # è·å–ç°æœ‰å…³é”®è¯ä»¥é¿å…é‡å¤
    existing_kws = set()
    if not st.session_state.expansion_tasks.empty:
        existing_kws = set(st.session_state.expansion_tasks["keyword"].tolist())

    new_rows = []
    count = 0
    for ent in selected_entities:
        if ent not in existing_kws:
            new_rows.append({
                "enabled": True,
                "keyword": ent,
                "limit": 5,
                "category": "general",
                "from": "",
                "to": "",
                "sortby": ""
            })
            count += 1

    if new_rows:
        new_df = pd.DataFrame(new_rows)
        st.session_state.expansion_tasks = pd.concat(
            [st.session_state.expansion_tasks, new_df],
            ignore_index=True
        )
        st.success(f"å·²æ·»åŠ  {count} ä¸ªæ–°ä»»åŠ¡ï¼")
        st.rerun()
    else:
        st.warning("é€‰ä¸­çš„å®ä½“å·²åœ¨åˆ—è¡¨ä¸­ã€‚")

def render_datetime_picker():
    """æ¸²æŸ“æ—¥æœŸæ—¶é—´é€‰æ‹©å™¨"""
    with st.expander("æ—¥æœŸæ—¶é—´å¿«æ·å¡«å……ï¼ˆå¯é€‰ï¼‰", expanded=False):
        col_from, col_to = st.columns(2)
        min_date = datetime(2020, 1, 1).date()
        max_date = datetime.now().date()
        with col_from:
            d_from = st.date_input("From æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_from_date_expansion_picker")
            t_from = st.time_input("From æ—¶é—´", value=None, key="gnews_from_time_expansion_picker")
        with col_to:
            d_to = st.date_input("To æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_to_date_expansion_picker")
            t_to = st.time_input("To æ—¶é—´", value=None, key="gnews_to_time_expansion_picker")

        def combine(dt, tm):
            if dt is None:
                return None
            tm = tm or datetime.min.time()
            return datetime.combine(dt, tm, tzinfo=timezone.utc).isoformat().replace("+00:00", "Z")

        from_iso = combine(d_from, t_from)
        to_iso = combine(d_to, t_to)
        st.caption(f"From (ISO8601): {from_iso or 'æœªè®¾ç½®'}")
        st.caption(f"To   (ISO8601): {to_iso or 'æœªè®¾ç½®'}")

        apply_from_all = st.checkbox("å°† From å¡«å……åˆ°æ‰€æœ‰è¡Œï¼ˆè‹¥è®¾ç½®ï¼‰", value=False, key="apply_from_all_expansion")
        apply_to_all = st.checkbox("å°† To å¡«å……åˆ°æ‰€æœ‰è¡Œï¼ˆè‹¥è®¾ç½®ï¼‰", value=False, key="apply_to_all_expansion")

        if st.button("åº”ç”¨åˆ°ä»»åŠ¡è¡¨", type="primary", key="apply_datetime_expansion"):
            apply_datetime_to_tasks(from_iso, to_iso, apply_from_all, apply_to_all)

def apply_datetime_to_tasks(from_iso, to_iso, apply_from_all, apply_to_all):
    """åº”ç”¨æ—¥æœŸæ—¶é—´è®¾ç½®åˆ°ä»»åŠ¡è¡¨"""
    df = st.session_state.expansion_tasks.copy()
    if from_iso:
        if apply_from_all:
            df["from"] = from_iso
        else:
            df.loc[(df["from"].isna()) | (df["from"] == ""), "from"] = from_iso
    if to_iso:
        if apply_to_all:
            df["to"] = to_iso
        else:
            df.loc[(df["to"].isna()) | (df["to"] == ""), "to"] = to_iso
    st.session_state.expansion_tasks = df
    st.success("å·²åº”ç”¨åˆ°ä»»åŠ¡è¡¨ï¼Œè¯·åœ¨ä¸‹æ–¹è¡¨æ ¼ç¡®è®¤ã€‚")
    st.rerun()

def render_expansion_tab():
    """æ¸²æŸ“çŸ¥è¯†æ‹“å±•æ ‡ç­¾é¡µ"""
    st.header("ğŸ” Knowledge Expansion")
    st.caption("Search for news based on keywords to discover new entities.")

    st.subheader("Define Search Tasks")
    st.info("ç®¡ç†æœç´¢å…³é”®è¯ã€‚æ‚¨å¯ä»¥ä»çŸ¥è¯†å›¾è°±æ·»åŠ å®ä½“ï¼Œæˆ–åœ¨è¡¨æ ¼ä¸­æ‰‹åŠ¨è¾“å…¥æ–°å…³é”®è¯ã€‚")

    # é€‰æ‹©å¯ç”¨çš„æœç´¢ API
    selected_apis = st.session_state.ingestion_apis[st.session_state.ingestion_apis["enabled"] == True]["name"].tolist()

    # å®ä½“é€‰æ‹©å™¨
    render_entity_selector()

    # æ—¥æœŸæ—¶é—´é€‰æ‹©å™¨
    render_datetime_picker()

    # ä»»åŠ¡è¡¨æ ¼ç¼–è¾‘å™¨
    render_expansion_tasks_editor()

    # è¿è¡Œæ§åˆ¶é¢æ¿
    render_expansion_run_panel(selected_apis)

def render_expansion_tasks_editor():
    """æ¸²æŸ“æ‹“å±•ä»»åŠ¡ç¼–è¾‘å™¨"""
    edited_tasks = st.data_editor(
        st.session_state.expansion_tasks,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "enabled": st.column_config.CheckboxColumn("Enabled"),
            "keyword": st.column_config.TextColumn("Keyword", required=True, help="Type manually or added from dropdown"),
            "limit": st.column_config.NumberColumn("Limit", min_value=1, max_value=10, default=5),
            "category": st.column_config.TextColumn("Category", help="GNews categoryï¼Œå¦‚ general/business/...ï¼Œå¯ç©º"),
            "from": st.column_config.TextColumn("From (ISO8601)", help="å¯ç©ºï¼Œå¦‚ 2025-12-01T00:00:00Z"),
            "to": st.column_config.TextColumn("To (ISO8601)", help="å¯ç©ºï¼Œå¦‚ 2025-12-31T23:59:59Z"),
            "sortby": st.column_config.SelectboxColumn("Sortby", options=["", "publishedAt", "relevance"]),
        },
        key="expansion_tasks_editor"
    )
    st.session_state.expansion_tasks = edited_tasks

def render_expansion_run_panel(selected_apis):
    """æ¸²æŸ“æ‹“å±•ä»»åŠ¡è¿è¡Œæ§åˆ¶é¢æ¿"""
    st.subheader("ğŸš€ Run Expansion")

    # è¿‡æ»¤å‡ºå¯ç”¨çš„ä»»åŠ¡
    active_tasks = st.session_state.expansion_tasks[st.session_state.expansion_tasks["enabled"] == True]

    c1, c2 = st.columns(2)
    c1.metric("Selected APIs", len(selected_apis))
    c2.metric("Active Tasks", len(active_tasks))

    st.subheader("å›¾è°±æ›´æ–°æ¨¡å¼")
    col_mode, col_forms = st.columns(2)
    with col_mode:
        append_only_mode = st.checkbox("ä»…è¿½åŠ ï¼ˆä¸æ”¹æ—§æ•°æ®ï¼‰", value=True, help="ä¸ä¿®æ”¹å·²æœ‰å®ä½“/äº‹ä»¶ï¼Œåªæ–°å¢ä¸å­˜åœ¨çš„è®°å½•")
    with col_forms:
        allow_append_forms = st.checkbox("è¿½åŠ æ—§å®ä½“çš„ original_forms", value=True, help="ä»…åœ¨ä»…è¿½åŠ æ¨¡å¼ä¸‹ç”Ÿæ•ˆï¼›å…³é—­åˆ™å®Œå…¨ä¸æ”¹æ—§å®ä½“å­—æ®µ")
    if st.button("Start Expansion Task", type="primary", use_container_width=True, key="pipeline_start_expansion_task"):
        execute_expansion_pipeline(selected_apis, active_tasks, append_only_mode, allow_append_forms)

def execute_expansion_pipeline(selected_apis, active_tasks, append_only_mode, allow_append_forms):
    """æ‰§è¡Œæ‹“å±•æµæ°´çº¿"""
    if not selected_apis:
        st.error("Please select at least one Search API.")
        return
    if active_tasks.empty:
        st.error("Please define and enable at least one Search Task.")
        return

    # æ„å»º Pipelineï¼šä¸ºæ¯ä¸ªå¯ç”¨ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªæ­¥éª¤
    pipeline_steps = create_expansion_pipeline_steps(selected_apis, active_tasks, append_only_mode, allow_append_forms)

    pipeline_def = {
        "name": "Knowledge Expansion Batch",
        "steps": pipeline_steps
    }
    execute_pipeline(pipeline_def)

class ExpansionPipelineBuilder:
    """çŸ¥è¯†æ‰©å±•æµæ°´çº¿æ„å»ºå™¨"""

    def __init__(self):
        self.steps = []

    def add_search_steps(self, selected_apis, active_tasks):
        """æ·»åŠ æœç´¢æ­¥éª¤"""
        for idx, row in active_tasks.iterrows():
            kw = row["keyword"]
            step_id = f"search_{kw.replace(' ', '_')}_{idx}"

            # æœç´¢æ­¥éª¤
            self.steps.append({
                "id": step_id,
                "tool": "search_news_by_keywords",
                "inputs": {
                    "keywords": [kw],  # å·¥å…·æœŸæœ›åˆ—è¡¨
                    "apis": selected_apis,
                    "limit": int(row.get("limit", 50)),
                    "category": row.get("category") or None,
                    "from": row.get("from") or None,
                    "to": row.get("to") or None,
                    "sortby": row.get("sortby") or None
                },
                "output": f"results_{idx}"
            })

            # äº‹ä»¶æå–æ­¥éª¤
            self.steps.append({
                "id": f"extract_{kw.replace(' ', '_')}_{idx}",
                "tool": "batch_process_news",
                "inputs": {"news_list": f"$results_{idx}"},
                "output": f"extracted_events_{idx}"
            })

            # ä¸´æ—¶ä¿å­˜æ­¥éª¤
            self.steps.append({
                "id": f"save_events_{kw.replace(' ', '_')}_{idx}",
                "tool": "save_extracted_events_tmp",
                "inputs": {"events": f"$extracted_events_{idx}"},
                "output": f"events_path_{idx}"
            })

            # æŒä¹…åŒ–æ­¥éª¤
            self.steps.append({
                "id": f"persist_{kw.replace(' ', '_')}_{idx}",
                "tool": "persist_expanded_news_tmp",
                "inputs": {"expanded_news": f"$results_{idx}"},
                "output": f"persist_result_{idx}"
            })

    def add_graph_update_steps(self, active_tasks, append_only_mode, allow_append_forms):
        """æ·»åŠ å›¾è°±æ›´æ–°æ­¥éª¤"""
        all_extracted_keys = [f"$extracted_events_{i}" for i in range(len(active_tasks))]

        if append_only_mode:
            self.steps.append({
                "id": "append_graph_from_expansion",
                "tool": "append_only_update_graph",
                "inputs": {
                    "events_list": all_extracted_keys,
                    "allow_append_original_forms": allow_append_forms
                },
                "output": "kg_update_result"
            })
        else:
            self.steps.append({
                "id": "update_graph_from_expansion",
                "tool": "update_graph_data",
                "inputs": {"events_list": all_extracted_keys},
                "output": "kg_update_result"
            })

    def add_final_steps(self, active_tasks):
        """æ·»åŠ æœ€ç»ˆæ­¥éª¤ï¼ˆåˆ·æ–°å’ŒæŠ¥å‘Šï¼‰"""
        all_extracted_keys = [f"$extracted_events_{i}" for i in range(len(active_tasks))]

        # åˆ·æ–°å›¾è°±
        self.steps.append({
            "id": "refresh_kg_after_expansion",
            "tool": "refresh_knowledge_graph",
            "inputs": {},
            "output": "kg_refresh_result"
        })

        # ç”ŸæˆæŠ¥å‘Š
        self.steps.append({
            "id": "report_expansion",
            "tool": "generate_markdown_report",
            "inputs": {
                "events_list": all_extracted_keys,
                "title": "Expansion Extracted Events Report"
            },
            "output": "expansion_report_md"
        })

    def build(self, selected_apis, active_tasks, append_only_mode, allow_append_forms):
        """æ„å»ºå®Œæ•´çš„æ‰©å±•æµæ°´çº¿"""
        self.steps = []

        # æ·»åŠ æœç´¢ç›¸å…³æ­¥éª¤
        self.add_search_steps(selected_apis, active_tasks)

        # æ·»åŠ å›¾è°±æ›´æ–°æ­¥éª¤
        self.add_graph_update_steps(active_tasks, append_only_mode, allow_append_forms)

        # æ·»åŠ æœ€ç»ˆæ­¥éª¤
        self.add_final_steps(active_tasks)

        return {
            "name": "Knowledge Expansion Batch",
            "steps": self.steps
        }

def create_expansion_pipeline_steps(selected_apis, active_tasks, append_only_mode, allow_append_forms):
    """åˆ›å»ºæ‹“å±•æµæ°´çº¿æ­¥éª¤ï¼ˆå…¼å®¹æ€§æ¥å£ï¼‰"""
    builder = ExpansionPipelineBuilder()
    return builder.build(selected_apis, active_tasks, append_only_mode, allow_append_forms)

def render_maintenance_tab():
    """æ¸²æŸ“å›¾è°±ç»´æŠ¤æ ‡ç­¾é¡µ"""
    st.header("ğŸ•¸ï¸ Graph Maintenance")

    # è·å–ä¸´æ—¶æ•°æ®ç»Ÿè®¡
    maintenance_stats = get_maintenance_stats()

    # æ¸²æŸ“ç»Ÿè®¡æŒ‡æ ‡
    render_maintenance_metrics(maintenance_stats)

    # æ¸²æŸ“ä¸´æ—¶æ•°æ®é¢„è§ˆ
    render_temp_data_preview(maintenance_stats)

    # æ¸²æŸ“ç»´æŠ¤æ“ä½œè¡¨å•
    render_maintenance_form()

def render_maintenance_form():
    """æ¸²æŸ“ç»´æŠ¤æ“ä½œè¡¨å•"""
    with st.form("maintenance_form"):
        # å»é‡å‚æ•°
        render_deduplication_params()

        # æ¸…ç†å‚æ•°
        render_cleaning_params()

        # ä¸´æ—¶æ•°æ®å¯¼å…¥å‚æ•°
        render_temp_data_import_params()

        # æ‰§è¡ŒæŒ‰é’®
        submitted = st.form_submit_button("ğŸš€ Run Maintenance", type="primary", use_container_width=True)

    # å¤„ç†è¡¨å•æäº¤
    if submitted:
        execute_maintenance_pipeline()

def render_deduplication_params():
    """æ¸²æŸ“å»é‡å‚æ•°"""
    c1, c2 = st.columns(2)
    with c1:
        st.subheader("Deduplication")
        st.checkbox("Strict Mode", True)
        st.slider("Similarity", 0.5, 1.0, 0.9)
def render_cleaning_params():
    """æ¸²æŸ“æ¸…ç†å‚æ•°"""
    with st.container():
        st.subheader("Cleaning")
st.checkbox("Remove Isolated Nodes", key="unknown_checkbox_auto_8")
def render_temp_data_import_params():
    """æ¸²æŸ“ä¸´æ—¶æ•°æ®å¯¼å…¥å‚æ•°"""
    st.subheader("å¯¼å…¥ tmp æŠ½å–ç»“æœ")
    st.checkbox("åˆ·æ–°å‰å…ˆè¿½åŠ  tmp/extracted_events_*.jsonl", value=True)
    st.number_input("æœ€å¤šè¯»å–æ–‡ä»¶æ•°ï¼ˆ0=å…¨éƒ¨ï¼‰", min_value=0, value=0, step=1)
    st.checkbox("è¿½åŠ æ—§å®ä½“ original_formsï¼ˆè¿½åŠ æ¨¡å¼ï¼‰", value=True)
def execute_maintenance_pipeline():
    """æ‰§è¡Œç»´æŠ¤æµæ°´çº¿"""
    # è¿™é‡Œéœ€è¦è·å–è¡¨å•æ•°æ®ï¼Œä½†ç”±äºStreamlitçš„é™åˆ¶ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¡¨å•å†…éƒ¨å¤„ç†
    # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œä½¿ç”¨é»˜è®¤å€¼
    pipeline_def = {
        "name": "Graph Maintenance",
        "steps": [
            {
                "id": "append_tmp_events",
                "tool": "append_tmp_extracted_events",
                "inputs": {
                    "max_files": 0,
                    "allow_append_original_forms": True
                },
                "output": "tmp_append_result"
            },
            {
                "id": "refresh_kg",
                "tool": "refresh_knowledge_graph",
                "inputs": {},
                "output": "status"
            }
        ]
    }
    execute_pipeline(pipeline_def)

    # æ¸…ç†ä¸´æ—¶ç¼“å­˜æ–‡ä»¶
    cleanup_temp_cache()

def cleanup_temp_cache():
    """æ¸…ç†ä¸´æ—¶ç¼“å­˜æ–‡ä»¶"""
    try:
        data_dir = ROOT_DIR / "data"
        entities_tmp_file = data_dir / "tmp" / "entities_tmp.json"
        events_tmp_file = data_dir / "tmp" / "abstract_to_event_map_tmp.json"

        for p in [entities_tmp_file, events_tmp_file]:
            if p.exists():
                p.unlink()
        st.cache_data.clear()
        st.success("å·²æ¸…ç†ä¸´æ—¶ç¼“å­˜æ–‡ä»¶")
    except Exception as e:
        st.warning(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

def get_maintenance_stats():
    """è·å–ç»´æŠ¤ç›¸å…³çš„ç»Ÿè®¡æ•°æ®"""
    data_dir = ROOT_DIR / "data"
    entities_tmp_file = data_dir / "tmp" / "entities_tmp.json"
    events_tmp_file = data_dir / "tmp" / "abstract_to_event_map_tmp.json"
    extracted_dir = data_dir / "tmp"
    deduped_dir = data_dir / "tmp" / "deduped_news"
    raw_dir = data_dir / "tmp" / "raw_news"

    @st.cache_data(ttl=60)
    def load_json_cached(path: Path):
        try:
            if path.exists():
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            st.warning(f"{path.name} è¯»å–å¤±è´¥: {e}")
        return {}

    entities_tmp = load_json_cached(entities_tmp_file)
    events_tmp = load_json_cached(events_tmp_file)

    @st.cache_data(ttl=60)
    def list_extracted_files(base: Path):
        files = sorted(base.glob("extracted_events_*.jsonl"), key=lambda x: x.stat().st_mtime, reverse=True)
        return [str(f) for f in files]

    @st.cache_data(ttl=60)
    def list_news_files(base: Path, pattern: str):
        files = sorted(base.glob(pattern), key=lambda x: x.stat().st_mtime, reverse=True)
        return [str(f) for f in files]

    extracted_files = list_extracted_files(extracted_dir)
    deduped_files = list_news_files(deduped_dir, "*.jsonl")
    raw_files = list_news_files(raw_dir, "*.jsonl")

    return {
        'entities_tmp': entities_tmp,
        'events_tmp': events_tmp,
        'extracted_files': extracted_files,
        'deduped_files': deduped_files,
        'raw_files': raw_files
    }

def render_maintenance_metrics(stats):
    """æ¸²æŸ“ç»´æŠ¤ç»Ÿè®¡æŒ‡æ ‡"""
    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("ä¸´æ—¶å®ä½“ï¼ˆç¼“å­˜æ¡æ•°ï¼‰", len(stats['entities_tmp']))
    c2.metric("ä¸´æ—¶äº‹ä»¶ï¼ˆç¼“å­˜æ¡æ•°ï¼‰", len(stats['events_tmp']))
    c3.metric("æå–ç»“æœæ–‡ä»¶æ•°", len(stats['extracted_files']))
    c4.metric("å»é‡æ–°é—»æ–‡ä»¶æ•°", len(stats['deduped_files']))
    c5.metric("åŸå§‹æ–°é—»æ–‡ä»¶æ•°", len(stats['raw_files']))

def render_temp_data_preview(stats):
    """æ¸²æŸ“ä¸´æ—¶æ•°æ®é¢„è§ˆ"""
    with st.expander("æŸ¥çœ‹ä¸´æ—¶å®ä½“ / äº‹ä»¶ç¤ºä¾‹", expanded=False):
        render_entities_preview(stats['entities_tmp'])
        render_events_preview(stats['events_tmp'])
        render_files_preview("æå–ç»“æœæ–‡ä»¶", stats['extracted_files'])
        render_files_preview("å»é‡æ–°é—»æ–‡ä»¶", stats['deduped_files'])
        render_files_preview("åŸå§‹æ–°é—»æ–‡ä»¶", stats['raw_files'])

def render_entities_preview(entities_tmp):
    """æ¸²æŸ“å®ä½“é¢„è§ˆ"""
    if entities_tmp:
        df_ent = pd.DataFrame([
            {
                "name": k,
                "first_seen": v.get("first_seen", ""),
                "sources": ",".join([
                    (s.get("name") or s.get("id") or s.get("url") or str(s))
                    if isinstance(s, dict) else str(s)
                    for s in v.get("sources", [])
                ])[:80],
            }
            for k, v in list(entities_tmp.items())[:50]
        ])
        st.write("ä¸´æ—¶å®ä½“ï¼ˆæœ€å¤š50æ¡é¢„è§ˆï¼‰")
        st.dataframe(df_ent, use_container_width=True)
    else:
        st.info("æš‚æ— ä¸´æ—¶å®ä½“æ•°æ®")

def render_events_preview(events_tmp):
    """æ¸²æŸ“äº‹ä»¶é¢„è§ˆ"""
    if events_tmp:
        df_evt = pd.DataFrame([
            {
                "abstract": k,
                "first_seen": v.get("first_seen", ""),
                "entities": ",".join(v.get("entities", []))[:80]
            }
            for k, v in list(events_tmp.items())[:50]
        ])
        st.write("ä¸´æ—¶äº‹ä»¶ï¼ˆæœ€å¤š50æ¡é¢„è§ˆï¼‰")
        st.dataframe(df_evt, use_container_width=True)
    else:
        st.info("æš‚æ— ä¸´æ—¶äº‹ä»¶æ•°æ®")

def render_files_preview(title, files):
    """æ¸²æŸ“æ–‡ä»¶åˆ—è¡¨é¢„è§ˆ"""
    if files:
        st.write(f"{title}ï¼ˆæœ€æ–°5ä¸ªï¼‰")
        st.table({"path": files[:5]})
    else:
        st.info(f"æš‚æ— {title.lower()}")


def render_template_management():
    """æ¸²æŸ“Pipelineæ¨¡æ¿ç®¡ç†"""
    with st.expander("ğŸ“š Pipelineæ¨¡æ¿", expanded=False):
        template_name = st.text_input("æ¨¡æ¿åç§°", key="template_name")
        col_save_template, col_load_template = st.columns(2)

        with col_save_template:
            if st.button("ğŸ’¾ ä¿å­˜æ¨¡æ¿", use_container_width=True, disabled=not template_name or not st.session_state.pipeline_steps, key="pipeline_save_template"):
                save_pipeline_template(template_name)

        with col_load_template:
            render_template_loader()

def save_pipeline_template(template_name):
    """ä¿å­˜Pipelineæ¨¡æ¿"""
    try:
        template_data = {
            "name": template_name,
            "steps": st.session_state.pipeline_steps.copy(),
            "created_at": datetime.now().isoformat(),
            "tool_count": len(st.session_state.pipeline_steps)
        }

        # ä¿å­˜åˆ°session_state (å¯ä»¥æ‰©å±•åˆ°æ–‡ä»¶ç³»ç»Ÿ)
        if "pipeline_templates" not in st.session_state:
            st.session_state.pipeline_templates = {}

        st.session_state.pipeline_templates[template_name] = template_data
        st.success(f"æ¨¡æ¿ '{template_name}' å·²ä¿å­˜ï¼")
    except Exception as e:
        st.error(f"ä¿å­˜å¤±è´¥: {e}")

def render_template_loader():
    """æ¸²æŸ“æ¨¡æ¿åŠ è½½å™¨"""
    if "pipeline_templates" in st.session_state and st.session_state.pipeline_templates:
        template_options = list(st.session_state.pipeline_templates.keys())
        selected_template = st.selectbox("é€‰æ‹©æ¨¡æ¿", [""] + template_options, key="load_template")

        if selected_template and st.button("ğŸ“‚ åŠ è½½æ¨¡æ¿", use_container_width=True, key="pipeline_load_template"):
            load_pipeline_template(selected_template)
    else:
        st.caption("æš‚æ— ä¿å­˜çš„æ¨¡æ¿")

def load_pipeline_template(selected_template):
    """åŠ è½½Pipelineæ¨¡æ¿"""
    try:
        template_data = st.session_state.pipeline_templates[selected_template]
        st.session_state.pipeline_steps = template_data["steps"].copy()
        st.success(f"æ¨¡æ¿ '{selected_template}' å·²åŠ è½½ ({template_data['tool_count']} ä¸ªæ­¥éª¤)")
        st.rerun()
    except Exception as e:
        st.error(f"åŠ è½½å¤±è´¥: {e}")

def render_pipeline_toolbar():
    """æ¸²æŸ“Pipelineå·¥å…·æ """
    c_add, c_clear, c_reorder = st.columns([2, 1, 1])
    with c_add:
        add_new_pipeline_step()

    with c_clear:
        clear_all_pipeline_steps()

    with c_reorder:
        reorder_pipeline_steps()

def add_new_pipeline_step():
    """æ·»åŠ æ–°çš„Pipelineæ­¥éª¤"""
    tools = FunctionRegistry.get_all_tools()
    selected_tool = st.selectbox("Select Tool", list(tools.keys()), label_visibility="collapsed")
    if st.button("â• Add Step", use_container_width=True, key="pipeline_add_step"):
        st.session_state.pipeline_steps.append({
            "id": f"step_{len(st.session_state.pipeline_steps) + 1}",
            "tool": selected_tool,
            "inputs": {}
        })
        st.rerun()

def clear_all_pipeline_steps():
    """æ¸…ç©ºæ‰€æœ‰Pipelineæ­¥éª¤"""
    if st.button("ğŸ§¹ Clear All", use_container_width=True, disabled=not st.session_state.pipeline_steps, key="pipeline_clear_all"):
        st.session_state.pipeline_steps = []
        st.success("å·²æ¸…ç©ºæ‰€æœ‰æ­¥éª¤")
        st.rerun()

def reorder_pipeline_steps():
    """é‡æ–°ç¼–å·Pipelineæ­¥éª¤"""
    if len(st.session_state.pipeline_steps) > 1:
        if st.button("ğŸ”„ Reorder", use_container_width=True, key="pipeline_reorder_steps"):
            # ç®€å•çš„é‡æ–°ç¼–å·
            for i, step in enumerate(st.session_state.pipeline_steps):
                step["id"] = f"step_{i + 1}"
            st.success("æ­¥éª¤å·²é‡æ–°ç¼–å·")
            st.rerun()

def render_pipeline_steps():
    """æ¸²æŸ“Pipelineæ­¥éª¤åˆ—è¡¨"""
    if not st.session_state.pipeline_steps:
        st.info("No steps added. Select a tool to start.")
        return

    tools = FunctionRegistry.get_all_tools()

    for i, step in enumerate(st.session_state.pipeline_steps):
        tool_name = step["tool"]
        tool_meta = tools.get(tool_name, {})

        with st.expander(f"Step {i+1}: {tool_name}", expanded=False):
            render_step_header(i, step)
            render_step_content(i, step, tool_meta)

def render_step_header(i, step):
    """æ¸²æŸ“æ­¥éª¤å¤´éƒ¨ï¼ˆIDå’Œæ§åˆ¶æŒ‰é’®ï¼‰"""
    c_id, c_move, c_del = st.columns([3, 1.5, 1])
    step["id"] = c_id.text_input("ID", step["id"], key=f"id_{i}")

    # æ­¥éª¤ç§»åŠ¨æŒ‰é’®
    with c_move:
        render_step_move_buttons(i)

    # åˆ é™¤æŒ‰é’®
    if c_del.button("ğŸ—‘ï¸", key=f"del_{i}"):
        st.session_state.pipeline_steps.pop(i)
        st.rerun()

def render_step_move_buttons(i):
    """æ¸²æŸ“æ­¥éª¤ç§»åŠ¨æŒ‰é’®"""
    col_up, col_down = st.columns(2)
    with col_up:
        if st.button("â¬†ï¸", key=f"up_{i}", disabled=i==0):
            # ä¸Šç§»æ­¥éª¤
            st.session_state.pipeline_steps[i], st.session_state.pipeline_steps[i-1] = \
            st.session_state.pipeline_steps[i-1], st.session_state.pipeline_steps[i]
            st.rerun()
    with col_down:
        if st.button("â¬‡ï¸", key=f"down_{i}", disabled=i==len(st.session_state.pipeline_steps)-1):
            # ä¸‹ç§»æ­¥éª¤
            st.session_state.pipeline_steps[i], st.session_state.pipeline_steps[i+1] = \
            st.session_state.pipeline_steps[i+1], st.session_state.pipeline_steps[i]
            st.rerun()

def render_step_content(i, step, tool_meta):
    """æ¸²æŸ“æ­¥éª¤å†…å®¹ï¼ˆæè¿°ã€å‚æ•°ã€è¾“å‡ºï¼‰"""
    st.caption(tool_meta.get("description", ""))

    # å‚æ•°ç¼–è¾‘åŒº
    params = tool_meta.get("parameters", {})
    if params:
        for p_name, p_info in params.items():
            render_input_field(i, p_name, p_info, step.get("inputs", {}), step)
    else:
        st.info("æ— å‚æ•°")

    step["output"] = st.text_input("Output to ($var)", step.get("output", ""), key=f"out_{i}")
def render_custom_builder():
    """æ¸²æŸ“è‡ªå®šä¹‰Pipelineæ„å»ºå™¨"""
    st.header("ğŸ› ï¸ Custom Pipeline Builder")

    # Pipelineæ¨¡æ¿ç®¡ç†
    col_template, _ = st.columns([1, 3])
    with col_template:
        render_template_management()

    col_builder, col_preview = st.columns([1.5, 1])

    with col_builder:
        # å·¥å…·æ 
        render_pipeline_toolbar()

        # æ­¥éª¤ç¼–è¾‘
        render_pipeline_steps()

    with col_preview:
        # Pipelineå†å²è®°å½•
        if "pipeline_history" not in st.session_state:
            st.session_state.pipeline_history = []

        with st.expander("ğŸ“š æ‰§è¡Œå†å²", expanded=False):
            if st.session_state.pipeline_history:
                for i, hist in enumerate(reversed(st.session_state.pipeline_history[-5:])):  # æœ€è¿‘5ä¸ª
                    col_time, col_status = st.columns([2, 1])
                    with col_time:
                        st.caption(f"{hist['timestamp']} - {hist['name']}")
                    with col_status:
                        if hist['status'] == 'success':
                            st.caption("âœ… æˆåŠŸ")
                        else:
                            st.caption("âŒ å¤±è´¥")
            else:
                st.caption("æš‚æ— æ‰§è¡Œå†å²")

        st.subheader("Preview")
        pipeline_def = {"name": "Custom Pipeline", "steps": st.session_state.pipeline_steps}
        st.code(yaml.dump(pipeline_def, sort_keys=False), language="yaml")

        # PipelineéªŒè¯
        if st.button("ğŸ” éªŒè¯Pipeline", use_container_width=True, disabled=not st.session_state.pipeline_steps, key="pipeline_validate"):
            validation_errors = []

            # æ£€æŸ¥æ­¥éª¤å®Œæ•´æ€§
            for i, step in enumerate(st.session_state.pipeline_steps):
                if not step.get("tool"):
                    validation_errors.append(f"æ­¥éª¤ {i+1}: ç¼ºå°‘å·¥å…·é€‰æ‹©")
                if not step.get("id"):
                    validation_errors.append(f"æ­¥éª¤ {i+1}: ç¼ºå°‘æ­¥éª¤ID")

                # æ£€æŸ¥å¿…éœ€å‚æ•°
                tool_meta = tools.get(step.get("tool", ""))
                if tool_meta:
                    params = tool_meta.get("parameters", [])
                    for param in params:
                        if param.get("required", False):
                            param_name = param["name"]
                            if param_name not in step.get("inputs", {}):
                                validation_errors.append(f"æ­¥éª¤ {i+1} ({step.get('tool')}): ç¼ºå°‘å¿…éœ€å‚æ•° '{param_name}'")

            if validation_errors:
                st.error("PipelineéªŒè¯å¤±è´¥:")
                for error in validation_errors:
                    st.write(f"â€¢ {error}")
            else:
                st.success("âœ… PipelineéªŒè¯é€šè¿‡ï¼æ‰€æœ‰æ­¥éª¤é…ç½®æ­£ç¡®ã€‚")

        if st.button("ğŸš€ Run Pipeline", type="primary", use_container_width=True, disabled=not st.session_state.pipeline_steps, key="pipeline_run"):
            execute_pipeline(pipeline_def)

def render_snapshots_tab():
    st.header("ğŸ“¸ Knowledge Graph Snapshots")
    st.caption("ç”Ÿæˆ/æŸ¥çœ‹å¯è§†åŒ–å¿«ç…§ï¼ˆkg_visual.json / kg_visual_timeline.jsonï¼‰")
    if st.button("ç”Ÿæˆå¿«ç…§", type="primary", key="snapshots_generate"):
        try:
            from src.functions.graph_ops import generate_kg_visual_snapshots
            res = generate_kg_visual_snapshots()
            st.success(f"ç”Ÿæˆå®Œæˆ: {res}")
        except Exception as e:
            st.error(f"ç”Ÿæˆå¤±è´¥: {e}")
    data_root = ROOT_DIR / "data"
    vis_path = data_root / "kg_visual.json"
    tl_path = data_root / "kg_visual_timeline.json"
    st.write("å¿«ç…§æ–‡ä»¶è·¯å¾„ï¼š")
    st.write(f"- å›¾è°±å¿«ç…§: {vis_path}")
    st.write(f"- æ—¶é—´çº¿å¿«ç…§: {tl_path}")
    for p in [vis_path, tl_path]:
        if p.exists():
            ts = datetime.fromtimestamp(p.stat().st_mtime).strftime("%Y-%m-%d %H:%M:%S")
            st.info(f"{p.name} å·²å­˜åœ¨ï¼Œå¤§å° {p.stat().st_size} å­—èŠ‚ï¼Œä¿®æ”¹æ—¶é—´ {ts}")
        else:
            st.warning(f"{p.name} å°šæœªç”Ÿæˆ")


def get_tool_usage_stats():
    """è·å–å·¥å…·ä½¿ç”¨ç»Ÿè®¡"""
    tool_stats = {}
    if "pipeline_steps" in st.session_state and st.session_state.pipeline_steps:
        for step in st.session_state.pipeline_steps:
            tool_name = step.get("tool", "")
            if tool_name:
                tool_stats[tool_name] = tool_stats.get(tool_name, 0) + 1
    return tool_stats

def render_tool_usage_stats(tool_stats):
    """æ¸²æŸ“å·¥å…·ä½¿ç”¨ç»Ÿè®¡"""
    if tool_stats:
        st.info(f"ğŸ“Š å½“å‰Pipelineä½¿ç”¨äº† {len(tool_stats)} ç§ä¸åŒå·¥å…·ï¼Œæ€»è®¡ {sum(tool_stats.values())} ä¸ªæ­¥éª¤")

        # å·¥å…·ä½¿ç”¨é¢‘ç‡
        sorted_tools = sorted(tool_stats.items(), key=lambda x: x[1], reverse=True)
        with st.expander("ğŸ”¥ å½“å‰Pipelineå·¥å…·ä½¿ç”¨ç»Ÿè®¡", expanded=False):
            for tool_name, count in sorted_tools:
                st.write(f"â€¢ **{tool_name}**: {count} æ¬¡ä½¿ç”¨")

def get_tool_categories():
    """è·å–å·¥å…·åˆ†ç±»é…ç½®"""
    return {
        "Data Fetch": ["fetch", "search", "scrape", "crawl"],
        "Extraction": ["extract", "process", "parse", "llm"],
        "Graph Ops": ["graph", "update", "refresh", "merge", "kg", "node", "edge"],
        "Reporting": ["report", "markdown", "summary", "export"],
        "Utility": ["save", "load", "tmp", "debug", "test"],
    }

def categorize_tools(all_tools):
    """å¯¹å·¥å…·è¿›è¡Œåˆ†ç±»"""
    CATEGORY_ORDER = get_tool_categories()

    def get_category(tool_name: str) -> str:
        name_lower = tool_name.lower()
        for cat, keywords in CATEGORY_ORDER.items():
            if any(k in name_lower for k in keywords):
                return cat
        return "Other"

    # æ·»åŠ åˆ†ç±»
    categorized = {}
    for name, meta in all_tools.items():
        cat = meta.get("category") or get_category(name)  # æ”¯æŒæ‰‹åŠ¨ category
        categorized.setdefault(cat, []).append((name, meta))

    return categorized

def render_tool_explorer_tab():
    """æ¸²æŸ“å·¥å…·æ¢ç´¢å™¨æ ‡ç­¾é¡µ"""
    st.header("Tool Explorer")
    st.caption("è‡ªåŠ¨å‘ç°æ‰€æœ‰æ³¨å†Œå·¥å…· Â· æ”¯æŒæœç´¢ã€é¢„è§ˆã€å¤åˆ¶ã€ä¸€é”®æ‰§è¡Œ")

    # 1. è‡ªåŠ¨åŠ è½½æ‰€æœ‰çœŸå®å·¥å…·ï¼ˆæ ¸å¿ƒï¼ï¼‰
    all_tools = FunctionRegistry.get_all_tools()  # <-- ä½ çš„çœŸå®æ³¨å†Œè¡¨
    if not all_tools:
        st.warning("æœªæ£€æµ‹åˆ°å·²æ³¨å†Œçš„å·¥å…·ï¼Œè¯·æ£€æŸ¥ FunctionRegistry")
        return

    # å·¥å…·ä½¿ç”¨ç»Ÿè®¡
    tool_stats = get_tool_usage_stats()
    render_tool_usage_stats(tool_stats)

    # å¯¹å·¥å…·è¿›è¡Œåˆ†ç±»
    categorized = categorize_tools(all_tools)

    # æ™ºèƒ½æ¨è
    render_smart_recommendations(all_tools, categorized)

    # æœç´¢å’Œæ‰¹é‡æ“ä½œ
    search_query = render_search_and_batch_tools(categorized)

    # è¿‡æ»¤å·¥å…·
    if search_query:
        categorized = filter_tools_by_search(categorized, search_query)

    # æ¸²æŸ“å·¥å…·ç½‘æ ¼
    render_tool_grid(categorized)

def render_smart_recommendations(all_tools, categorized):
    """æ¸²æŸ“æ™ºèƒ½æ¨è"""
    current_tools = set()
    if "pipeline_steps" in st.session_state:
        current_tools = {step.get("tool") for step in st.session_state.pipeline_steps if step.get("tool")}

    if current_tools:
        # åŸºäºå½“å‰Pipelineæ¨èç›¸å…³å·¥å…·
        recommendations = []
        for tool_name, meta in all_tools.items():
            if tool_name not in current_tools:
                category = meta.get("category") or get_category_from_categorized(categorized, tool_name)
                # æ¨èç›¸åŒç±»åˆ«çš„å·¥å…·
                current_categories = {get_category_from_categorized(categorized, step.get("tool", "")) for step in st.session_state.pipeline_steps if step.get("tool")}
                if category in current_categories:
                    recommendations.append((tool_name, category))

        if recommendations:
            with st.expander(f"ğŸ’¡ æ™ºèƒ½æ¨è ({len(recommendations)})", expanded=False):
                for tool_name, category in recommendations[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ªæ¨è
                    st.write(f"â€¢ **{tool_name}** ({category})")

def get_category_from_categorized(categorized, tool_name):
    """ä»å·²åˆ†ç±»çš„å·¥å…·ä¸­è·å–ç±»åˆ«"""
    for category, tools in categorized.items():
        if any(name == tool_name for name, _ in tools):
            return category
    return "Other"

def render_search_and_batch_tools(categorized):
    """æ¸²æŸ“æœç´¢å’Œæ‰¹é‡æ“ä½œå·¥å…·æ """
    col_search, col_batch = st.columns([3, 1])
    with col_search:
        # 3. æœç´¢æ¡†
        search = st.text_input("Search Tools", placeholder="è¾“å…¥å·¥å…·åæˆ–æè¿°å…³é”®è¯...", key="tool_search")

    with col_batch:
        # æ‰¹é‡å¤åˆ¶åŠŸèƒ½
        if st.button("ğŸ“‹ æ‰¹é‡å¤åˆ¶é€‰ä¸­", use_container_width=True, disabled=len(categorized) == 0, key="tool_batch_copy_selected"):
            handle_batch_copy(categorized)

    return search

def handle_batch_copy(categorized):
    """å¤„ç†æ‰¹é‡å¤åˆ¶åŠŸèƒ½"""
    if "selected_tools" not in st.session_state:
        st.session_state.selected_tools = []

    if st.session_state.selected_tools:
        # ç”Ÿæˆæ‰¹é‡YAML
        batch_steps = []
        all_tools = FunctionRegistry.get_all_tools()

        for tool_name in st.session_state.selected_tools:
            if tool_name in all_tools:
                step_yaml = {
                    "id": tool_name,
                    "tool": tool_name,
                    "inputs": {},
                    "output": f"{tool_name}_result"
                }
                batch_steps.append(step_yaml)

        if batch_steps:
            batch_yaml = yaml.dump({"steps": batch_steps}, sort_keys=False, allow_unicode=True)
            st.code(batch_yaml, language="yaml")
            st.success(f"å·²ç”Ÿæˆ {len(batch_steps)} ä¸ªå·¥å…·çš„æ‰¹é‡é…ç½®")
    else:
        st.warning("è¯·å…ˆé€‰æ‹©è¦æ‰¹é‡å¤åˆ¶çš„å·¥å…·")

    st.session_state.selected_tools = []  # é‡ç½®é€‰æ‹©

def filter_tools_by_search(categorized, search):
    """æ ¹æ®æœç´¢æ¡ä»¶è¿‡æ»¤å·¥å…·"""
    if not search:
        return categorized

    filtered = {}
    for cat, tools in categorized.items():
        matched = []
        for name, meta in tools:
            if (search.lower() in name.lower() or
                (meta.get("description") and search.lower() in meta.get("description", "").lower())):
                matched.append((name, meta))
        if matched:
            filtered[cat] = matched
    return filtered

def render_tool_grid(categorized):
    """æ¸²æŸ“å·¥å…·ç½‘æ ¼"""
    # 4. ä¸»æ¸²æŸ“åŒº - å“åº”å¼å¡ç‰‡æµ
    for category, tools in categorized.items():
        with st.expander(f"**{category}** Â· {len(tools)} tools", expanded=True):
            cols = st.columns(3, gap="medium")  # æ¯è¡Œ3ä¸ªå¡ç‰‡ï¼Œå¯æ”¹æˆ2æˆ–4
            for idx, (tool_name, meta) in enumerate(tools):
                with cols[idx % 3]:
                    render_tool_card(tool_name, meta, idx)

def render_tool_card(tool_name, meta, idx):
    """æ¸²æŸ“å•ä¸ªå·¥å…·å¡ç‰‡"""
    with st.container(border=True):
        # æ‰¹é‡é€‰æ‹©å¤é€‰æ¡†
        selected = st.checkbox("", key=f"select_{tool_name}_{idx}", label_visibility="collapsed")
        if selected:
            if "selected_tools" not in st.session_state:
                st.session_state.selected_tools = []
            if tool_name not in st.session_state.selected_tools:
                st.session_state.selected_tools.append(tool_name)
        elif "selected_tools" in st.session_state and tool_name in st.session_state.selected_tools:
            st.session_state.selected_tools.remove(tool_name)

        st.markdown(f"**`{tool_name}`**")

        desc = meta.get("description") or "No description"
        st.caption(desc)

        # å‚æ•°è¡¨å•ï¼ˆå¯ç¼–è¾‘ï¼‰
        params = meta.get("parameters", {})
        if params:
            render_tool_parameters(tool_name, meta, idx)
        else:
            st.info("No parameters")

        # åº•éƒ¨æ ‡ç­¾
        render_tool_tags(tool_name, meta)

def render_tool_parameters(tool_name, meta, idx):
    """æ¸²æŸ“å·¥å…·å‚æ•°è¡¨å•"""
    with st.form(key=f"form_{tool_name}_{idx}", clear_on_submit=False, border=False):
        inputs = {}
        for p_name, p_info in meta.get("parameters", {}).items():
            p_type = p_info.get("type", "str")
            default = p_info.get("default")
            desc = p_info.get("description", "")

            label = f"{p_name}{' *' if p_info.get('required') else ''}"

            if p_type == "bool":
                val = st.checkbox(label, value=bool(default), help=desc)
            elif p_type in ("int", "integer"):
                val = st.number_input(label, value=int(default) if default is not None else 0, step=1, help=desc)
            elif p_type == "float":
                val = st.number_input(label, value=float(default) if default is not None else 0.0, step=0.1, help=desc)
            elif p_type in ("list", "dict", "json"):
                val_str = json.dumps(default, ensure_ascii=False) if default is not None else "[]"
                val_input = st.text_area(label, value=val_str, height=80, help=desc + "\næ”¯æŒ JSON æˆ– $å˜é‡å¼•ç”¨")
                if val_input.strip().startswith("$"):
                    inputs[p_name] = val_input.strip()
                else:
                    try:
                        inputs[p_name] = json.loads(val_input) if val_input.strip() else None
                    except:
                        inputs[p_name] = val_input  # å…è®¸å­—ç¬¦ä¸²
            else:
                val = st.text_input(label, value=str(default) if default is not None else "", help=desc)
                inputs[p_name] = val if val else None

        col_run, col_copy = st.columns([1, 2])
        with col_run:
            run_now = st.form_submit_button("Run", type="primary", use_container_width=True)
        with col_copy:
            copy_step = st.form_submit_button("Copy as Step", use_container_width=True)

        # ä¸€é”®è¿è¡Œï¼ˆè°ƒè¯•ç¥å™¨ï¼ï¼‰
        if run_now:
            execute_tool_debug(tool_name, inputs)

        # ä¸€é”®å¤åˆ¶ä¸º Pipeline Step
        if copy_step:
            copy_tool_as_step(tool_name, inputs)

def execute_tool_debug(tool_name, inputs):
    """æ‰§è¡Œå·¥å…·è°ƒè¯•"""
    with st.spinner(f"Running {tool_name}..."):
        try:
            context = PipelineContext()
            engine = PipelineEngine(context)
            result = asyncio.run(engine.run_task({
                "id": f"debug_{tool_name}",
                "tool": tool_name,
                "inputs": inputs
            }))
            st.success("Success!")
            st.json(result, expanded=False)
        except Exception as e:
            st.error(f"Failed: {e}")

def copy_tool_as_step(tool_name, inputs):
    """å¤åˆ¶å·¥å…·ä¸ºPipelineæ­¥éª¤"""
    step_yaml = {
        "id": tool_name,
        "tool": tool_name,
        "inputs": inputs,
        "output": f"{tool_name}_result"
    }
    yaml_str = yaml.dump(step_yaml, sort_keys=False, allow_unicode=True)
    st.code(yaml_str, language="yaml")
    st.toast("å·²å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼ˆæ¨¡æ‹Ÿï¼‰", icon="clipboard")
    # çœŸå®å¤åˆ¶åˆ°å‰ªè´´æ¿ï¼ˆStreamlit 1.30+ï¼‰
    try:
        st.code(yaml_str, language="yaml")
        st.success("Step YAML å·²ç”Ÿæˆï¼Œå¯ç›´æ¥ç²˜è´´åˆ° Custom Builder")
    except:
        pass

def render_tool_tags(tool_name, meta):
    """æ¸²æŸ“å·¥å…·æ ‡ç­¾"""
    tags = []
    if meta.get("async"): tags.append("async")
    if "llm" in tool_name.lower(): tags.append("LLM")
    if tags:
        st.caption(" Â· ".join(tags))

# --- ä¸»å¯¼èˆª ---
tabs = st.tabs(["Configuration","Ingestion", "Expansion", "Maintenance", "Snapshots", "Tools", "Custom Builder"])

with tabs[0]: render_configuration_tab()
with tabs[1]: render_ingestion_tab()
with tabs[2]: render_expansion_tab()
with tabs[3]: render_maintenance_tab()
with tabs[4]: render_snapshots_tab()
with tabs[5]: render_tool_explorer_tab()
with tabs[6]: render_custom_builder()

