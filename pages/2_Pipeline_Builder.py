import streamlit as st
import yaml
import asyncio
import sys
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import time
import threading
from datetime import timezone
from dotenv import dotenv_values

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
ROOT_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(ROOT_DIR))
ENV_PATH = ROOT_DIR / "config" / ".env.local"

from src.core.registry import FunctionRegistry
from src.core.engine import PipelineEngine
from src.core.context import PipelineContext
from src.data.api_client import DataAPIPool, get_apis_config
from src.web import utils
import src.functions.data_fetch
import src.functions.extraction
import src.functions.graph_ops
import src.functions.reporting

st.set_page_config(page_title="Pipeline Builder - Market Lens", page_icon="â›“ï¸", layout="wide")

# --- å…¨å±€ä»»åŠ¡ç®¡ç†å™¨ ---

class GlobalTaskManager:
    def __init__(self):
        self.is_running = False
        self.logs = []
        self.status_info = {"label": "Idle", "state": "idle", "expanded": False}
        self.current_step_idx = 0
        self.total_steps = 0
        self.final_report = None
        self._lock = threading.Lock()
        
    def start_task(self, pipeline_def):
        if self.is_running:
            return False
            
        self.is_running = True
        self.logs = []
        self.status_info = {"label": "Starting...", "state": "running", "expanded": True}
        self.final_report = None
        self.current_step_idx = 0
        steps = pipeline_def.get("steps", [])
        self.total_steps = len(steps)
        
        def _worker():
            asyncio.run(self._async_runner(pipeline_def))
            
        thread = threading.Thread(target=_worker, daemon=True)
        thread.start()
        return True
        
    async def _async_runner(self, pipeline_def):
        def log_callback(entry):
            with self._lock:
                ts = entry['timestamp'].split('T')[1][:8]
                msg = f"[{ts}] [{entry['level']}] {entry['message']}"
                self.logs.append(msg)
                # ä¿ç•™æœ€è¿‘ 1000 æ¡æ—¥å¿—
                if len(self.logs) > 1000:
                    self.logs.pop(0)

        context = PipelineContext(log_callback=log_callback)
        engine = PipelineEngine(context)
        
        steps = pipeline_def.get("steps", [])
        
        try:
            for i, step in enumerate(steps):
                step_id = step.get('id')
                self.current_step_idx = i + 1
                
                # æ›´æ–°çŠ¶æ€
                with self._lock:
                    self.status_info = {
                        "label": f"Executing Step {self.current_step_idx}/{self.total_steps}: **{step_id}**", 
                        "state": "running", 
                        "expanded": True
                    }
                
                # æ‰§è¡Œä»»åŠ¡
                await engine.run_task(step)
                
            # å®Œæˆ
            with self._lock:
                self.status_info = {"label": "âœ… Pipeline Execution Completed!", "state": "complete", "expanded": False}
                self.final_report = context.get("final_report_md")
                
        except Exception as e:
            with self._lock:
                self.status_info = {"label": f"âŒ Execution Failed: {str(e)}", "state": "error", "expanded": True}
                self.logs.append(f"[System] Error: {str(e)}")
        finally:
            self.is_running = False

@st.cache_resource
def get_task_manager():
    return GlobalTaskManager()

task_manager = get_task_manager()

# --- UI ç»„ä»¶ ---

st.title("Task Center & Pipeline Builder")

# ä»»åŠ¡ç›‘æ§åŒº (å§‹ç»ˆæ˜¾ç¤ºåœ¨é¡¶éƒ¨)
def render_task_monitor():
    if task_manager.is_running or task_manager.status_info["state"] != "idle":
        with st.container(border=True):
            col_status, col_ctrl = st.columns([4, 1])
            
            with col_status:
                # ä½¿ç”¨ st.status å±•ç¤ºçŠ¶æ€
                state = task_manager.status_info["state"]
                label = task_manager.status_info["label"]
                expanded = task_manager.status_info["expanded"]
                
                status_container = st.status(label, expanded=expanded, state=state)
                
                # æ˜¾ç¤ºæœ€åå‡ æ¡æ—¥å¿—
                with status_container:
                    st.write("Recent Logs:")
                    with task_manager._lock:
                        recent_logs = task_manager.logs[-10:]
                    st.code("\n".join(recent_logs) if recent_logs else "Initializing...", language="text")
            
            with col_ctrl:
                if task_manager.is_running:
                    st.caption("Running in background...")
                    if st.button("ğŸ”„ Refresh View"):
                        st.rerun()
                    # è‡ªåŠ¨åˆ·æ–°é€»è¾‘ (å®éªŒæ€§)
                    time.sleep(2)
                    st.rerun()
                else:
                    if st.button("Clear Status"):
                        task_manager.status_info["state"] = "idle"
                        st.rerun()

        # ç»“æœå±•ç¤º
        if not task_manager.is_running and task_manager.final_report:
            with st.expander("ğŸ“„ Final Report Result", expanded=True):
                st.markdown(task_manager.final_report)
                st.download_button(
                    "Download Report", 
                    task_manager.final_report, 
                    file_name=f"report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                )

render_task_monitor()

# åˆå§‹åŒ– Session State
if "pipeline_steps" not in st.session_state:
    st.session_state.pipeline_steps = []

# åˆå§‹åŒ– API é…ç½® (ä»…è¿è¡Œä¸€æ¬¡)
if "ingestion_apis" not in st.session_state:
    st.session_state.ingestion_apis = utils.get_default_api_sources_df()

if "expansion_tasks" not in st.session_state:
    # åˆå§‹åŒ– expansion_tasks (ç©º) â€”â€” ä½¿ç”¨å½“å‰æ”¯æŒçš„å­—æ®µ
    st.session_state.expansion_tasks = pd.DataFrame(
        columns=["enabled", "keyword", "limit", "category", "from", "to", "sortby"]
    ).astype({
        "enabled": "bool",
        "keyword": "str",
        "limit": "int",
        "category": "str",
        "from": "str",
        "to": "str",
        "sortby": "str",
    })

# åˆå§‹åŒ– .env.local Key-Value
def load_env_df():
    kv = dotenv_values(ENV_PATH) if ENV_PATH.exists() else {}
    rows = [{"key": k, "value": v or ""} for k, v in kv.items()]
    return pd.DataFrame(rows, columns=["key", "value"])

if "env_kv" not in st.session_state:
    st.session_state.env_kv = load_env_df()

# --- è¾…åŠ©å‡½æ•° ---

def execute_pipeline(pipeline_def):
    """æäº¤ä»»åŠ¡åˆ°åå°ç®¡ç†å™¨"""
    if task_manager.is_running:
        st.warning("âš ï¸ A task is already running. Please wait for it to finish.")
        return

    success = task_manager.start_task(pipeline_def)
    if success:
        st.toast("ğŸš€ Task started in background!")
        st.rerun()
    else:
        st.error("Failed to start task.")

def render_input_field(step_idx, p_name, p_info, current_inputs, step):
    """
    æ™ºèƒ½æ¸²æŸ“è¾“å…¥ç»„ä»¶
    """
    p_type = p_info.get('type', 'Any')
    p_required = p_info.get('required', False)
    default_val = p_info.get('default')
    
    label = f"{p_name}{' *' if p_required else ''}"
    help_text = f"Type: {p_type}" + (f", Default: {default_val}" if default_val else "")
    key = f"in_{step_idx}_{p_name}"
    
    current_val = current_inputs.get(p_name, default_val)
    is_ref = isinstance(current_val, str) and current_val.startswith("$")
    
    if is_ref:
         new_val = st.text_input(label + " (Variable)", value=current_val, key=key, help=help_text)
         step["inputs"][p_name] = new_val
         return

    if "bool" in p_type.lower():
        val = st.checkbox(label, value=bool(current_val) if current_val is not None else False, key=key, help=help_text)
        step["inputs"][p_name] = val
        
    elif "int" in p_type.lower():
        val = st.number_input(label, value=int(current_val) if current_val is not None else 0, step=1, key=key, help=help_text)
        step["inputs"][p_name] = int(val)
        
    elif "list" in p_type.lower() or "dict" in p_type.lower():
        val_str = str(current_val) if current_val is not None else "[]"
        new_val_str = st.text_area(label + " (JSON/List)", value=val_str, height=100, key=key, help="Enter valid JSON or $variable")
        if new_val_str.startswith("$"):
            step["inputs"][p_name] = new_val_str
        else:
            try:
                import ast
                if new_val_str.strip():
                    parsed = ast.literal_eval(new_val_str)
                    step["inputs"][p_name] = parsed
            except:
                step["inputs"][p_name] = new_val_str
    else:
        val = st.text_input(label, value=str(current_val) if current_val is not None else "", key=key, help=help_text)
        if val:
            step["inputs"][p_name] = val

# --- åœºæ™¯åŒ–æ¨¡å—æ¸²æŸ“ ---
def render_configuration_tab():
    st.header("ğŸ“š Configuration")
    source_config, source_api_pool= st.tabs(["Source Configuration","Source API Pool"])
    with source_config:
        st.subheader("Source Configuration")
        st.write("Source Configuration")
        edited_df = st.data_editor(
            st.session_state.ingestion_apis,
            num_rows="dynamic",
            use_container_width=True,
            column_config={
                "enabled": st.column_config.CheckboxColumn("Enabled"),
                "name": st.column_config.TextColumn("Source Name", required=True),
                "type": st.column_config.SelectboxColumn("API Type", options=["gnews"], required=True),
                "language": st.column_config.SelectboxColumn("Language", options=["ar", "zh", "nl", "en", "fr", "de", "el", "he", "hi", "id", "it", "ja", "ml", "mr", "no", "pt", "pa", "ro", "ru", "es", "sv", "ta", "te", "tr", "uk"]),
                "timeout": st.column_config.NumberColumn("Timeout (s)"),
                "country": st.column_config.SelectboxColumn("Country", options=["ar", "au", "br", "ca", "cn", "co", "eg", "fr", "de", "gr", "hk", "in", "id", "ie", "il", "it", "jp", "my", "mx", "nl", "no", "pk", "pe", "ph", "pt", "ro", "ru", "sg", "es", "se", "ch", "tw", "tr", "ua", "gb", "us"]),
            },
            key="ingestion_editor_main"
        )
        st.session_state.ingestion_apis = edited_df
        
        # å®æ—¶æ˜¾ç¤ºé€‰ä¸­çš„æºæ•°é‡ï¼Œå¹¶ä¿å­˜å½“å‰é€‰æ‹©
        selected_apis = edited_df[edited_df["enabled"] == True]["name"].tolist()
        st.session_state["ingestion_selected_apis"] = selected_apis
        st.caption(f"âœ… Selected Sources: {len(selected_apis)}")

        # GNews å¯é€‰å‚æ•°é…ç½®
        gnews_params = st.session_state.get("gnews_params", {})
        with st.expander("GNews å¯é€‰å‚æ•°", expanded=False):
            category = st.selectbox(
                "Category",
                ["", "general", "world", "business", "technology", "sports", "science", "health", "entertainment"],
                index=0,
                help="ç•™ç©ºåˆ™ä¸æŒ‡å®šåˆ†ç±»"
            )
            query = st.text_input("Query (å…³é”®è¯æœç´¢ï¼Œå¯ç©º)")
            col_from, col_to = st.columns(2)
            min_date = datetime(2020, 1, 1).date()
            max_date = datetime.now().date()
            with col_from:
                d_from = st.date_input("From æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_from_date")
                t_from = st.time_input("From æ—¶é—´", value=None, key="gnews_from_time")
            with col_to:
                d_to = st.date_input("To æ—¥æœŸ", value=None, min_value=min_date, max_value=max_date, key="gnews_to_date")
                t_to = st.time_input("To æ—¶é—´", value=None, key="gnews_to_time")

            def combine(dt, tm):
                if dt is None:
                    return None
                tm = tm or datetime.min.time()
                # ç»Ÿä¸€ä½¿ç”¨ UTC è¾“å‡º ISO8601
                return datetime.combine(dt, tm, tzinfo=timezone.utc).isoformat().replace("+00:00", "Z")

            from_iso = combine(d_from, t_from)
            to_iso = combine(d_to, t_to)
            nullable = st.text_input("Nullable", value=gnews_params.get("nullable", ""), help="å¦‚ description,content")
            truncate = st.text_input("Truncate", value=gnews_params.get("truncate", ""), help="å¦‚ content")
            sortby = st.selectbox("Sortby", ["", "publishedAt", "relevance"], index=0)
            in_fields = st.text_input("In fields", value=gnews_params.get("in_fields", ""), help="å¦‚ title,description")
            page = st.number_input("Page", min_value=1, value=gnews_params.get("page", 1), step=1)

            # ä¿å­˜åˆ° session_state
            st.session_state["gnews_params"] = {
                "category": category or None,
                "query": query or None,
                "from_": from_iso,
                "to": to_iso,
                "nullable": nullable or None,
                "truncate": truncate or None,
                "sortby": sortby or None,
                "in_fields": in_fields or None,
                "page": int(page) if page else None,
            }

    with source_api_pool:
        st.subheader("Source API Pool")
        st.caption("ç¼–è¾‘å¹¶ä¿å­˜åˆ° config/.env.localï¼ˆè¦†ç›–å†™å…¥ï¼Œä¸ä¿ç•™æ³¨é‡Š/ç©ºè¡Œï¼‰ã€‚ä¸Šæ–¹â€œä¿å­˜è¯¥è¡Œ/è¡¨æ ¼ä¿®æ”¹â€ä»…æ›´æ–°å†…å­˜ï¼Œéœ€åœ¨ä¸‹æ–¹ç‚¹å‡»ä¿å­˜æ‰å†™å…¥æ–‡ä»¶ã€‚")
        edited_env = st.session_state.env_kv
        if st.checkbox("æ˜¾ç¤º Key/Value è¡¨æ ¼ç¼–è¾‘å™¨", value=False, key="env_editor_toggle"):
            edited_env = st.data_editor(
                st.session_state.env_kv,
                num_rows="dynamic",
                use_container_width=True,
                column_config={
                    "key": st.column_config.TextColumn("Key", required=True),
                    "value": st.column_config.TextColumn("Value", required=False, help="å¯è¾“å…¥å ä½ç¬¦ï¼Œæ³¨æ„é¿å…æ³„éœ²æ•æ„Ÿå€¼")
                },
                key="env_editor"
            )
            st.session_state.env_kv = edited_env
        def try_parse_json(val: str):
            if not isinstance(val, str):
                return None, "éå­—ç¬¦ä¸²"
            txt = val.strip()
            if not txt:
                return None, "ç©ºå€¼"
            try:
                obj = json.loads(txt)
                return obj, ""
            except Exception as e:
                return None, str(e)

        for _, row in edited_env.iterrows():
            idx = row.name
            k = str(row.get("key", "")).strip()
            v = str(row.get("value", "")).strip()
            if not k:
                continue
            parsed, err = try_parse_json(v)
            with st.expander(f"{k}", expanded=False):
                if parsed is not None:
                    pretty_txt = json.dumps(parsed, ensure_ascii=False, indent=2)
                    new_txt = st.text_area(
                        "JSON ç¼–è¾‘ï¼ˆä¿å­˜å³å†™å› valueï¼‰",
                        value=pretty_txt,
                        key=f"json_edit_{idx}",
                        height=200
                    )
                    if st.button("ä¿å­˜è¯¥è¡Œ", key=f"save_json_{idx}", use_container_width=True):
                        try:
                            parsed_new = json.loads(new_txt)
                            # å†™å›è¡¨æ ¼ç¼“å­˜ï¼Œä¿æŒç´§å‡‘å­˜å‚¨
                            edited_env.at[idx, "value"] = json.dumps(parsed_new, ensure_ascii=False)
                            st.session_state.env_kv = edited_env
                            st.success("å·²æ›´æ–°è¯¥è¡Œçš„ value")
                        except Exception as e:
                            st.error(f"JSON è§£æå¤±è´¥: {e}")
                    # è¡¨æ ¼åŒ–å±•ç¤ºï¼ˆä¼˜å…ˆ list[dict] æˆ– dict -> DataFrameï¼‰ï¼Œå¦åˆ™ç”¨ json
                    def to_table(obj):
                        if isinstance(obj, list) and obj and all(isinstance(x, dict) for x in obj):
                            return pd.DataFrame(obj), "list"
                        if isinstance(obj, dict):
                            return pd.DataFrame([obj]), "dict"
                        return None, ""
                    df_preview, kind = to_table(parsed)
                    if df_preview is not None and not df_preview.empty:
                        edited_df = st.data_editor(
                            df_preview,
                            num_rows="dynamic",
                            use_container_width=True,
                            key=f"env_table_{idx}"
                        )
                        if st.button("ä¿å­˜è¡¨æ ¼ä¿®æ”¹", key=f"save_table_{idx}", use_container_width=True):
                            try:
                                if kind == "list":
                                    new_obj = edited_df.to_dict(orient="records")
                                else:
                                    new_obj = edited_df.to_dict(orient="records")[0] if not edited_df.empty else {}
                                edited_env.at[idx, "value"] = json.dumps(new_obj, ensure_ascii=False)
                                st.session_state.env_kv = edited_env
                                st.success("å·²æ ¹æ®è¡¨æ ¼ä¿®æ”¹æ›´æ–° value")
                            except Exception as e:
                                st.error(f"å†™å› JSON å¤±è´¥: {e}")
                    else:
                        st.json(parsed)
                else:
                    st.warning(f"æ— æ³•è§£æä¸º JSONï¼š{err}")

        if st.button("ğŸ’¾ ä¿å­˜åˆ° .env.local", type="primary", use_container_width=True):
            lines = []
            for _, row in edited_env.iterrows():
                k = str(row.get("key", "")).strip()
                if not k:
                    continue
                v = str(row.get("value", "")).strip()
                lines.append(f"{k}={v}")
            ENV_PATH.parent.mkdir(parents=True, exist_ok=True)
            ENV_PATH.write_text("\n".join(lines) + "\n", encoding="utf-8")
            st.success(f"å·²å†™å…¥ {ENV_PATH.name} ï¼Œå…± {len(lines)} æ¡è®°å½•")



def render_ingestion_tab():
    st.header("ğŸ“¥ Data Ingestion")
    st.caption("Fetch news from sources (Feed/Search) and extract events.")
    
    st.subheader("Processing Parameters")
    
    col_p1, col_p2 = st.columns(2)
    with col_p1:
            st.markdown("##### ğŸ“¥ Fetch Settings")
            news_limit = st.number_input("Limit (per source)", 1, 10, 5, 1, help="Max news items to fetch per source.")
            
    with col_p2:
            st.markdown("##### âš™ï¸ Pipeline Actions")
            auto_update_kg = st.checkbox("Auto Update Knowledge Graph", True, help="Automatically extract entities and update the graph.")
            enable_report = st.checkbox("Generate Summary Report", True, help="Create a markdown report after processing.")

    st.subheader("ğŸš€ Ready to Start?")
    
    # æ±‡æ€»é…ç½®
    current_df = st.session_state.ingestion_apis
    selected_sources = current_df[current_df["enabled"] == True]["name"].tolist()
    
    st.write("Summary:")
    c1, c2, c3 = st.columns(3)
    c1.metric("Sources Selected", len(selected_sources))
    c2.metric("Max Items", news_limit)
    c3.metric("Auto-Update KG", "Yes" if auto_update_kg else "No")
    gnews_params = st.session_state.get("gnews_params", {})
    
    if not selected_sources:
        st.error("âŒ No sources selected. Please go back to 'Data Sources' tab.")
        btn_disabled = True
    else:
        btn_disabled = False
        
    if st.button("Start Ingestion Task", type="primary", disabled=btn_disabled, use_container_width=True):
        pipeline_def = {
            "name": "Data Ingestion Task",
            "steps": [
                {
                    "id": "fetch_news",
                    "tool": "fetch_news_stream",
                    "inputs": {
                        "limit": news_limit, 
                        "sources": selected_sources,
                        # GNews å¯é€‰å‚æ•°é€ä¼ ï¼ˆä»…å½“æœ‰å€¼ï¼‰
                        **{k: v for k, v in gnews_params.items() if v}
                    },
                    "output": "raw_news_data"
                },
                {
                    "id": "process_news",
                    "tool": "batch_process_news",
                    "inputs": {"news_list": "$raw_news_data"},
                    "output": "extracted_events"
                }
            ]
        }
        
        if auto_update_kg:
            pipeline_def["steps"].append({
                "id": "update_kg",
                "tool": "update_graph_data",
                "inputs": {"events_list": "$extracted_events"},
                "output": "update_status"
            })
            
        if enable_report:
            pipeline_def["steps"].append({
                "id": "generate_report",
                "tool": "generate_markdown_report",
                "inputs": {"events_list": "$extracted_events", "title": f"Ingestion Report {datetime.now().strftime('%Y-%m-%d')}"},
                "output": "final_report_md"
            })
            
        execute_pipeline(pipeline_def)

def render_expansion_tab():
    st.header("ğŸ” Knowledge Expansion")
    st.caption("Search for news based on keywords to discover new entities.")
    
    st.subheader("Define Search Tasks")
    st.info("Manage search keywords. You can add entities from the Knowledge Graph or manually type new keywords in the table.")
    # é€‰æ‹©å¯ç”¨çš„æœç´¢ APIï¼ˆä¸ Configuration å…±ç”¨åŒä¸€é…ç½®ï¼‰
    selected_apis = st.session_state.ingestion_apis[st.session_state.ingestion_apis["enabled"] == True]["name"].tolist()
    
    # å·¥å…·æ ï¼šä»ä¸‹æ‹‰åˆ—è¡¨æ·»åŠ 
    entities = utils.load_entities()
    if entities:
        all_entity_names = sorted(list(entities.keys()))
        
        c_add_sel, c_add_btn = st.columns([3, 1])
        with c_add_sel:
            selected_entities = st.multiselect(
                "Select Entities from Graph", 
                options=all_entity_names,
                placeholder="Choose entities to add..."
            )
        with c_add_btn:
            st.write("") # Spacer
            st.write("") 
            if st.button("â• Add Selected", use_container_width=True):
                if selected_entities:
                    new_rows = []
                    # è·å–ç°æœ‰å…³é”®è¯ä»¥é¿å…é‡å¤
                    existing_kws = set()
                    if not st.session_state.expansion_tasks.empty:
                        existing_kws = set(st.session_state.expansion_tasks["keyword"].tolist())
                        
                    count = 0
                    for ent in selected_entities:
                        if ent not in existing_kws:
                            new_rows.append({
                                "enabled": True,
                                "keyword": ent,
                                "limit": 5,
                                "category": "general",
                                "from": "",
                                "to": "",
                                "sortby": ""
                            })
                            count += 1
                    
                    if new_rows:
                        new_df = pd.DataFrame(new_rows)
                        st.session_state.expansion_tasks = pd.concat(
                            [st.session_state.expansion_tasks, new_df], 
                            ignore_index=True
                        )
                        st.success(f"Added {count} new tasks!")
                        st.rerun()
                    else:
                        st.warning("Selected entities are already in the list.")
                else:
                    st.warning("Please select entities first.")

    # ä»»åŠ¡è¡¨æ ¼ç¼–è¾‘å™¨
    edited_tasks = st.data_editor(
        st.session_state.expansion_tasks,
        num_rows="dynamic",
        use_container_width=True,
        column_config={
            "enabled": st.column_config.CheckboxColumn("Enabled"),
            "keyword": st.column_config.TextColumn("Keyword", required=True, help="Type manually or added from dropdown"),
            "limit": st.column_config.NumberColumn("Limit", min_value=1, max_value=10, default=5),
            "category": st.column_config.TextColumn("Category", help="GNews categoryï¼Œå¦‚ general/business/...ï¼Œå¯ç©º"),
            "from": st.column_config.TextColumn("From (ISO8601)", help="å¯ç©ºï¼Œå¦‚ 2025-12-01T00:00:00Z"),
            "to": st.column_config.TextColumn("To (ISO8601)", help="å¯ç©ºï¼Œå¦‚ 2025-12-31T23:59:59Z"),
            "sortby": st.column_config.SelectboxColumn("Sortby", options=["", "publishedAt", "relevance"]),
        },
        key="expansion_tasks_editor"
    )
    st.session_state.expansion_tasks = edited_tasks

    st.subheader("ğŸš€ Run Expansion")
    
    # è¿‡æ»¤å‡ºå¯ç”¨çš„ä»»åŠ¡
    active_tasks = st.session_state.expansion_tasks[st.session_state.expansion_tasks["enabled"] == True]
    
    c1, c2 = st.columns(2)
    c1.metric("Selected APIs", len(selected_apis))
    c2.metric("Active Tasks", len(active_tasks))
    
    if st.button("Start Expansion Task", type="primary", use_container_width=True):
        if not selected_apis:
            st.error("Please select at least one Search API.")
            return
        if active_tasks.empty:
            st.error("Please define and enable at least one Search Task.")
            return
            
        # æ„å»º Pipelineï¼šä¸ºæ¯ä¸ªå¯ç”¨ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªæ­¥éª¤
        pipeline_steps = []
        for idx, row in active_tasks.iterrows():
            kw = row["keyword"]
            step_id = f"search_{kw.replace(' ', '_')}_{idx}"
            
            pipeline_steps.append({
                    "id": step_id,
                    "tool": "search_news_by_keywords", 
                    "inputs": {
                        "keywords": [kw], # å·¥å…·æœŸæœ›åˆ—è¡¨
                        "apis": selected_apis,
                        "limit": int(row.get("limit", 50)),
                        "category": row.get("category") or None,
                        "from": row.get("from") or None,
                        "to": row.get("to") or None,
                        "sortby": row.get("sortby") or None
                    },
                    "output": f"results_{idx}"
            })
        
        pipeline_def = {
            "name": "Knowledge Expansion Batch",
            "steps": pipeline_steps
        }
        execute_pipeline(pipeline_def)

def render_maintenance_tab():
    st.header("ğŸ•¸ï¸ Graph Maintenance")
    
    # å½“å‰ä¸´æ—¶æ•°æ®å¿«ç…§ï¼ˆå®ä½“/äº‹ä»¶ï¼‰
    data_dir = ROOT_DIR / "data"
    entities_tmp_file = data_dir / "tmp" / "entities_tmp.json"
    events_tmp_file = data_dir / "tmp" / "abstract_to_event_map_tmp.json"

    def load_json(path):
        try:
            if path.exists():
                with open(path, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            st.warning(f"{path.name} è¯»å–å¤±è´¥: {e}")
        return {}

    entities_tmp = load_json(entities_tmp_file)
    events_tmp = load_json(events_tmp_file)

    c1, c2 = st.columns(2)
    c1.metric("ä¸´æ—¶å®ä½“", len(entities_tmp))
    c2.metric("ä¸´æ—¶äº‹ä»¶", len(events_tmp))

    with st.expander("æŸ¥çœ‹ä¸´æ—¶å®ä½“ / äº‹ä»¶ç¤ºä¾‹", expanded=False):
        if entities_tmp:
            df_ent = pd.DataFrame(
                [{"name": k, "first_seen": v.get("first_seen", ""), "sources": ",".join(v.get("sources", []))[:80]} for k, v in list(entities_tmp.items())[:50]]
            )
            st.write("ä¸´æ—¶å®ä½“ï¼ˆæœ€å¤š50æ¡é¢„è§ˆï¼‰")
            st.dataframe(df_ent, use_container_width=True)
        else:
            st.info("æš‚æ— ä¸´æ—¶å®ä½“æ•°æ®")

        if events_tmp:
            df_evt = pd.DataFrame(
                [{"abstract": k, "first_seen": v.get("first_seen", ""), "entities": ",".join(v.get("entities", []))[:80]} for k, v in list(events_tmp.items())[:50]]
            )
            st.write("ä¸´æ—¶äº‹ä»¶ï¼ˆæœ€å¤š50æ¡é¢„è§ˆï¼‰")
            st.dataframe(df_evt, use_container_width=True)
        else:
            st.info("æš‚æ— ä¸´æ—¶äº‹ä»¶æ•°æ®")
    
    with st.form("maintenance_form"):
        c1, c2 = st.columns(2)
        with c1:
            st.subheader("Deduplication")
            strict = st.checkbox("Strict Mode", True)
            thresh = st.slider("Similarity", 0.5, 1.0, 0.9)
        with c2:
            st.subheader("Cleaning")
            rm_iso = st.checkbox("Remove Isolated Nodes")
            
        submitted = st.form_submit_button("ğŸš€ Run Maintenance", type="primary", use_container_width=True)
        
    if submitted:
        pipeline_def = {
            "name": "Graph Maintenance",
            # è°ƒç”¨ Agent3 åˆ·æ–°å‹ç¼©çŸ¥è¯†å›¾è°±ï¼ˆåŸºäºç°æœ‰å®ä½“/äº‹ä»¶æ–‡ä»¶ï¼‰
            "steps": [
                {
                    "id": "refresh_kg",
                    "tool": "refresh_knowledge_graph",
                    "inputs": {},
                    "output": "status"
                }
            ]
        }
        execute_pipeline(pipeline_def)

def render_custom_builder():
    st.header("ğŸ› ï¸ Custom Pipeline Builder")
    
    col_builder, col_preview = st.columns([1.5, 1])
    
    with col_builder:
        # å·¥å…·æ 
        c_add, c_save, c_load = st.columns([2, 1, 1])
        with c_add:
            tools = FunctionRegistry.get_all_tools()
            selected_tool = st.selectbox("Select Tool", list(tools.keys()), label_visibility="collapsed")
            if st.button("Add Step", use_container_width=True):
                 st.session_state.pipeline_steps.append({
                    "id": f"step_{len(st.session_state.pipeline_steps) + 1}",
                    "tool": selected_tool,
                    "inputs": {}
                })
                 st.rerun()

        # æ­¥éª¤ç¼–è¾‘
        if not st.session_state.pipeline_steps:
            st.info("No steps added. Select a tool to start.")
        else:
            for i, step in enumerate(st.session_state.pipeline_steps):
                tool_name = step["tool"]
                tool_meta = tools.get(tool_name, {})
                
                with st.expander(f"Step {i+1}: {tool_name}", expanded=False):
                    c_id, c_del = st.columns([4, 1])
                    step["id"] = c_id.text_input("ID", step["id"], key=f"id_{i}")
                    if c_del.button("ğŸ—‘ï¸", key=f"del_{i}"):
                        st.session_state.pipeline_steps.pop(i)
                        st.rerun()
                        
                    st.caption(tool_meta.get("description", ""))
                    
                    # å‚æ•°ç¼–è¾‘åŒº
                    params = tool_meta.get("parameters", {})
                    if params:
                        for p_name, p_info in params.items():
                            render_input_field(i, p_name, p_info, step.get("inputs", {}), step)
                    
                    step["output"] = st.text_input("Output to ($var)", step.get("output", ""), key=f"out_{i}")

    with col_preview:
        st.subheader("Preview")
        pipeline_def = {"name": "Custom Pipeline", "steps": st.session_state.pipeline_steps}
        st.code(yaml.dump(pipeline_def, sort_keys=False), language="yaml")
        
        if st.button("ğŸš€ Run Pipeline", type="primary", use_container_width=True):
            execute_pipeline(pipeline_def)

# --- ä¸»å¯¼èˆª ---
tabs = st.tabs(["Configuration","Ingestion", "Expansion", "Maintenance", "Custom Builder"])

with tabs[0]: render_configuration_tab()
with tabs[1]: render_ingestion_tab()
with tabs[2]: render_expansion_tab()
with tabs[3]: render_maintenance_tab()
with tabs[4]: render_custom_builder()
