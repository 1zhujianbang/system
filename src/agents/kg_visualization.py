# src/agents/kg_visualization.py
"""
çŸ¥è¯†å›¾è°±å¯è§£é‡Šæ€§å±•ç¤ºç»„ä»¶

è¯¥ç»„ä»¶æä¾›ï¼š
1. å®ä½“å…³ç³»å¯è§†åŒ–åŠŸèƒ½
2. äº¤äº’å¼åˆ†æç•Œé¢
3. å›¾è°±è§£é‡ŠæŠ¥å‘Šç”Ÿæˆ
4. å¼‚å¸¸å…³ç³»æ£€æµ‹ä¸è§£é‡Š
"""

import json
import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
from typing import Dict, List, Set, Optional, Tuple, Union
from datetime import datetime, timedelta
import os
import io
import base64
import warnings
warnings.filterwarnings("ignore")

from ..utils.tool_function import tools
from .kg_interface import get_knowledge_graph

# å…¨å±€çŸ¥è¯†å›¾è°±å®ä¾‹
KG = None

# ç¡®ä¿KGå®ä¾‹è¢«æ­£ç¡®åˆå§‹åŒ–
def _ensure_kg():
    global KG
    tools.log(f"ğŸ” [DEBUG] ç¡®ä¿çŸ¥è¯†å›¾è°±å®ä¾‹ï¼Œå½“å‰çŠ¶æ€: {'å·²å­˜åœ¨' if KG else 'ä¸å­˜åœ¨'}")
    if KG is None:
        tools.log(f"ğŸ” [DEBUG] åˆ›å»ºæ–°çš„çŸ¥è¯†å›¾è°±å®ä¾‹")
        KG = get_knowledge_graph()
        tools.log(f"âœ… [DEBUG] çŸ¥è¯†å›¾è°±å®ä¾‹åˆ›å»ºå®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(KG.nodes) if KG else 0}")
    return KG

# åˆå§‹åŒ–KGå®ä¾‹
KG = _ensure_kg()

class KGVisualizer:
    """çŸ¥è¯†å›¾è°±å¯è§†åŒ–å™¨"""
    
    def __init__(self):
        tools.log(f"ğŸ” [DEBUG] KGVisualizeråˆå§‹åŒ–å¼€å§‹")
        self.reset()
        tools.log(f"âœ… [DEBUG] KGVisualizeråˆå§‹åŒ–å®Œæˆ")
        
    def reset(self):
        """é‡ç½®å¯è§†åŒ–å™¨çŠ¶æ€"""
        tools.log(f"ğŸ” [DEBUG] é‡ç½®KGVisualizerçŠ¶æ€")
        self.graph = nx.Graph()
        self.node_colors = {}
        self.node_sizes = {}
        self.edge_weights = {}
        self.edge_colors = {}
        self.entity_to_events = {}
    
    def build_visualization_graph(self, entities: List[str] = None, depth: int = 2, 
                                 include_events: bool = True) -> nx.Graph:
        """
        æ„å»ºå¯è§†åŒ–å›¾è°±
        
        Args:
            entities: èµ·å§‹å®ä½“åˆ—è¡¨ï¼Œä¸ºç©ºæ—¶å±•ç¤ºå…¨éƒ¨å›¾è°±
            depth: å…³ç³»æ·±åº¦
            include_events: æ˜¯å¦åŒ…å«äº‹ä»¶èŠ‚ç‚¹
        
        Returns:
            æ„å»ºå¥½çš„NetworkXå›¾
        """
        import time
        start_time = time.time()
        tools.log(f"ğŸ” [DEBUG] å¼€å§‹æ„å»ºå¯è§†åŒ–å›¾è°±ï¼Œå®ä½“åˆ—è¡¨: {entities}, æ·±åº¦: {depth}, åŒ…å«äº‹ä»¶: {include_events}")
        
        try:
            _ensure_kg()
            
            self.reset()
            
            # è·å–åŸºç¡€å›¾è°±æ•°æ®
            tools.log(f"ğŸ” [DEBUG] è·å–åŸºç¡€å›¾è°±æ•°æ®...")
            all_entities = KG.get_all_entities()
            all_events = KG.get_all_events()
            entity_entity_relations = KG.get_entity_relations()
            entity_event_relations = KG.get_entity_event_relations()
            
            tools.log(f"ğŸ“Š [DEBUG] å›¾è°±åŸºç¡€æ•°æ®ç»Ÿè®¡ - å®ä½“æ•°: {len(all_entities)}, äº‹ä»¶æ•°: {len(all_events)}")
        
            # å¦‚æœæŒ‡å®šäº†å®ä½“ï¼Œä½¿ç”¨å¹¿åº¦ä¼˜å…ˆæœç´¢æ„å»ºå­å›¾
            if entities:
                tools.log(f"ğŸ” [DEBUG] ä½¿ç”¨æŒ‡å®šå®ä½“æ„å»ºå­å›¾ï¼Œèµ·å§‹å®ä½“æ•°: {len(entities)}")
                visited_entities = set()
                queue = [(entity, 0) for entity in entities]
            
                while queue:
                    current_entity, current_depth = queue.pop(0)
                    if current_entity in visited_entities or current_depth > depth:
                        continue
                    
                    visited_entities.add(current_entity)
                    tools.log(f"ğŸ” [DEBUG] å¤„ç†å®ä½“: '{current_entity}'ï¼Œå½“å‰æ·±åº¦: {current_depth}")
                
                    # æ·»åŠ å®ä½“èŠ‚ç‚¹
                    self.graph.add_node(current_entity, type="entity")
                    self.node_colors[current_entity] = "#3498db"  # è“è‰²è¡¨ç¤ºå®ä½“
                    
                    # è®¡ç®—å®ä½“å‚ä¸çš„äº‹ä»¶æ•°é‡
                    entity_info = KG.get_entity_info(current_entity)
                    entity_id = entity_info["id"] if entity_info else None
                    event_count = 0
                    if entity_id:
                        for rel in KG.relationships:
                            if rel["source"] == entity_id and rel["type"] == "participates_in":
                                event_count += 1
                    self.node_sizes[current_entity] = 1000 + min(event_count * 100, 3000)
                
                    # æ·»åŠ ç›´æ¥ç›¸å…³çš„äº‹ä»¶
                    if include_events:
                        tools.log(f"ğŸ” [DEBUG] è·å–å®ä½“ '{current_entity}' å‚ä¸çš„äº‹ä»¶")
                        related_events = KG.get_entity_events(current_entity)
                        tools.log(f"ğŸ“Š [DEBUG] å®ä½“ '{current_entity}' å‚ä¸äº† {len(related_events)} ä¸ªäº‹ä»¶")
                        self.entity_to_events[current_entity] = related_events
                        
                        for event in related_events:
                            if event in all_events:
                                self.graph.add_node(event, type="event")
                                self.node_colors[event] = "#e74c3c"  # çº¢è‰²è¡¨ç¤ºäº‹ä»¶
                                self.node_sizes[event] = 800
                                
                                # æ·»åŠ å®ä½“-äº‹ä»¶è¾¹
                                edge_key = (current_entity, event)
                                self.graph.add_edge(*edge_key)
                                self.edge_colors[edge_key] = "#95a5a6"
                                self.edge_weights[edge_key] = 1
                    
                    # æ·»åŠ ç›¸å…³å®ä½“
                    if current_depth < depth:
                        tools.log(f"ğŸ” [DEBUG] è·å–å®ä½“ '{current_entity}' çš„ç›¸å…³å®ä½“")
                        related_entities = KG.get_related_entities(current_entity)
                        tools.log(f"ğŸ“Š [DEBUG] å®ä½“ '{current_entity}' ç›¸å…³å®ä½“æ•°: {len(related_entities)}")
                        for related_entity in related_entities:
                            if related_entity in all_entities:
                                # æ·»åŠ å®ä½“-å®ä½“è¾¹
                                edge_key = tuple(sorted([current_entity, related_entity]))
                                self.graph.add_edge(*edge_key)
                                self.edge_colors[edge_key] = "#2ecc71"
                                self.edge_weights[edge_key] = 1.5
                                
                                if related_entity not in visited_entities:
                                    queue.append((related_entity, current_depth + 1))
            else:
                # å±•ç¤ºå…¨éƒ¨å›¾è°±ï¼ˆé™åˆ¶æ•°é‡ä»¥é˜²æ€§èƒ½é—®é¢˜ï¼‰
                tools.log(f"ğŸ” [DEBUG] æ„å»ºå…¨å±€å›¾è°±ï¼Œé™åˆ¶èŠ‚ç‚¹æ•°: 100")
                max_nodes = 100
                # ç¡®ä¿æ­£ç¡®å¤„ç†å®ä½“åˆ—è¡¨
                sampled_entities = list(all_entities)[:max_nodes]
                tools.log(f"ğŸ“Š [DEBUG] é‡‡æ ·å®ä½“æ•°: {len(sampled_entities)}")
            
                for entity in sampled_entities:
                    self.graph.add_node(entity, type="entity")
                    self.node_colors[entity] = "#3498db"
                    self.node_sizes[entity] = 1000
        
            processing_time = (time.time() - start_time) * 1000
            tools.log(f"âœ… [DEBUG] å¯è§†åŒ–å›¾è°±æ„å»ºå®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(self.graph.nodes())}, è¾¹æ•°: {len(self.graph.edges())}, è€—æ—¶: {processing_time:.2f}ms")
            return self.graph
        except Exception as e:
            tools.log(f"âŒ [DEBUG] æ„å»ºå¯è§†åŒ–å›¾è°±å‡ºé”™: {str(e)}")
            import traceback
            tools.log(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return self.graph
    
    def generate_plot_image(self, output_format: str = "base64") -> Union[str, plt.Figure]:
        """
        ç”Ÿæˆå›¾è°±å¯è§†åŒ–å›¾åƒ
        
        Args:
            output_format: è¾“å‡ºæ ¼å¼ï¼Œ"base64"æˆ–"figure"
        
        Returns:
            å›¾åƒçš„base64ç¼–ç å­—ç¬¦ä¸²æˆ–matplotlib Figureå¯¹è±¡
        """
        import time
        start_time = time.time()
        tools.log(f"ğŸ” [DEBUG] ç”Ÿæˆå¯è§†åŒ–å›¾åƒï¼Œè¾“å‡ºæ ¼å¼: {output_format}")
        
        try:
            if len(self.graph.nodes()) == 0:
                tools.log(f"âŒ [DEBUG] å›¾è°±ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆå¯è§†åŒ–å›¾åƒ")
                raise ValueError("å›¾è°±ä¸ºç©ºï¼Œè¯·å…ˆæ„å»ºå¯è§†åŒ–å›¾è°±")
            
            tools.log(f"ğŸ“Š [DEBUG] å½“å‰å›¾è°±ç»Ÿè®¡ - èŠ‚ç‚¹æ•°: {len(self.graph.nodes())}, è¾¹æ•°: {len(self.graph.edges())}")
        
            plt.figure(figsize=(16, 12))
        
            # ä½¿ç”¨springå¸ƒå±€
            pos = nx.spring_layout(self.graph, seed=42, k=0.15)
            
            # ç»˜åˆ¶èŠ‚ç‚¹
            node_list = list(self.graph.nodes())
            node_color_values = [self.node_colors.get(node, "#95a5a6") for node in node_list]
            node_size_values = [self.node_sizes.get(node, 500) for node in node_list]
            
            nx.draw_networkx_nodes(
                self.graph, pos, node_color=node_color_values, 
                node_size=node_size_values, alpha=0.8
            )
            
            # ç»˜åˆ¶è¾¹
            edge_list = list(self.graph.edges())
            edge_color_values = [self.edge_colors.get(tuple(sorted(edge)), "#95a5a6") for edge in edge_list]
            edge_width_values = [self.edge_weights.get(tuple(sorted(edge)), 1.0) for edge in edge_list]
            
            nx.draw_networkx_edges(
                self.graph, pos, edge_color=edge_color_values, 
                width=edge_width_values, alpha=0.6
            )
            
            # æ·»åŠ æ ‡ç­¾ï¼ˆåªå¯¹é‡è¦èŠ‚ç‚¹ï¼‰
            label_nodes = [node for node in node_list if self.node_sizes.get(node, 0) > 800]
            labels = {node: node[:20] + "..." if len(node) > 20 else node for node in label_nodes}
            
            nx.draw_networkx_labels(
                self.graph, pos, labels=labels, font_size=10, font_color="#333333"
            )
            
            plt.title("çŸ¥è¯†å›¾è°±å¯è§†åŒ– - å®ä½“ä¸äº‹ä»¶å…³ç³»", fontsize=16)
            plt.axis("off")
            plt.tight_layout()
        
            if output_format == "base64":
                buffer = io.BytesIO()
                plt.savefig(buffer, format="png", dpi=100, bbox_inches="tight")
                buffer.seek(0)
                image_base64 = base64.b64encode(buffer.read()).decode('utf-8')
                plt.close()
                processing_time = (time.time() - start_time) * 1000
                tools.log(f"âœ… [DEBUG] å›¾åƒç”Ÿæˆå®Œæˆï¼Œæ ¼å¼: base64ï¼Œå¤§å°: {len(image_base64)} bytesï¼Œè€—æ—¶: {processing_time:.2f}ms")
                return image_base64
            else:
                processing_time = (time.time() - start_time) * 1000
                tools.log(f"âœ… [DEBUG] å›¾åƒç”Ÿæˆå®Œæˆï¼Œæ ¼å¼: figureï¼Œè€—æ—¶: {processing_time:.2f}ms")
                return plt.gcf()
        except Exception as e:
            tools.log(f"âŒ [DEBUG] ç”Ÿæˆå¯è§†åŒ–å›¾åƒå‡ºé”™: {str(e)}")
            import traceback
            tools.log(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise

class KGExplainer:
    """çŸ¥è¯†å›¾è°±è§£é‡Šå™¨"""
    
    def __init__(self):
        tools.log(f"ğŸ” [DEBUG] KGExplaineråˆå§‹åŒ–å¼€å§‹")
        self.visualizer = KGVisualizer()
        tools.log(f"âœ… [DEBUG] KGExplaineråˆå§‹åŒ–å®Œæˆ")
        
    def generate_explanation_report(self, focus_entity: Optional[str] = None) -> Dict:
        """
        ç”ŸæˆçŸ¥è¯†å›¾è°±è§£é‡ŠæŠ¥å‘Š
        
        Args:
            focus_entity: å…³æ³¨çš„å®ä½“ï¼Œå¦‚æœä¸ºNoneåˆ™ç”Ÿæˆå…¨å±€æŠ¥å‘Š
        
        Returns:
            è§£é‡ŠæŠ¥å‘Šå­—å…¸
        """
        import time
        start_time = time.time()
        tools.log(f"ğŸ” [DEBUG] ç”Ÿæˆè§£é‡ŠæŠ¥å‘Šï¼Œå…³æ³¨å®ä½“: {focus_entity if focus_entity else 'å…¨å±€'}")
        
        try:
            global KG
            if KG is None:
                tools.log(f"ğŸ” [DEBUG] è·å–çŸ¥è¯†å›¾è°±å®ä¾‹")
                KG = get_knowledge_graph()
        
            report = {
                "generated_at": datetime.now().isoformat(),
                "graph_statistics": self._get_graph_statistics(),
                "key_entities": [],
                "key_events": [],
                "relationship_insights": [],
                "temporal_patterns": [],
                "recommendations": []
            }
        
            if focus_entity:
                report["focus_entity"] = focus_entity
                tools.log(f"ğŸ” [DEBUG] ç”Ÿæˆå®ä½“ '{focus_entity}' çš„è¯¦ç»†æŠ¥å‘Š")
                entity_report = self._generate_entity_report(focus_entity)
                report.update(entity_report)
            else:
                # ç”Ÿæˆå…¨å±€æŠ¥å‘Š
                tools.log(f"ğŸ” [DEBUG] ç”Ÿæˆå…¨å±€æŠ¥å‘Š")
                report["key_entities"] = self._get_top_entities(n=10)
                tools.log(f"ğŸ“Š [DEBUG] æå–äº† {len(report['key_entities'])} ä¸ªå…³é”®å®ä½“")
                report["key_events"] = self._get_top_events(n=10)
                tools.log(f"ğŸ“Š [DEBUG] æå–äº† {len(report['key_events'])} ä¸ªå…³é”®äº‹ä»¶")
                report["relationship_insights"] = self._generate_relationship_insights()
                tools.log(f"ğŸ“Š [DEBUG] ç”Ÿæˆäº† {len(report['relationship_insights'])} ä¸ªå…³ç³»æ´å¯Ÿ")
                report["temporal_patterns"] = self._analyze_temporal_patterns()
                tools.log(f"ğŸ“Š [DEBUG] åˆ†æäº† {len(report['temporal_patterns'])} ä¸ªæ—¶é—´æ¨¡å¼")
        
            report["recommendations"] = self._generate_recommendations(report)
        
            processing_time = (time.time() - start_time) * 1000
            tools.log(f"âœ… [DEBUG] è§£é‡ŠæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ms")
            return report
        except Exception as e:
            tools.log(f"âŒ [DEBUG] ç”Ÿæˆè§£é‡ŠæŠ¥å‘Šå‡ºé”™: {str(e)}")
            import traceback
            tools.log(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return {"error": str(e), "generated_at": datetime.now().isoformat()}
    
    def _get_graph_statistics(self) -> Dict:
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        import time
        start_time = time.time()
        tools.log(f"ğŸ” [DEBUG] è®¡ç®—å›¾è°±ç»Ÿè®¡ä¿¡æ¯")
        
        try:
            global KG
            all_entities = KG.get_all_entities()
            tools.log(f"ğŸ“Š [DEBUG] å½“å‰å®ä½“æ€»æ•°: {len(all_entities)}")
            total_event_relations = 0
            total_entity_relations = 0
            
            # è®¡ç®—æ‰€æœ‰å®ä½“-äº‹ä»¶å…³ç³»æ•°é‡
            tools.log(f"ğŸ” [DEBUG] è®¡ç®—å®ä½“-äº‹ä»¶å…³ç³»æ•°é‡")
            for entity in all_entities:
                total_event_relations += len(KG.get_entity_events(entity))
            
            # è®¡ç®—æ‰€æœ‰å®ä½“-å®ä½“å…³ç³»æ•°é‡
            tools.log(f"ğŸ” [DEBUG] è®¡ç®—å®ä½“-å®ä½“å…³ç³»æ•°é‡")
            for entity in all_entities:
                total_entity_relations += len(KG.get_related_entities(entity))
            
            stats = {
                "total_entities": len(all_entities),
                "total_events": len(KG.get_all_events()),
                "total_entity_event_relations": total_event_relations,
                "total_entity_entity_relations": total_entity_relations,
                "avg_events_per_entity": self._calculate_avg_events_per_entity()
            }
            
            processing_time = (time.time() - start_time) * 1000
            tools.log(f"ğŸ“Š [DEBUG] å›¾è°±ç»Ÿè®¡å®Œæˆ - å®ä½“: {stats['total_entities']}, äº‹ä»¶: {stats['total_events']}, å®ä½“-äº‹ä»¶å…³ç³»: {stats['total_entity_event_relations']}, å®ä½“-å®ä½“å…³ç³»: {stats['total_entity_entity_relations']}, è€—æ—¶: {processing_time:.2f}ms")
            return stats
        except Exception as e:
            tools.log(f"âŒ [DEBUG] è®¡ç®—å›¾è°±ç»Ÿè®¡å‡ºé”™: {str(e)}")
            import traceback
            tools.log(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return {"error": str(e)}
    
    def _calculate_avg_events_per_entity(self) -> float:
        """è®¡ç®—æ¯ä¸ªå®ä½“å¹³å‡å…³è”çš„äº‹ä»¶æ•°"""
        global KG
        all_entities = KG.get_all_entities()
        if not all_entities:
            return 0.0
        
        total_events = 0
        for entity in all_entities:
            total_events += len(KG.get_entity_events(entity))
            
        return round(total_events / len(all_entities), 2)
    
    def _get_top_entities(self, n: int = 10) -> List[Dict]:
        """è·å–äº‹ä»¶å…³è”æœ€å¤šçš„å‰Nä¸ªå®ä½“"""
        global KG
        entity_event_counts = {}
        
        # ç»Ÿè®¡æ¯ä¸ªå®ä½“çš„äº‹ä»¶æ•°é‡
        all_entities = KG.get_all_entities()
        for entity in all_entities:
            entity_event_counts[entity] = len(KG.get_entity_events(entity))
        
        # æ’åºå¹¶è¿”å›å‰Nä¸ª
        sorted_entities = sorted(entity_event_counts.items(), key=lambda x: x[1], reverse=True)[:n]
        
        return [{
            "entity": entity,
            "event_count": count,
            "related_entities": list(KG.get_related_entities(entity))[:5]
        } for entity, count in sorted_entities]
    
    def _get_top_events(self, n: int = 10) -> List[Dict]:
        """è·å–å®ä½“å…³è”æœ€å¤šçš„å‰Nä¸ªäº‹ä»¶"""
        global KG
        event_entity_counts = {}
        
        # ç»Ÿè®¡æ¯ä¸ªäº‹ä»¶å…³è”çš„å®ä½“æ•°é‡
        all_entities = KG.get_all_entities()
        for entity in all_entities:
            events = KG.get_entity_events(entity)
            for event in events:
                if event not in event_entity_counts:
                    event_entity_counts[event] = 0
                event_entity_counts[event] += 1
        
        # æ’åºå¹¶è¿”å›å‰Nä¸ª
        sorted_events = sorted(event_entity_counts.items(), key=lambda x: x[1], reverse=True)[:n]
        
        return [{
            "event": event,
            "entity_count": count
        } for event, count in sorted_events]
    
    def _generate_entity_report(self, entity: str) -> Dict:
        """ç”Ÿæˆç‰¹å®šå®ä½“çš„è¯¦ç»†æŠ¥å‘Š"""
        import time
        start_time = time.time()
        tools.log(f"ğŸ” [DEBUG] ç”Ÿæˆå®ä½“ '{entity}' çš„è¯¦ç»†æŠ¥å‘Š")
        
        try:
            global KG
            
            if entity not in KG.get_all_entities():
                tools.log(f"âŒ [DEBUG] å®ä½“ '{entity}' ä¸å­˜åœ¨äºçŸ¥è¯†å›¾è°±ä¸­")
                return {"error": f"å®ä½“ '{entity}' ä¸å­˜åœ¨äºçŸ¥è¯†å›¾è°±ä¸­"}
        
            entity_data = KG.get_all_entities()[entity]
            tools.log(f"ğŸ” [DEBUG] è·å–å®ä½“ '{entity}' çš„ç›¸å…³äº‹ä»¶")
            related_events = KG.get_entity_events(entity)
            tools.log(f"ğŸ“Š [DEBUG] å®ä½“ '{entity}' å‚ä¸äº† {len(related_events)} ä¸ªäº‹ä»¶")
            
            tools.log(f"ğŸ” [DEBUG] è·å–å®ä½“ '{entity}' çš„ç›¸å…³å®ä½“")
            related_entities = KG.get_related_entities(entity)
            tools.log(f"ğŸ“Š [DEBUG] å®ä½“ '{entity}' å…³è”äº† {len(related_entities)} ä¸ªå…¶ä»–å®ä½“")
            
            # åˆ†æå®ä½“çš„äº‹ä»¶æ—¶é—´åˆ†å¸ƒ
            tools.log(f"ğŸ” [DEBUG] åˆ†æå®ä½“ '{entity}' çš„äº‹ä»¶æ—¶é—´åˆ†å¸ƒ")
            event_timeline = []
            for event in related_events:
                event_data = KG.get_event_data(event)
                if event_data and "first_seen" in event_data:
                    event_timeline.append({
                        "event": event,
                        "timestamp": event_data["first_seen"]
                    })
            
            # æŒ‰æ—¶é—´æ’åº
            event_timeline.sort(key=lambda x: x["timestamp"])
            
            importance = self._calculate_entity_importance(entity)
            tools.log(f"ğŸ“Š [DEBUG] å®ä½“ '{entity}' é‡è¦æ€§è¯„åˆ†: {importance}")
            
            result = {
                "entity_details": entity_data,
                "related_events": related_events[:20],  # é™åˆ¶æ•°é‡
                "related_entities": list(related_entities)[:10],  # é™åˆ¶æ•°é‡
                "event_timeline": event_timeline,
                "entity_importance": importance
            }
            
            processing_time = (time.time() - start_time) * 1000
            tools.log(f"âœ… [DEBUG] å®ä½“ '{entity}' æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ms")
            return result
        except Exception as e:
            tools.log(f"âŒ [DEBUG] ç”Ÿæˆå®ä½“æŠ¥å‘Šå‡ºé”™: {str(e)}")
            import traceback
            tools.log(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return {"error": str(e)}
    
    def _calculate_entity_importance(self, entity: str) -> float:
        """è®¡ç®—å®ä½“çš„é‡è¦æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰"""
        global KG
        
        # åŸºäºäº‹ä»¶æ•°é‡ã€å…³ç³»æ•°é‡å’Œæ—¶é—´å› ç´ è®¡ç®—é‡è¦æ€§
        event_count = len(KG.get_entity_events(entity))
        relation_count = len(KG.get_related_entities(entity))
        
        # å½’ä¸€åŒ–è®¡ç®—
        max_event_count = 100  # å‡è®¾æœ€å¤§äº‹ä»¶æ•°ä¸º100
        max_relation_count = 50  # å‡è®¾æœ€å¤§å…³ç³»æ•°ä¸º50
        
        event_score = min(event_count / max_event_count, 1.0)
        relation_score = min(relation_count / max_relation_count, 1.0)
        
        # ç»¼åˆåˆ†æ•°
        importance = (event_score * 0.6) + (relation_score * 0.4)
        return round(importance, 3)
    
    def _generate_relationship_insights(self) -> List[Dict]:
        """ç”Ÿæˆå…³ç³»æ´å¯Ÿ"""
        insights = []
        
        # æ£€æµ‹é«˜åº¦è¿æ¥çš„å®ä½“ç»„ï¼ˆç¤¾åŒºï¼‰
        self.visualizer.build_visualization_graph(depth=1)
        if len(self.visualizer.graph.nodes()) > 5:
            communities = nx.community.greedy_modularity_communities(self.visualizer.graph)
            if len(communities) > 1:
                insights.append({
                    "type": "community_detection",
                    "message": f"æ£€æµ‹åˆ° {len(communities)} ä¸ªå®ä½“ç¤¾åŒº",
                    "details": f"æœ€å¤§ç¤¾åŒºåŒ…å« {max(len(c) for c in communities)} ä¸ªå®ä½“"
                })
        
        # æ£€æµ‹å…³é”®æ¡¥æ¥å®ä½“
        bridges = list(nx.bridges(self.visualizer.graph))
        if bridges:
            bridge_entities = set()
            for u, v in bridges:
                bridge_entities.add(u)
                bridge_entities.add(v)
            
            insights.append({
                "type": "bridge_entities",
                "message": f"å‘ç° {len(bridge_entities)} ä¸ªæ¡¥æ¥å®ä½“ï¼Œå®ƒä»¬è¿æ¥ä¸åŒçš„å®ä½“ç¤¾åŒº",
                "details": list(bridge_entities)[:5]  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            })
        
        return insights
    
    def _analyze_temporal_patterns(self) -> List[Dict]:
        """åˆ†ææ—¶é—´æ¨¡å¼"""
        global KG
        patterns = []
        
        # æ”¶é›†æ‰€æœ‰äº‹ä»¶çš„æ—¶é—´æˆ³
        all_timestamps = []
        for event, event_data in KG.get_all_events().items():
            if "first_seen" in event_data:
                try:
                    dt = datetime.fromisoformat(event_data["first_seen"])
                    all_timestamps.append(dt)
                except:
                    pass
        
        if all_timestamps:
            # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡äº‹ä»¶æ•°é‡
            from collections import defaultdict
            date_counts = defaultdict(int)
            for dt in all_timestamps:
                date_key = dt.date()
                date_counts[date_key] += 1
            
            # æ‰¾å‡ºäº‹ä»¶æœ€æ´»è·ƒçš„æ—¥æœŸ
            if date_counts:
                top_date = max(date_counts.items(), key=lambda x: x[1])
                patterns.append({
                    "type": "active_date",
                    "message": f"äº‹ä»¶æœ€æ´»è·ƒçš„æ—¥æœŸï¼š{top_date[0]}ï¼Œå…±æœ‰ {top_date[1]} ä¸ªäº‹ä»¶",
                    "details": {"date": str(top_date[0]), "count": top_date[1]}
                })
        
        return patterns
    
    def _generate_recommendations(self, report: Dict) -> List[Dict]:
        """ç”ŸæˆçŸ¥è¯†å›¾è°±ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºå›¾è°±ç»Ÿè®¡ç”Ÿæˆå»ºè®®
        stats = report.get("graph_statistics", {})
        
        # å¦‚æœå®ä½“æ•°é‡è¾ƒå°‘
        if stats.get("total_entities", 0) < 100:
            recommendations.append({
                "type": "entity_expansion",
                "message": "çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®æ‰©å¤§æ•°æ®æºæˆ–å¢åŠ å®ä½“æå–çš„å¹¿åº¦",
                "priority": "high"
            })
        
        # å¦‚æœå¹³å‡æ¯ä¸ªå®ä½“å…³è”çš„äº‹ä»¶å¤ªå°‘
        if stats.get("avg_events_per_entity", 0) < 5:
            recommendations.append({
                "type": "event_richness",
                "message": "å®ä½“å…³è”çš„äº‹ä»¶æ•°é‡åå°‘ï¼Œå¯èƒ½å½±å“å›¾è°±çš„ä¸°å¯Œåº¦å’Œåˆ†æä»·å€¼",
                "priority": "medium"
            })
        
        # å¦‚æœå®ä½“é—´å…³ç³»è¾ƒå°‘
        if stats.get("total_entity_entity_relations", 0) < stats.get("total_entities", 1) * 2:
            recommendations.append({
                "type": "relation_enhancement",
                "message": "å®ä½“é—´å…³ç³»å¯†åº¦è¾ƒä½ï¼Œå»ºè®®å¢å¼ºå®ä½“å…±ç°åˆ†æ",
                "priority": "medium"
            })
        
        return recommendations
    
    def detect_anomalies(self) -> List[Dict]:
        """æ£€æµ‹çŸ¥è¯†å›¾è°±ä¸­çš„å¼‚å¸¸å…³ç³»"""
        import time
        start_time = time.time()
        tools.log(f"ğŸ” [DEBUG] å¼€å§‹æ£€æµ‹çŸ¥è¯†å›¾è°±å¼‚å¸¸")
        
        try:
            global KG
            anomalies = []
            
            # æ£€æµ‹å­¤ç‚¹å®ä½“ï¼ˆæ²¡æœ‰å…³è”äº‹ä»¶å’Œå…³ç³»çš„å®ä½“ï¼‰
            tools.log(f"ğŸ” [DEBUG] æ£€æµ‹å­¤ç‚¹å®ä½“")
            lonely_entities = []
            all_entities = KG.get_all_entities()
            tools.log(f"ğŸ” [DEBUG] æ€»å®ä½“æ•°: {len(all_entities)}")
            
            for entity in all_entities:
                entity_events = KG.get_entity_events(entity)
                related_entities = KG.get_related_entities(entity)
                if len(entity_events) == 0 and len(related_entities) == 0:
                    lonely_entities.append(entity)
            
            if lonely_entities:
                tools.log(f"âš ï¸ [DEBUG] å‘ç° {len(lonely_entities)} ä¸ªå­¤ç‚¹å®ä½“")
                anomalies.append({
                    "type": "lonely_entities",
                    "message": f"å‘ç° {len(lonely_entities)} ä¸ªå­¤ç‚¹å®ä½“ï¼ˆæ²¡æœ‰å…³è”äº‹ä»¶å’Œå…³ç³»ï¼‰",
                    "details": lonely_entities[:10]  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                })
        
            # æ£€æµ‹è¿‡åº¦å…³è”çš„å®ä½“ï¼ˆå¯èƒ½å­˜åœ¨å™ªå£°ï¼‰
            tools.log(f"ğŸ” [DEBUG] æ£€æµ‹è¿‡åº¦å…³è”çš„å®ä½“")
            overly_connected = []
            
            for entity in all_entities:
                events = KG.get_entity_events(entity)
                if len(events) > 100:  # å…³è”äº‹ä»¶è¶…è¿‡100ä¸ª
                    overly_connected.append({
                        "entity": entity,
                        "event_count": len(events)
                    })
            
            if overly_connected:
                tools.log(f"âš ï¸ [DEBUG] å‘ç° {len(overly_connected)} ä¸ªè¿‡åº¦å…³è”çš„å®ä½“")
                anomalies.append({
                    "type": "overly_connected",
                    "message": f"å‘ç° {len(overly_connected)} ä¸ªè¿‡åº¦å…³è”çš„å®ä½“ï¼ˆäº‹ä»¶å…³è”æ•°å¼‚å¸¸é«˜ï¼‰",
                    "details": overly_connected[:5]
                })
            
            processing_time = (time.time() - start_time) * 1000
            tools.log(f"âœ… [DEBUG] å¼‚å¸¸æ£€æµ‹å®Œæˆï¼Œå‘ç° {len(anomalies)} ç§å¼‚å¸¸ï¼Œè€—æ—¶: {processing_time:.2f}ms")
            return anomalies
        except Exception as e:
            tools.log(f"âŒ [DEBUG] æ£€æµ‹å¼‚å¸¸å‡ºé”™: {str(e)}")
            import traceback
            tools.log(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return [{"error": str(e)}]

# å…¨å±€å®ä¾‹
kg_visualizer = KGVisualizer()
kg_explainer = KGExplainer()

# ä¾¿æ·å‡½æ•°
def visualize_entities(entities: List[str], depth: int = 2, output_format: str = "base64") -> Union[str, plt.Figure]:
    """å¯è§†åŒ–æŒ‡å®šå®ä½“åŠå…¶å…³ç³»"""
    tools.log(f"ğŸ” [DEBUG] è°ƒç”¨ä¾¿æ·å‡½æ•° visualize_entitiesï¼Œå®ä½“æ•°: {len(entities)}, æ·±åº¦: {depth}")
    kg_visualizer.build_visualization_graph(entities=entities, depth=depth)
    return kg_visualizer.generate_plot_image(output_format)

def visualize_full_graph(max_nodes: int = 100, output_format: str = "base64") -> Union[str, plt.Figure]:
    """å¯è§†åŒ–å®Œæ•´å›¾è°±ï¼ˆé™åˆ¶èŠ‚ç‚¹æ•°é‡ï¼‰"""
    tools.log(f"ğŸ” [DEBUG] è°ƒç”¨ä¾¿æ·å‡½æ•° visualize_full_graphï¼Œæœ€å¤§èŠ‚ç‚¹æ•°: {max_nodes}")
    # æ„å»ºä¸€ä¸ªåŒ…å«é‡è¦å®ä½“çš„å›¾è°±
    global KG
    if KG is None:
        tools.log(f"ğŸ” [DEBUG] è·å–çŸ¥è¯†å›¾è°±å®ä¾‹")
        from .kg_interface import get_knowledge_graph
        KG = get_knowledge_graph()
    
    # è·å–äº‹ä»¶æœ€å¤šçš„å®ä½“ä½œä¸ºèµ·å§‹ç‚¹
    entity_event_counts = {}
    # ç»Ÿè®¡æ¯ä¸ªå®ä½“å‚ä¸çš„äº‹ä»¶æ•°é‡
    all_entities = KG.get_all_entities()
    for entity in all_entities:
        entity_events = KG.get_entity_events(entity)
        entity_event_counts[entity] = len(entity_events)
    
    # æ’åºå¹¶è·å–å‰Nä¸ªå®ä½“
    sorted_entities = sorted(entity_event_counts.items(), key=lambda x: x[1], reverse=True)[:max_nodes//2]
    start_entities = [entity for entity, _ in sorted_entities]
    
    kg_visualizer.build_visualization_graph(entities=start_entities, depth=1)
    return kg_visualizer.generate_plot_image(output_format)

def generate_report(focus_entity: Optional[str] = None) -> Dict:
    """ç”ŸæˆçŸ¥è¯†å›¾è°±è§£é‡ŠæŠ¥å‘Š"""
    tools.log(f"ğŸ” [DEBUG] è°ƒç”¨ä¾¿æ·å‡½æ•° generate_reportï¼Œå…³æ³¨å®ä½“: {focus_entity if focus_entity else 'å…¨å±€'}")
    return kg_explainer.generate_explanation_report(focus_entity)

def detect_graph_anomalies() -> List[Dict]:
    """æ£€æµ‹çŸ¥è¯†å›¾è°±å¼‚å¸¸"""
    tools.log(f"ğŸ” [DEBUG] è°ƒç”¨ä¾¿æ·å‡½æ•° detect_graph_anomalies")
    return kg_explainer.detect_anomalies()

def get_graph_statistics() -> Dict:
    """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
    tools.log(f"ğŸ” [DEBUG] è°ƒç”¨ä¾¿æ·å‡½æ•° get_graph_statistics")
    return kg_explainer._get_graph_statistics()

def export_graph_data(format: str = "json") -> Union[Dict, str]:
    """
    å¯¼å‡ºçŸ¥è¯†å›¾è°±æ•°æ®
    
    Args:
        format: å¯¼å‡ºæ ¼å¼ï¼Œæ”¯æŒ"json"
    
    Returns:
        å¯¼å‡ºçš„æ•°æ®
    """
    import time
    start_time = time.time()
    tools.log(f"ğŸ” [DEBUG] è°ƒç”¨ä¾¿æ·å‡½æ•° export_graph_dataï¼Œæ ¼å¼: {format}")
    
    try:
        global KG
        if KG is None:
            tools.log(f"ğŸ” [DEBUG] è·å–çŸ¥è¯†å›¾è°±å®ä¾‹")
            from .kg_interface import get_knowledge_graph
            KG = get_knowledge_graph()
            
        tools.log(f"ğŸ” [DEBUG] å‡†å¤‡å¯¼å‡ºçŸ¥è¯†å›¾è°±æ•°æ®")
        export_data = {
            "entities": KG.get_all_entities(),
            "events": KG.get_all_events(),
            "entity_relations": KG.get_entity_relations(),
            "entity_event_relations": KG.get_entity_event_relations(),
            "metadata": {
                "exported_at": datetime.now().isoformat(),
                "entity_count": len(KG.get_all_entities()),
                "event_count": len(KG.get_all_events())
            }
        }
        
        tools.log(f"ğŸ“Š [DEBUG] å¯¼å‡ºæ•°æ®ç»Ÿè®¡ - å®ä½“æ•°: {export_data['metadata']['entity_count']}, äº‹ä»¶æ•°: {export_data['metadata']['event_count']}")
        
        if format == "json":
            processing_time = (time.time() - start_time) * 1000
            tools.log(f"âœ… [DEBUG] æ•°æ®å¯¼å‡ºå®Œæˆï¼Œæ ¼å¼: jsonï¼Œè€—æ—¶: {processing_time:.2f}ms")
            return export_data
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")
    except Exception as e:
        tools.log(f"âŒ [DEBUG] å¯¼å‡ºæ•°æ®å‡ºé”™: {str(e)}")
        import traceback
        tools.log(f"âŒ [DEBUG] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise
    
    export_data = {
        "entities": KG.get_all_entities(),
        "events": KG.get_all_events(),
        "entity_relations": KG.get_entity_relations(),
        "entity_event_relations": KG.get_entity_event_relations(),
        "metadata": {
            "exported_at": datetime.now().isoformat(),
            "entity_count": len(KG.get_all_entities()),
            "event_count": len(KG.get_all_events())
        }
    }
    
    if format == "json":
        return export_data
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")
