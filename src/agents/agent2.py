# src/agents/agent2.py
"""
æ™ºèƒ½ä½“2ï¼šå®ä½“æ‹“å±•æ–°é—»

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»å®ä½“åº“ä¸­è·å–å·²æå–çš„å®ä½“
2. ä½¿ç”¨è¿™äº›å®ä½“ä½œä¸ºå…³é”®è¯æœç´¢ç›¸å…³æ–°é—»
3. å¯¹æœç´¢åˆ°çš„æ–°é—»è¿›è¡Œå¤„ç†ï¼Œæå–æ›´å¤šç›¸å…³å®ä½“å’Œäº‹ä»¶
4. æ›´æ–°å®ä½“åº“å’Œäº‹ä»¶æ˜ å°„
"""

import os
import sys
import json
import asyncio
import time
import argparse
from collections import defaultdict
from pathlib import Path
from typing import List, Dict, Set, Optional
from datetime import datetime, timezone, timedelta
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from dotenv import load_dotenv
from ..utils.tool_function import tools
from ..data.api_client import DataAPIPool
from ..data.news_collector import NewsType
from .agent1 import llm_extract_events, NewsDeduplicator
from ..utils.entity_updater import update_entities, update_abstract_map
from .agent3 import refresh_graph  # å¯¼å…¥çŸ¥è¯†å›¾è°±åˆ·æ–°åŠŸèƒ½

# åˆå§‹åŒ–å·¥å…·
tools = tools()

# ç¯å¢ƒå˜é‡/é…ç½®åŠ è½½
load_dotenv(dotenv_path=tools.CONFIG_DIR / ".env.local")

def _load_agent2_settings():
    defaults = {
        "max_workers": 3,
        "rate_limit_per_sec": 1.0,
    }
    cfg_path = tools.CONFIG_DIR / "config.yaml"
    try:
        import yaml
        if cfg_path.exists():
            data = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
            cfg = data.get("agent2_config", {})
            if isinstance(cfg, dict):
                for k, v in cfg.items():
                    if k in defaults:
                        defaults[k] = v
    except Exception:
        pass
    return defaults

_agent2_settings = _load_agent2_settings()
AGENT2_MAX_WORKERS = _agent2_settings["max_workers"]
AGENT2_RATE_LIMIT = _agent2_settings["rate_limit_per_sec"]

# åˆå§‹åŒ–æ•°æ®APIæ±  - ä½¿ç”¨æ›´æ–°åçš„APIæ± å®ç°
data_api_pool = DataAPIPool()

async def expand_news_by_entities(entities: List[Dict], limit_per_entity: int = 10, time_window_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    æ ¹æ®å®ä½“åˆ—è¡¨æœç´¢ç›¸å…³æ–°é—»ï¼Œæ”¯æŒä½¿ç”¨åŸå§‹è¯è¿›è¡Œæ£€ç´¢
    
    Args:
        entities: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«nameå’Œoriginal_formså­—æ®µ
        limit_per_entity: æ¯ä¸ªå®ä½“æœç´¢çš„æ–°é—»æ•°é‡é™åˆ¶
        time_window_days: é»˜è®¤æ£€ç´¢æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢ï¼Œå¦‚æœä¸ºTrueåˆ™ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´
        
    Returns:
        æœç´¢åˆ°çš„ç›¸å…³æ–°é—»åˆ—è¡¨
    """
    expanded_news = []
    news_id_set = set()  # ç”¨äºå»é‡
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨
    news_collectors = []
    available_sources = data_api_pool.list_available_sources()
    
    # ä¸æ›´æ–°åçš„APIæ± å…¼å®¹ï¼Œç§»é™¤å¯èƒ½çš„å¤‡ç”¨é€»è¾‘
    for source_name in available_sources:
        try:
            collector = data_api_pool.get_collector(source_name)
            news_collectors.append(collector)
        except Exception as e:
            tools.log(f"âš ï¸ æ— æ³•åˆ›å»ºæ–°é—»æ”¶é›†å™¨ {source_name}: {e}")
    
    if not news_collectors:
        tools.log("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨")
        return expanded_news
    
    # ä¸ºæ¯ä¸ªå®ä½“æœç´¢ç›¸å…³æ–°é—»
    for entity in entities:
        entity_name = entity['name']
        original_forms = entity.get('original_forms', [])
        
        # æ„å»ºä½¿ç”¨ORæ“ä½œç¬¦è¿æ¥çš„æœç´¢æŸ¥è¯¢ï¼šå®ä½“åç§° + æ‰€æœ‰åŸå§‹è¯
        all_terms = [entity_name] + original_forms
        
        # ç”ŸæˆæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶
        query_batches = []
        current_batch = []
        current_length = 0
        
        for term in all_terms:
            quoted_term = f'"{term}"'
            term_length = len(quoted_term)
            
            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯ï¼Œç›´æ¥æ·»åŠ ï¼›å¦åˆ™éœ€è¦è€ƒè™‘ORæ“ä½œç¬¦çš„é•¿åº¦
            if current_batch:
                required_length = current_length + 4 + term_length  # 4æ˜¯" OR "çš„é•¿åº¦
                if required_length > 200:
                    # å¦‚æœæ·»åŠ å½“å‰è¯ä¼šè¶…è¿‡é™åˆ¶ï¼Œä¿å­˜å½“å‰æ‰¹æ¬¡å¹¶å¼€å§‹æ–°æ‰¹æ¬¡
                    query_batches.append(" OR ".join(current_batch))
                    current_batch = [quoted_term]
                    current_length = term_length
                else:
                    current_batch.append(quoted_term)
                    current_length = required_length
            else:
                current_batch.append(quoted_term)
                current_length = term_length
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            query_batches.append(" OR ".join(current_batch))
        
        tools.log(f"ğŸ” ä¸ºå®ä½“ '{entity_name}' æœç´¢ç›¸å…³æ–°é—»ï¼Œä½¿ç”¨ORæŸ¥è¯¢è¿æ¥ {len(original_forms)} ä¸ªåŸå§‹è¯...")
        tools.log(f"   ğŸ“ ç”Ÿæˆäº† {len(query_batches)} ä¸ªæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶")
        
        # è·å–æ—¶é—´èŒƒå›´
        time_ranges = get_time_ranges(time_window_days, full_search)
        
        for collector in news_collectors:
            try:
                # å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰¹æ¬¡å’Œæ—¶é—´èŒƒå›´è¿›è¡Œæœç´¢
                for time_range in time_ranges:
                    start_date = time_range['start']
                    end_date = time_range['end']
                    
                    tools.log(f"   ğŸ“… æœç´¢æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
                    
                    for batch_index, batch_query in enumerate(query_batches):
                        tools.log(f"   ğŸ“ æ‰§è¡ŒæŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1}/{len(query_batches)}: '{batch_query}'")
                        
                        try:
                            # ä½¿ç”¨æœç´¢åŠŸèƒ½è·å–ç›¸å…³æ–°é—»ï¼Œä¼ å…¥æ—¶é—´èŒƒå›´å‚æ•°
                            search_params = {
                                'keyword' if hasattr(collector, 'search_news_by_keyword') else 'query': batch_query,
                                'limit': limit_per_entity // (len(query_batches) * len(time_ranges)) + 1  # å¹³å‡åˆ†é…é™åˆ¶
                            }
                            
                            # å¦‚æœæ”¶é›†å™¨æ”¯æŒæ—¶é—´èŒƒå›´å‚æ•°ï¼Œåˆ™æ·»åŠ 
                            if hasattr(collector, 'search_news_by_keyword'):
                                if 'start_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date
                            elif hasattr(collector, 'search'):
                                if 'start_date' in collector.search.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date
                            
                            # ä½¿ç”¨æ›´æ–°åçš„APIè°ƒç”¨æ–¹æ³•
                            # ä¼˜å…ˆä½¿ç”¨search_news_by_keywordæ–¹æ³•ï¼Œç¡®ä¿ä¸æ›´æ–°åçš„APIæ± å…¼å®¹
                            if hasattr(collector, 'search_news_by_keyword'):
                                news_list = await collector.search_news_by_keyword(**search_params)
                            elif hasattr(collector, 'search'):
                                news_list = await collector.search(**search_params)
                            else:
                                tools.log(f"âš ï¸ æ”¶é›†å™¨ {collector.__class__.__name__} æ²¡æœ‰æ”¯æŒçš„æœç´¢æ–¹æ³•")
                                continue
                            
                            # ä¸ºæ¯æ¡æ–°é—»æ·»åŠ å®ä½“æ ‡ç­¾å¹¶å»é‡
                            for news in news_list:
                                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦ç”¨äºå»é‡
                                news_id = f"{news.get('url', '')}_{news.get('publishedAt', '')}"
                                if news_id not in news_id_set:
                                    news_id_set.add(news_id)
                                    news['expanded_from_entity'] = entity_name
                                    news['search_term'] = batch_query  # è®°å½•ä½¿ç”¨çš„æœç´¢è¯
                                    news['source'] = collector.__class__.__name__.replace('Collector', '').lower()
                                    news['query_batch'] = batch_index + 1  # è®°å½•æŸ¥è¯¢æ‰¹æ¬¡
                                    news['search_time_range'] = f"{start_date} to {end_date}"  # è®°å½•æœç´¢æ—¶é—´èŒƒå›´
                                    expanded_news.append(news)
                        except Exception as batch_error:
                            tools.log(f"âš ï¸ æŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1} æ‰§è¡Œå¤±è´¥: {batch_error}")
            except Exception as e:
                tools.log(f"âš ï¸ ä» {collector.__class__.__name__} æœç´¢å¤±è´¥: {e}")
    
    return expanded_news

def get_time_ranges(default_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    è·å–æœç´¢çš„æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
    1. æ—¶é—´èŒƒå›´åªèƒ½ä»2020å¹´è‡³ä»Š
    2. é»˜è®¤æ£€ç´¢å‰30å¤©å†…çš„æ–°é—»
    3. å…¨é¢æ£€ç´¢æ—¶ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´
    
    Args:
        default_days: é»˜è®¤çš„æ—¶é—´çª—å£å¤©æ•°ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢
        
    Returns:
        æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«startå’Œendæ—¥æœŸå­—ç¬¦ä¸²
    """
    time_ranges = []
    now = datetime.now(timezone.utc)
    
    # å®šä¹‰2020å¹´1æœˆ1æ—¥ä½œä¸ºèµ·å§‹æ—¶é—´
    start_date_2020 = datetime(2020, 1, 1, tzinfo=timezone.utc)
    
    if not full_search:
        # éå…¨é¢æ£€ç´¢ï¼Œåªè¿”å›é»˜è®¤æ—¶é—´çª—å£
        end_date = now
        start_date = max(start_date_2020, now - timedelta(days=default_days))
        time_ranges.append({
            'start': start_date.strftime('%Y-%m-%d'),
            'end': end_date.strftime('%Y-%m-%d')
        })
    else:
        # å…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´
        tools.log("ğŸ”„ æ‰§è¡Œå…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æ‰¹æ¬¡ç›´åˆ°2020å¹´...")
        
        end_date = now
        batch_count = 0
        
        while end_date > start_date_2020:
            start_date = max(start_date_2020, end_date - timedelta(days=default_days))
            
            # ç¡®ä¿ä¸é‡å¤æ·»åŠ ç›¸åŒçš„æ—¶é—´èŒƒå›´
            if not time_ranges or time_ranges[-1]['start'] != start_date.strftime('%Y-%m-%d'):
                time_ranges.append({
                    'start': start_date.strftime('%Y-%m-%d'),
                    'end': end_date.strftime('%Y-%m-%d')
                })
            
            batch_count += 1
            end_date = start_date - timedelta(days=1)  # é¿å…æ—¥æœŸé‡å 
            
        tools.log(f"âœ… ç”Ÿæˆäº† {batch_count} ä¸ªæ—¶é—´èŒƒå›´æ‰¹æ¬¡")
    
    return time_ranges

def get_recent_entities(time_window_days: int = 30, limit: int = 50) -> List[Dict]:
    """
    è·å–æœ€è¿‘æ—¶é—´çª—å£å†…çš„å®ä½“åˆ—è¡¨ï¼ŒåŒ…å«åŸå§‹è¯ä¿¡æ¯
    
    Args:
        time_window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        limit: è¿”å›çš„å®ä½“æ•°é‡é™åˆ¶
        
    Returns:
        æœ€è¿‘çš„å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«åç§°å’ŒåŸå§‹è¯ä¿¡æ¯
    """
    entities = []
    
    if not tools.ENTITIES_FILE.exists():
        tools.log("âš ï¸ å®ä½“åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return entities
    
    # è¯»å–å®ä½“åº“
    with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
        entity_data = json.load(f)
    
    # æ ¹æ® first_seen æ’åºï¼Œè·å–æœ€è¿‘çš„å®ä½“
    sorted_entities = sorted(
        entity_data.items(),
        key=lambda x: x[1].get('first_seen', ''),
        reverse=True
    )
    
    # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„å®ä½“
    now = datetime.now(timezone.utc)
    time_window = timedelta(days=time_window_days)
    
    for entity_name, entity_info in sorted_entities:
        first_seen = entity_info.get('first_seen')
        if first_seen:
            try:
                # è§£ææ—¶é—´å­—ç¬¦ä¸²
                if 'T' in first_seen:
                    # ISOæ ¼å¼æ—¶é—´
                    seen_time = datetime.fromisoformat(first_seen.replace('Z', '+00:00'))
                else:
                    # æ™®é€šæ ¼å¼æ—¶é—´
                    seen_time = datetime.strptime(first_seen, '%Y-%m-%d %H:%M:%S')
                    seen_time = seen_time.replace(tzinfo=timezone.utc)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
                if now - seen_time <= time_window:
                    entity_info = {
                        'name': entity_name,
                        'original_forms': entity_data[entity_name].get('original_forms', [])
                    }
                    entities.append(entity_info)
                    if len(entities) >= limit:
                        break
            except Exception as e:
                tools.log(f"âš ï¸ è§£æå®ä½“ '{entity_name}' çš„æ—¶é—´æˆ³å¤±è´¥: {e}")
    
    tools.log(f"âœ… è·å–äº† {len(entities)} ä¸ªæœ€è¿‘å®ä½“")
    return entities

async def process_expanded_news(expanded_news: List[Dict]) -> int:
    """
    å¤„ç†æ‹“å±•çš„æ–°é—»ï¼Œæå–å®ä½“å’Œäº‹ä»¶
    
    Args:
        expanded_news: æ‹“å±•çš„æ–°é—»åˆ—è¡¨
        
    Returns:
        å¤„ç†çš„æ–°é—»æ•°é‡
    """
    processed_count = 0
    
    # åˆå§‹åŒ–æ–°é—»å»é‡å™¨
    deduplicator = NewsDeduplicator(threshold=tools.DEDUPE_THRESHOLD if hasattr(tools, 'DEDUPE_THRESHOLD') else 3)
    
    # åˆ›å»ºå»é‡é›†åˆï¼ˆIDå»é‡ï¼‰
    seen_news = set()
    
    # å¹¶å‘æ§åˆ¶
    sem = asyncio.Semaphore(AGENT2_MAX_WORKERS if AGENT2_MAX_WORKERS > 0 else 1)
    limiter_interval = 1.0 / AGENT2_RATE_LIMIT if AGENT2_RATE_LIMIT > 0 else 0
    limiter_lock = asyncio.Lock()

    async def rate_limit():
        if limiter_interval <= 0:
            return
        async with limiter_lock:
            # ç®€å•ä¸²è¡Œé™é€Ÿ
            await asyncio.sleep(limiter_interval)

    async def handle_one(news: Dict) -> int:
        nonlocal processed_count
        try:
            async with sem:
                news_id = news.get('id')
                source = news.get('source', 'unknown')
                if news_id:
                    news_key = f"{source}:{news_id}"
                    if news_key in seen_news:
                        return 0
                    seen_news.add(news_key)
                
                title = news.get('title', '')
                content = news.get('content', '')
                if not title:
                    return 0

                news_text = f"{title} {content}".strip()
                if deduplicator.is_duplicate(news_text):
                    return 0

                await rate_limit()
                loop = asyncio.get_running_loop()
                extracted = await loop.run_in_executor(None, llm_extract_events, title, content)

                if extracted:
                    all_entities = []
                    for ev in extracted:
                        all_entities.extend(ev['entities'])
                    
                    if all_entities:
                        published_at = news.get('datetime')
                        if published_at and isinstance(published_at, datetime):
                            published_at = published_at.isoformat()
                        
                        all_entities_original = all_entities
                        update_entities(all_entities, all_entities_original, source, published_at)
                        update_abstract_map(extracted, source, published_at)
                        return 1
        except Exception as e:
            tools.log(f"âš ï¸ å¤„ç†æ‹“å±•æ–°é—»å¤±è´¥: {e}")
        return 0

    tasks = [handle_one(news) for news in expanded_news]
    results = await asyncio.gather(*tasks)
    processed_count = sum(results)

    return processed_count


def persist_expanded_news_to_tmp(expanded_news: List[Dict], processed_ids: Set[str]) -> Optional[Path]:
    """
    å°†æ‹“å±•æ–°é—»å†™å…¥ tmp åŸå§‹æ–‡ä»¶å¹¶åšå»é‡ï¼Œè¿”å›å»é‡åçš„æ–‡ä»¶è·¯å¾„ã€‚
    """
    if not expanded_news:
        return None
    tools.RAW_NEWS_TMP_DIR.mkdir(parents=True, exist_ok=True)
    tools.DEDUPED_NEWS_TMP_DIR.mkdir(parents=True, exist_ok=True)

    ts = time.strftime("%Y%m%d%H%M%S")
    raw_path = tools.RAW_NEWS_TMP_DIR / f"expanded_{ts}.jsonl"
    deduped_path = tools.DEDUPED_NEWS_TMP_DIR / f"expanded_{ts}_deduped.jsonl"

    def _sanitize(item: Dict) -> Dict:
        clean = {}
        for k, v in item.items():
            if isinstance(v, datetime):
                clean[k] = v.isoformat()
            else:
                clean[k] = v
        return clean

    with open(raw_path, "w", encoding="utf-8") as f:
        for news in expanded_news:
            safe_news = _sanitize(news)
            f.write(json.dumps(safe_news, ensure_ascii=False) + "\n")

    deduper = NewsDeduplicator(threshold=tools.DEDUPE_THRESHOLD if hasattr(tools, 'DEDUPE_THRESHOLD') else 3)
    deduper.dedupe_file(raw_path, deduped_path, processed_ids)
    return deduped_path


def load_merge_rules(path: Path) -> Dict[str, str]:
    if not path.exists():
        return {}
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        return data.get("merge_rules", {})
    except Exception:
        return {}


def build_equiv_index(entities_file: Path, merge_rules_file: Path) -> Dict[str, Set[str]]:
    """
    æ„å»ºå®ä½“ç­‰ä»·è¯ç´¢å¼•ï¼šå®ä½“å/åŸå§‹è¯/åˆå¹¶è§„åˆ™åˆ«åäº’ç›¸æŒ‡å‘ã€‚
    """
    idx: Dict[str, Set[str]] = defaultdict(set)
    entities = {}
    if entities_file.exists():
        try:
            entities = json.loads(entities_file.read_text(encoding="utf-8"))
        except Exception as e:
            tools.log(f"âš ï¸ åŠ è½½å®ä½“åº“å¤±è´¥: {e}")

    merge_rules = load_merge_rules(merge_rules_file)
    # å»ºåå‘ç´¢å¼• primary -> duplicates
    rev_rules: Dict[str, Set[str]] = defaultdict(set)
    for dup, primary in merge_rules.items():
        rev_rules[primary].add(dup)

    for name, data in entities.items():
        forms = set()
        if name:
            forms.add(name)
        for f in data.get("original_forms", []):
            if isinstance(f, str) and f.strip():
                forms.add(f.strip())
        if name in merge_rules:  # name æ˜¯åˆ«å
            forms.add(merge_rules[name])
        if name in rev_rules:    # æœ‰åˆ«åæŒ‡å‘ name
            forms.update(rev_rules[name])
        for f in forms:
            idx[f].update(forms)

    # è§„åˆ™é‡Œå‡ºç°ä½†æœªåœ¨å®ä½“åº“çš„åˆ«å/ä¸»å
    for dup, primary in merge_rules.items():
        idx[dup].add(primary)
        idx[dup].add(dup)
        idx[primary].add(primary)
        idx[primary].add(dup)
    return idx


def expand_keywords_with_equivs(keywords: List[str], idx: Dict[str, Set[str]]) -> List[Dict[str, List[str]]]:
    """
    å°†è¾“å…¥å…³é”®è¯æ‰©å±•ä¸ºå®ä½“åŠå…¶åŸå§‹å½¢æ€åˆ—è¡¨ï¼Œä¾› OR åˆå¹¶ä½¿ç”¨ã€‚
    """
    expanded = []
    for kw in keywords:
        kw_norm = kw.strip()
        if not kw_norm:
            continue
        forms = set([kw_norm])
        if kw_norm in idx:
            forms.update(idx[kw_norm])
        expanded.append({
            "name": kw_norm,
            "original_forms": [f for f in forms if f != kw_norm]
        })
    return expanded

async def main(args: Optional[argparse.Namespace] = None):
    """
    ä¸»å‡½æ•°ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šå…³é”®è¯ã€æ—¶é—´çª—å£ã€æ•°é‡ç­‰ã€‚
    """
    tools.log("ğŸš€ å¯åŠ¨ Agent2ï¼šå®ä½“æ‹“å±•æ–°é—»...")
    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r", encoding="utf-8", errors="ignore") as f:
            processed_ids = set(line.strip() for line in f if line.strip())
    
    # 1. è·å–å®ä½“æ¥æºï¼šå‘½ä»¤è¡Œå…³é”®è¯æˆ–æœ€è¿‘å®ä½“
    if args and args.keywords:
        merge_rules_file = tools.CONFIG_DIR / "entity_merge_rules.json"
        idx = build_equiv_index(tools.ENTITIES_FILE, merge_rules_file)
        recent_entities = expand_keywords_with_equivs(args.keywords, idx)
        tools.log(f"ğŸ”– ä½¿ç”¨å‘½ä»¤è¡Œå…³é”®è¯ {len(recent_entities)} ä¸ªä½œä¸ºå®ä½“ï¼ˆå«ç­‰ä»·è¯æ‰©å±•ï¼‰")
    else:
        entity_limit = args.entity_limit if args else 1
        window_days = args.time_window_days if args else 30
        recent_entities = get_recent_entities(time_window_days=window_days, limit=entity_limit)
        if not recent_entities:
            tools.log("ğŸ“­ æ²¡æœ‰å¯ç”¨çš„å®ä½“è¿›è¡Œæ–°é—»æ‹“å±•")
            return
    
    # 2. ä½¿ç”¨å®ä½“æœç´¢ç›¸å…³æ–°é—»
    limit_per_entity = args.limit_per_entity if args else 120
    window_days = args.time_window_days if args else 30
    full_search = args.full_search if args else False
    tools.log(f"ğŸ” å¼€å§‹æœç´¢ {len(recent_entities)} ä¸ªå®ä½“çš„ç›¸å…³æ–°é—»...")
    expanded_news = await expand_news_by_entities(
        recent_entities,
        limit_per_entity=limit_per_entity,
        time_window_days=window_days,
        full_search=full_search
    )
    tools.log(f"âœ… å…±æœç´¢åˆ° {len(expanded_news)} æ¡ç›¸å…³æ–°é—»")
    
    # 3. å¤„ç†æœç´¢åˆ°çš„æ–°é—»
    if expanded_news:
        deduped_path = persist_expanded_news_to_tmp(expanded_news, processed_ids)
        processed_count = 0
        if deduped_path and deduped_path.exists():
            tools.log(f"ğŸ“„ å¼€å§‹å¤„ç†æ‹“å±•çš„æ–°é—» (deduped: {deduped_path.name}) ...")
            news_list = []
            with open(deduped_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news_list.append(json.loads(line))
                    except Exception as e:
                        tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
            processed_count = await process_expanded_news(news_list)
            # æ¸…ç† tmp æ–‡ä»¶
            try:
                raw_file = tools.RAW_NEWS_TMP_DIR / deduped_path.name.replace("_deduped", "")
                if raw_file.exists():
                    raw_file.unlink()
                    tools.log(f"ğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶åŸå§‹æ–‡ä»¶: {raw_file}")
                deduped_path.unlink()
                tools.log(f"ğŸ—‘ï¸ åˆ é™¤ä¸´æ—¶å»é‡æ–‡ä»¶: {deduped_path}")
            except Exception as e:
                tools.log(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        tools.log(f"âœ… æˆåŠŸå¤„ç† {processed_count} æ¡æ‹“å±•æ–°é—»")
    
    tools.log("ğŸ‰ å®ä½“æ‹“å±•æ–°é—»ä»»åŠ¡å®Œæˆï¼")


def parse_args():
    parser = argparse.ArgumentParser(description="Agent2 å®ä½“æ‹“å±•æ–°é—»")
    parser.add_argument("--keywords", "-k", nargs="+", help="æŒ‡å®šå®ä½“å…³é”®è¯åˆ—è¡¨ï¼Œæ›¿ä»£æœ€è¿‘å®ä½“")
    parser.add_argument("--entity-limit", type=int, default=1, help="ä»æœ€è¿‘å®ä½“åº“é€‰æ‹©çš„æ•°é‡ï¼ˆæœªæŒ‡å®šå…³é”®è¯æ—¶ç”Ÿæ•ˆï¼‰")
    parser.add_argument("--time-window-days", type=int, default=30, help="æœ€è¿‘å®ä½“æ—¶é—´çª—å£ / æœç´¢æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰")
    parser.add_argument("--limit-per-entity", type=int, default=120, help="æ¯ä¸ªå®ä½“æœç´¢æ–°é—»æ•°é‡ä¸Šé™")
    parser.add_argument("--full-search", action="store_true", help="æ˜¯å¦å…¨é¢æ£€ç´¢è‡³2020å¹´")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    asyncio.run(main(args))
