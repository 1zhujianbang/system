# src/agents/agent2.py
"""
æ™ºèƒ½ä½“2ï¼šå®ä½“æ‹“å±•æ–°é—»

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä»å®ä½“åº“ä¸­è·å–å·²æå–çš„å®ä½“
2. ä½¿ç”¨è¿™äº›å®ä½“ä½œä¸ºå…³é”®è¯æœç´¢ç›¸å…³æ–°é—»
3. å¯¹æœç´¢åˆ°çš„æ–°é—»è¿›è¡Œå¤„ç†ï¼Œæå–æ›´å¤šç›¸å…³å®ä½“å’Œäº‹ä»¶
4. æ›´æ–°å®ä½“åº“å’Œäº‹ä»¶æ˜ å°„
"""

import os
import sys
import json
import asyncio
from pathlib import Path
from typing import List, Dict, Set, Optional
from datetime import datetime, timezone, timedelta
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from dotenv import load_dotenv
from ..utils.tool_function import tools
from ..data.api_client import DataAPIPool
from ..data.news_collector import NewsType
from .agent1 import llm_extract_events, update_entities, update_abstract_map, NewsDeduplicator

# åˆå§‹åŒ–å·¥å…·
tools = tools()

# åˆå§‹åŒ–æ•°æ®APIæ±  - ä½¿ç”¨æ›´æ–°åçš„APIæ± å®ç°
data_api_pool = DataAPIPool()

async def expand_news_by_entities(entities: List[Dict], limit_per_entity: int = 10, time_window_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    æ ¹æ®å®ä½“åˆ—è¡¨æœç´¢ç›¸å…³æ–°é—»ï¼Œæ”¯æŒä½¿ç”¨åŸå§‹è¯è¿›è¡Œæ£€ç´¢
    
    Args:
        entities: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«nameå’Œoriginal_formså­—æ®µ
        limit_per_entity: æ¯ä¸ªå®ä½“æœç´¢çš„æ–°é—»æ•°é‡é™åˆ¶
        time_window_days: é»˜è®¤æ£€ç´¢æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢ï¼Œå¦‚æœä¸ºTrueåˆ™ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´
        
    Returns:
        æœç´¢åˆ°çš„ç›¸å…³æ–°é—»åˆ—è¡¨
    """
    expanded_news = []
    news_id_set = set()  # ç”¨äºå»é‡
    
    # è·å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨
    news_collectors = []
    available_sources = data_api_pool.list_available_sources()
    
    # ä¸æ›´æ–°åçš„APIæ± å…¼å®¹ï¼Œç§»é™¤å¯èƒ½çš„å¤‡ç”¨é€»è¾‘
    for source_name in available_sources:
        try:
            collector = data_api_pool.get_collector(source_name)
            news_collectors.append(collector)
        except Exception as e:
            tools.log(f"âš ï¸ æ— æ³•åˆ›å»ºæ–°é—»æ”¶é›†å™¨ {source_name}: {e}")
    
    if not news_collectors:
        tools.log("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨")
        return expanded_news
    
    # ä¸ºæ¯ä¸ªå®ä½“æœç´¢ç›¸å…³æ–°é—»
    for entity in entities:
        entity_name = entity['name']
        original_forms = entity.get('original_forms', [])
        
        # æ„å»ºä½¿ç”¨ORæ“ä½œç¬¦è¿æ¥çš„æœç´¢æŸ¥è¯¢ï¼šå®ä½“åç§° + æ‰€æœ‰åŸå§‹è¯
        all_terms = [entity_name] + original_forms
        
        # ç”ŸæˆæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶
        query_batches = []
        current_batch = []
        current_length = 0
        
        for term in all_terms:
            quoted_term = f'"{term}"'
            term_length = len(quoted_term)
            
            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯ï¼Œç›´æ¥æ·»åŠ ï¼›å¦åˆ™éœ€è¦è€ƒè™‘ORæ“ä½œç¬¦çš„é•¿åº¦
            if current_batch:
                required_length = current_length + 4 + term_length  # 4æ˜¯" OR "çš„é•¿åº¦
                if required_length > 200:
                    # å¦‚æœæ·»åŠ å½“å‰è¯ä¼šè¶…è¿‡é™åˆ¶ï¼Œä¿å­˜å½“å‰æ‰¹æ¬¡å¹¶å¼€å§‹æ–°æ‰¹æ¬¡
                    query_batches.append(" OR ".join(current_batch))
                    current_batch = [quoted_term]
                    current_length = term_length
                else:
                    current_batch.append(quoted_term)
                    current_length = required_length
            else:
                current_batch.append(quoted_term)
                current_length = term_length
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            query_batches.append(" OR ".join(current_batch))
        
        tools.log(f"ğŸ” ä¸ºå®ä½“ '{entity_name}' æœç´¢ç›¸å…³æ–°é—»ï¼Œä½¿ç”¨ORæŸ¥è¯¢è¿æ¥ {len(original_forms)} ä¸ªåŸå§‹è¯...")
        tools.log(f"   ğŸ“ ç”Ÿæˆäº† {len(query_batches)} ä¸ªæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶")
        
        # è·å–æ—¶é—´èŒƒå›´
        time_ranges = get_time_ranges(time_window_days, full_search)
        
        for collector in news_collectors:
            try:
                # å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰¹æ¬¡å’Œæ—¶é—´èŒƒå›´è¿›è¡Œæœç´¢
                for time_range in time_ranges:
                    start_date = time_range['start']
                    end_date = time_range['end']
                    
                    tools.log(f"   ğŸ“… æœç´¢æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")
                    
                    for batch_index, batch_query in enumerate(query_batches):
                        tools.log(f"   ğŸ“ æ‰§è¡ŒæŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1}/{len(query_batches)}: '{batch_query}'")
                        
                        try:
                            # ä½¿ç”¨æœç´¢åŠŸèƒ½è·å–ç›¸å…³æ–°é—»ï¼Œä¼ å…¥æ—¶é—´èŒƒå›´å‚æ•°
                            search_params = {
                                'keyword' if hasattr(collector, 'search_news_by_keyword') else 'query': batch_query,
                                'limit': limit_per_entity // (len(query_batches) * len(time_ranges)) + 1  # å¹³å‡åˆ†é…é™åˆ¶
                            }
                            
                            # å¦‚æœæ”¶é›†å™¨æ”¯æŒæ—¶é—´èŒƒå›´å‚æ•°ï¼Œåˆ™æ·»åŠ 
                            if hasattr(collector, 'search_news_by_keyword'):
                                if 'start_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date
                            elif hasattr(collector, 'search'):
                                if 'start_date' in collector.search.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date
                            
                            # ä½¿ç”¨æ›´æ–°åçš„APIè°ƒç”¨æ–¹æ³•
                            # ä¼˜å…ˆä½¿ç”¨search_news_by_keywordæ–¹æ³•ï¼Œç¡®ä¿ä¸æ›´æ–°åçš„APIæ± å…¼å®¹
                            if hasattr(collector, 'search_news_by_keyword'):
                                news_list = await collector.search_news_by_keyword(**search_params)
                            elif hasattr(collector, 'search'):
                                news_list = await collector.search(**search_params)
                            else:
                                tools.log(f"âš ï¸ æ”¶é›†å™¨ {collector.__class__.__name__} æ²¡æœ‰æ”¯æŒçš„æœç´¢æ–¹æ³•")
                                continue
                            
                            # ä¸ºæ¯æ¡æ–°é—»æ·»åŠ å®ä½“æ ‡ç­¾å¹¶å»é‡
                            for news in news_list:
                                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦ç”¨äºå»é‡
                                news_id = f"{news.get('url', '')}_{news.get('publishedAt', '')}"
                                if news_id not in news_id_set:
                                    news_id_set.add(news_id)
                                    news['expanded_from_entity'] = entity_name
                                    news['search_term'] = batch_query  # è®°å½•ä½¿ç”¨çš„æœç´¢è¯
                                    news['source'] = collector.__class__.__name__.replace('Collector', '').lower()
                                    news['query_batch'] = batch_index + 1  # è®°å½•æŸ¥è¯¢æ‰¹æ¬¡
                                    news['search_time_range'] = f"{start_date} to {end_date}"  # è®°å½•æœç´¢æ—¶é—´èŒƒå›´
                                    expanded_news.append(news)
                        except Exception as batch_error:
                            tools.log(f"âš ï¸ æŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1} æ‰§è¡Œå¤±è´¥: {batch_error}")
            except Exception as e:
                tools.log(f"âš ï¸ ä» {collector.__class__.__name__} æœç´¢å¤±è´¥: {e}")
    
    return expanded_news

def get_time_ranges(default_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    è·å–æœç´¢çš„æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
    1. æ—¶é—´èŒƒå›´åªèƒ½ä»2020å¹´è‡³ä»Š
    2. é»˜è®¤æ£€ç´¢å‰30å¤©å†…çš„æ–°é—»
    3. å…¨é¢æ£€ç´¢æ—¶ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´
    
    Args:
        default_days: é»˜è®¤çš„æ—¶é—´çª—å£å¤©æ•°ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢
        
    Returns:
        æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«startå’Œendæ—¥æœŸå­—ç¬¦ä¸²
    """
    time_ranges = []
    now = datetime.now(timezone.utc)
    
    # å®šä¹‰2020å¹´1æœˆ1æ—¥ä½œä¸ºèµ·å§‹æ—¶é—´
    start_date_2020 = datetime(2020, 1, 1, tzinfo=timezone.utc)
    
    if not full_search:
        # éå…¨é¢æ£€ç´¢ï¼Œåªè¿”å›é»˜è®¤æ—¶é—´çª—å£
        end_date = now
        start_date = max(start_date_2020, now - timedelta(days=default_days))
        time_ranges.append({
            'start': start_date.strftime('%Y-%m-%d'),
            'end': end_date.strftime('%Y-%m-%d')
        })
    else:
        # å…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´
        tools.log("ğŸ”„ æ‰§è¡Œå…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æ‰¹æ¬¡ç›´åˆ°2020å¹´...")
        
        end_date = now
        batch_count = 0
        
        while end_date > start_date_2020:
            start_date = max(start_date_2020, end_date - timedelta(days=default_days))
            
            # ç¡®ä¿ä¸é‡å¤æ·»åŠ ç›¸åŒçš„æ—¶é—´èŒƒå›´
            if not time_ranges or time_ranges[-1]['start'] != start_date.strftime('%Y-%m-%d'):
                time_ranges.append({
                    'start': start_date.strftime('%Y-%m-%d'),
                    'end': end_date.strftime('%Y-%m-%d')
                })
            
            batch_count += 1
            end_date = start_date - timedelta(days=1)  # é¿å…æ—¥æœŸé‡å 
            
        tools.log(f"âœ… ç”Ÿæˆäº† {batch_count} ä¸ªæ—¶é—´èŒƒå›´æ‰¹æ¬¡")
    
    return time_ranges

def get_recent_entities(time_window_days: int = 30, limit: int = 50) -> List[Dict]:
    """
    è·å–æœ€è¿‘æ—¶é—´çª—å£å†…çš„å®ä½“åˆ—è¡¨ï¼ŒåŒ…å«åŸå§‹è¯ä¿¡æ¯
    
    Args:
        time_window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        limit: è¿”å›çš„å®ä½“æ•°é‡é™åˆ¶
        
    Returns:
        æœ€è¿‘çš„å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«åç§°å’ŒåŸå§‹è¯ä¿¡æ¯
    """
    entities = []
    
    if not tools.ENTITIES_FILE.exists():
        tools.log("âš ï¸ å®ä½“åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return entities
    
    # è¯»å–å®ä½“åº“
    with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
        entity_data = json.load(f)
    
    # æ ¹æ® first_seen æ’åºï¼Œè·å–æœ€è¿‘çš„å®ä½“
    sorted_entities = sorted(
        entity_data.items(),
        key=lambda x: x[1].get('first_seen', ''),
        reverse=True
    )
    
    # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„å®ä½“
    now = datetime.now(timezone.utc)
    time_window = timedelta(days=time_window_days)
    
    for entity_name, entity_info in sorted_entities:
        first_seen = entity_info.get('first_seen')
        if first_seen:
            try:
                # è§£ææ—¶é—´å­—ç¬¦ä¸²
                if 'T' in first_seen:
                    # ISOæ ¼å¼æ—¶é—´
                    seen_time = datetime.fromisoformat(first_seen.replace('Z', '+00:00'))
                else:
                    # æ™®é€šæ ¼å¼æ—¶é—´
                    seen_time = datetime.strptime(first_seen, '%Y-%m-%d %H:%M:%S')
                    seen_time = seen_time.replace(tzinfo=timezone.utc)
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
                if now - seen_time <= time_window:
                    entity_info = {
                        'name': entity_name,
                        'original_forms': entity_data[entity_name].get('original_forms', [])
                    }
                    entities.append(entity_info)
                    if len(entities) >= limit:
                        break
            except Exception as e:
                tools.log(f"âš ï¸ è§£æå®ä½“ '{entity_name}' çš„æ—¶é—´æˆ³å¤±è´¥: {e}")
    
    tools.log(f"âœ… è·å–äº† {len(entities)} ä¸ªæœ€è¿‘å®ä½“")
    return entities

async def process_expanded_news(expanded_news: List[Dict]) -> int:
    """
    å¤„ç†æ‹“å±•çš„æ–°é—»ï¼Œæå–å®ä½“å’Œäº‹ä»¶
    
    Args:
        expanded_news: æ‹“å±•çš„æ–°é—»åˆ—è¡¨
        
    Returns:
        å¤„ç†çš„æ–°é—»æ•°é‡
    """
    processed_count = 0
    
    # åˆå§‹åŒ–æ–°é—»å»é‡å™¨
    deduplicator = NewsDeduplicator(threshold=tools.DEDUPE_THRESHOLD if hasattr(tools, 'DEDUPE_THRESHOLD') else 3)
    
    # åˆ›å»ºå»é‡é›†åˆï¼ˆIDå»é‡ï¼‰
    seen_news = set()
    
    for news in expanded_news:
        try:
            # 1. æ£€æŸ¥æ–°é—»æ˜¯å¦å·²å¤„ç†ï¼ˆIDå»é‡ï¼‰
            news_id = news.get('id')
            source = news.get('source', 'unknown')
            if news_id:
                news_key = f"{source}:{news_id}"
                if news_key in seen_news:
                    continue
                seen_news.add(news_key)
            
            title = news.get('title', '')
            content = news.get('content', '')
            
            if not title:
                continue
                
            # 2. ä½¿ç”¨å†…å®¹ç›¸ä¼¼åº¦å»é‡ï¼ˆåŸºäºsimhashï¼‰
            news_text = f"{title} {content}".strip()
            if deduplicator.is_duplicate(news_text):
                continue
            
            # æå–å®ä½“å’Œäº‹ä»¶
            extracted = llm_extract_events(title, content)
            
            if extracted:
                all_entities = []
                for ev in extracted:
                    all_entities.extend(ev['entities'])
                
                if all_entities:
                    # ä¼˜å…ˆä½¿ç”¨æ–°é—»è‡ªèº«çš„æ—¶é—´æˆ³
                    published_at = news.get('datetime')
                    if published_at and isinstance(published_at, datetime):
                        published_at = published_at.isoformat()
                    
                    # æ›´æ–°å®ä½“åº“å’Œäº‹ä»¶æ˜ å°„ï¼ˆåœ¨agent2ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰æå–åŸå§‹è¯ï¼Œæ‰€ä»¥ä½¿ç”¨å®ä½“åç§°ä½œä¸ºåŸå§‹è¯ï¼‰
                    all_entities_original = all_entities  # ä½¿ç”¨å®ä½“åç§°ä½œä¸ºåŸå§‹è¯
                    update_entities(all_entities, all_entities_original, source, published_at)
                    update_abstract_map(extracted, source, published_at)
                    processed_count += 1
                    
        except Exception as e:
            tools.log(f"âš ï¸ å¤„ç†æ‹“å±•æ–°é—»å¤±è´¥: {e}")
    
    return processed_count

async def main():
    """
    ä¸»å‡½æ•°
    """
    tools.log("ğŸš€ å¯åŠ¨ Agent2ï¼šå®ä½“æ‹“å±•æ–°é—»...")
    
    # 1. è·å–æœ€è¿‘çš„å®ä½“
    recent_entities = get_recent_entities(time_window_days=30, limit=1)
    
    if not recent_entities:
        tools.log("ğŸ“­ æ²¡æœ‰å¯ç”¨çš„å®ä½“è¿›è¡Œæ–°é—»æ‹“å±•")
        return
    
    # 2. ä½¿ç”¨å®ä½“æœç´¢ç›¸å…³æ–°é—»
    # é»˜è®¤åªæœç´¢æœ€è¿‘30å¤©çš„æ–°é—»ï¼Œè®¾ç½®full_search=Trueå¯è¿›è¡Œå…¨é¢æ£€ç´¢
    tools.log(f"ğŸ” å¼€å§‹æœç´¢ {len(recent_entities)} ä¸ªå®ä½“çš„ç›¸å…³æ–°é—»...")
    expanded_news = await expand_news_by_entities(recent_entities, limit_per_entity=120, time_window_days=30, full_search=False)
    tools.log(f"âœ… å…±æœç´¢åˆ° {len(expanded_news)} æ¡ç›¸å…³æ–°é—»")
    
    # 3. å¤„ç†æœç´¢åˆ°çš„æ–°é—»
    if expanded_news:
        tools.log("ğŸ“„ å¼€å§‹å¤„ç†æ‹“å±•çš„æ–°é—»...")
        processed_count = await process_expanded_news(expanded_news)
        tools.log(f"âœ… æˆåŠŸå¤„ç† {processed_count} æ¡æ‹“å±•æ–°é—»")
    
    tools.log("ğŸ‰ å®ä½“æ‹“å±•æ–°é—»ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
