# src/agents/agent1.py
"""
æ™ºèƒ½ä½“1ï¼šæµå¼æ–°é—»å»é‡ + LLMé©±åŠ¨çš„çœŸå®ä¸–ç•Œå®ä½“ä¸äº‹ä»¶æå–å™¨

æ ¸å¿ƒåŸåˆ™ï¼š
- å®ä½“ = èƒ½ç­¾ç½²åˆåŒã€è¢«èµ·è¯‰ã€å‘å¸ƒå…¬å‘Šã€æ‹¥æœ‰é“¶è¡Œè´¦æˆ·çš„ä¸»ä½“
  ï¼ˆè‡ªç„¶äººã€å…¬å¸ã€æ”¿åºœæœºæ„ã€å›½å®¶ã€åœ°åŒºã€å›½é™…ç»„ç»‡ï¼‰
- æ’é™¤ï¼šä»£å¸åç§°ã€æŠ€æœ¯æœ¯è¯­ã€æŠ½è±¡æ¦‚å¿µã€æƒ…ç»ªè¯ã€æ³›ç§°
- æå–å³è‡ªåŠ¨å†™å…¥ entities.jsonï¼Œæ— éœ€äººå·¥å®¡æ ¸
- æ¯ä¸ªäº‹ä»¶ç”Ÿæˆå”¯ä¸€æ‘˜è¦ï¼Œå¹¶å…³è”å®ä½“ä¸äº‹ä»¶æè¿°
"""

import os
import sys
import json
import re
import time
import hashlib
from pathlib import Path
from typing import List, Dict, Set
from datetime import datetime, timezone
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from dotenv import load_dotenv

# ======================
# è·¯å¾„ä¸é…ç½®
# ======================

ROOT_DIR = Path(__file__).parent.parent.parent
DATA_DIR = ROOT_DIR / "data"
CONFIG_DIR = ROOT_DIR / "config"
RAW_NEWS_DIR = DATA_DIR / "raw_news"
DEDUPED_NEWS_DIR = DATA_DIR / "deduped_news"
LOG_FILE = DATA_DIR / "logs" / "agent1.log"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for d in [DATA_DIR, RAW_NEWS_DIR, DEDUPED_NEWS_DIR, DATA_DIR / "logs"]:
    d.mkdir(parents=True, exist_ok=True)

# æ•°æ®æ–‡ä»¶
ENTITIES_FILE = DATA_DIR / "entities.json"
ABSTRACT_MAP_FILE = DATA_DIR / "abstract_to_event_map.json"
PROCESSED_IDS_FILE = DATA_DIR / "processed_ids.txt"
STOP_WORDS_FILE = DATA_DIR / "stop_words.txt"

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(CONFIG_DIR / ".env.local")
LLM_MODEL = os.getenv("AGENT1_LLM_MODEL", "deepseek-chat")
DEDUPE_THRESHOLD = int(os.getenv("AGENT1_DEDUPE_THRESHOLD", "3"))

# ======================
# å·¥å…·å‡½æ•°
# ======================

def log(msg: str):
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{now}] {msg}"
    print(line)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(line + "\n")

def load_stop_words() -> Set[str]:
    stop_words = set()
    if STOP_WORDS_FILE.exists():
        with open(STOP_WORDS_FILE, "r", encoding="utf-8") as f:
            for line in f:
                word = line.strip()
                if word and not word.startswith("#"):
                    stop_words.add(word)
    return stop_words

STOP_WORDS = load_stop_words()

def is_valid_entity(entity: str) -> bool:
    word = entity.strip()
    if not word or len(word) < 2:
        return False
    if word in STOP_WORDS:
        return False
    if word.isdigit() or re.match(r'^[0-9+\-\.%$â‚¬Â¥]+$', word):
        return False
    if not any(c.isalnum() for c in word):
        return False
    # æ’é™¤å¸¸è§æ— æ•ˆæ¨¡å¼
    if re.search(
        r'(ä¸Šæ¶¨|ä¸‹è·Œ|æš´æ¶¨|æš´è·Œ|åˆ©å¥½|åˆ©ç©º|å¸‚åœº|æŠ•èµ„è€…|ç”¨æˆ·|ç¤¾åŒº|'
        r'åè®®|é“¾|ä»£å¸|å¸ç§|é¡¹ç›®|å¹³å°|æŠ€æœ¯|ç³»ç»Ÿ|æœºåˆ¶|æ¦‚å¿µ|'
        r'è¡Œæƒ…|è¶‹åŠ¿|ä¿¡å·|ç­–ç•¥|æ¨¡å‹|ç®—æ³•|æ¡†æ¶|ç”Ÿæ€)$',
        word
    ):
        return False
    return True

def simhash(text: str, bits=64) -> int:
    text = re.sub(r'\s+', ' ', text.lower())
    tokens = text.split()
    v = [0] * bits
    for token in tokens:
        h = int(hashlib.md5(token.encode()).hexdigest(), 16)
        for i in range(bits):
            bit = (h >> i) & 1
            if bit:
                v[i] += 1
            else:
                v[i] -= 1
    hash_val = 0
    for i in range(bits):
        if v[i] > 0:
            hash_val |= (1 << i)
    return hash_val

def hamming_distance(h1: int, h2: int) -> int:
    return bin(h1 ^ h2).count("1")

# ======================
# æ–°é—»å»é‡å™¨
# ======================

class NewsDeduplicator:
    def __init__(self, threshold: int = 3):
        self.threshold = threshold
        self.seen_hashes: Set[int] = set()

    def is_duplicate(self, text: str) -> bool:
        h = simhash(text)
        for seen_h in self.seen_hashes:
            if hamming_distance(h, seen_h) <= self.threshold:
                return True
        self.seen_hashes.add(h)
        return False

    def dedupe_file(self, input_path: Path, output_path: Path):
        log(f"ğŸ” å»é‡ä¸­: {input_path.name}")
        seen_ids = set()
        if output_path.exists():
            with open(output_path, "r", encoding="utf-8") as f:
                for line in f:
                    item = json.loads(line)
                    seen_ids.add(item["id"])
        
        with open(input_path, "r", encoding="utf-8") as fin, \
             open(output_path, "a", encoding="utf-8") as fout:
            for line in fin:
                try:
                    news = json.loads(line)
                    if news["id"] in seen_ids:
                        continue
                    raw_text = (news.get("title", "") + " " + news.get("content", "")).strip()
                    if not raw_text:
                        continue
                    if self.is_duplicate(raw_text):
                        continue
                    fout.write(line)
                    seen_ids.add(news["id"])
                except Exception as e:
                    log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")

# ======================
# LLM ç»“æ„åŒ–æå–å™¨ï¼ˆå«ç²¾å‡†æç¤ºè¯ï¼‰
# ======================

def llm_extract_events(title: str, content: str) -> List[Dict]:
    try:
        from openai import OpenAI
    except ImportError:
        log("âŒ openai æœªå®‰è£…ï¼Œè·³è¿‡ LLM æå–")
        return []

    api_key = os.getenv("API_KEY")
    base_url = os.getenv("API_URL")
    if not api_key:
        log("âŒ API_KEY æœªè®¾ç½®")
        return []

    client = OpenAI(
        api_key=api_key,
        base_url=base_url
    )

    prompt = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡‘èä¸æ³•å¾‹ä¿¡æ¯ç»“æ„åŒ–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–°é—»ä¸­æå–æ‰€æœ‰**çœŸå®å­˜åœ¨çš„ã€å…·æœ‰æ³•å¾‹äººæ ¼æˆ–è¡Œæ”¿èŒèƒ½çš„å®ä½“**ã€‚

ã€å®ä½“å®šä¹‰ã€‘
âœ… å¿…é¡»æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
- æ˜¯è‡ªç„¶äººï¼ˆå¦‚ Elon Muskã€Cathie Woodï¼‰
- æ˜¯æ³¨å†Œå…¬å¸ï¼ˆå¦‚ Binanceã€Coinbaseã€Teslaï¼‰
- æ˜¯æ”¿åºœæœºæ„æˆ–éƒ¨é—¨ï¼ˆå¦‚ ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šã€ä¸­å›½äººæ°‘é“¶è¡Œã€æ¬§ç›Ÿå§”å‘˜ä¼šï¼‰
- æ˜¯ä¸»æƒå›½å®¶æˆ–æ˜ç¡®è¡Œæ”¿åŒºï¼ˆå¦‚ ç¾å›½ã€æ–°åŠ å¡ã€åŠ åˆ©ç¦å°¼äºšå·ã€é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºï¼‰
- æ˜¯å›½é™…ç»„ç»‡ï¼ˆå¦‚ å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡ã€è”åˆå›½ï¼‰

âŒ ä»¥ä¸‹å†…å®¹**ä¸å¾—**è§†ä¸ºå®ä½“ï¼š
- æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚ â€œå»ä¸­å¿ƒåŒ–â€ã€â€œæµåŠ¨æ€§â€ã€â€œå¸‚åœºæƒ…ç»ªâ€ï¼‰
- æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚ â€œæ™ºèƒ½åˆçº¦â€ã€â€œé›¶çŸ¥è¯†è¯æ˜â€ã€â€œPoSâ€ï¼‰
- ä»£å¸/èµ„äº§åç§°ï¼ˆå¦‚ â€œBTCâ€ã€â€œä»¥å¤ªåŠâ€ã€â€œSolanaâ€ï¼‰â€”â€”é™¤éæŒ‡ä»£å…¶åŸºé‡‘ä¼šæˆ–å¼€å‘å…¬å¸ï¼ˆå¦‚ â€œä»¥å¤ªåŠåŸºé‡‘ä¼šâ€ï¼‰
- æ³›ç§°ï¼ˆå¦‚ â€œæŠ•èµ„è€…â€ã€â€œç›‘ç®¡æœºæ„â€ã€â€œæŸäº¤æ˜“æ‰€â€ï¼‰
- æƒ…ç»ª/è¡Œæƒ…æè¿°ï¼ˆå¦‚ â€œç‰›å¸‚â€ã€â€œæš´è·Œâ€ã€â€œåˆ©å¥½â€ï¼‰

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. åˆ¤æ–­æ–°é—»æ˜¯å¦åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªç‹¬ç«‹äº‹ä»¶ã€‚
2. å¯¹æ¯ä¸ªäº‹ä»¶ï¼Œè¾“å‡ºï¼š
   - ä¸€ä¸ªç®€æ´ã€å®¢è§‚ã€æ— æƒ…ç»ªçš„ä¸­æ–‡æ‘˜è¦ï¼ˆä½œä¸ºäº‹ä»¶å”¯ä¸€æ ‡è¯†ï¼‰
   - æ‰€æœ‰ç¬¦åˆä¸Šè¿°å®šä¹‰çš„å®ä½“ï¼ˆå…¨ç§°ä¼˜å…ˆï¼Œé¿å…ç¼©å†™ï¼‰
   - è¯¥äº‹ä»¶çš„æœ¬è´¨æè¿°ï¼ˆä¸€å¥è¯è¯´æ˜â€œè°å¯¹è°åšäº†ä»€ä¹ˆâ€ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡æœ¬ï¼š
{{
  "events": [
    {{
      "abstract": "ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šæ¨è¿Ÿå¯¹æ¯”ç‰¹å¸ETFçš„æœ€ç»ˆå†³å®š",
      "entities": ["ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼š", "VanEck"],
      "event_summary": "ç›‘ç®¡æœºæ„å»¶é•¿äº†å¯¹æŸèµ„äº§ç®¡ç†å…¬å¸æ¯”ç‰¹å¸ETFç”³è¯·çš„å®¡æŸ¥æœŸ"
    }}
  ]
}}

ã€æ–°é—»ã€‘
æ ‡é¢˜ï¼š{title}
æ­£æ–‡ï¼š{content}"""

    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=8000,
            timeout=600,
            stream=False
        )
        content = response.choices[0].message.content.strip()

        # æ¸…ç† Markdown åŒ…è£¹
        if content.startswith("```json"):
            content = content.split("```json", 1)[1].split("```")[0]
        elif content.startswith("```"):
            content = content.split("```", 1)[1].split("```")[0]

        data = json.loads(content)
        events = data.get("events", [])
        result = []
        for item in events:
            abstract = item.get("abstract", "").strip()
            entities = [e for e in item.get("entities", []) if is_valid_entity(e)]
            summary = item.get("event_summary", "").strip()
            if abstract and entities and summary:
                result.append({
                    "abstract": abstract,
                    "entities": entities,
                    "event_summary": summary
                })
        return result
    except Exception as e:
        log(f"âŒ LLM æå–å¤±è´¥: {e}")
        return []

# ======================
# è‡ªåŠ¨æ›´æ–°çŸ¥è¯†åº“
# ======================

def update_entities(entities: List[str], source: str):
    """è‡ªåŠ¨å†™å…¥ä¸»å®ä½“åº“"""
    now = datetime.now(timezone.utc).isoformat()
    existing = {}
    if ENTITIES_FILE.exists():
        with open(ENTITIES_FILE, "r", encoding="utf-8") as f:
            existing = json.load(f)
    
    for ent in entities:
        if ent not in existing:
            existing[ent] = {
                "first_seen": now,
                "sources": [source]
            }
        else:
            if source not in existing[ent]["sources"]:
                existing[ent]["sources"].append(source)
    
    with open(ENTITIES_FILE, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)

def update_abstract_map(extracted_list: List[Dict], source: str):
    abstract_map = {}
    if ABSTRACT_MAP_FILE.exists():
        with open(ABSTRACT_MAP_FILE, "r", encoding="utf-8") as f:
            abstract_map = json.load(f)
    
    now = datetime.now(timezone.utc).isoformat()
    for item in extracted_list:
        key = item["abstract"]
        if key not in abstract_map:
            abstract_map[key] = {
                "entities": item["entities"],
                "event_summary": item["event_summary"],
                "sources": [source],
                "first_seen": now
            }
        else:
            s_set = set(abstract_map[key]["sources"])
            s_set.add(source)
            abstract_map[key]["sources"] = sorted(s_set)
    
    with open(ABSTRACT_MAP_FILE, "w", encoding="utf-8") as f:
        json.dump(abstract_map, f, ensure_ascii=False, indent=2)

# ======================
# ä¸»å¤„ç†æµç¨‹
# ======================

def get_unprocessed_news_files() -> List[Path]:
    processed_ids = set()
    if PROCESSED_IDS_FILE.exists():
        with open(PROCESSED_IDS_FILE, "r") as f:
            processed_ids = set(line.strip() for line in f if line.strip())
    
    unprocessed = []
    for raw_file in sorted(RAW_NEWS_DIR.glob("*.jsonl")):
        deduped_file = DEDUPED_NEWS_DIR / f"{raw_file.stem}_deduped.jsonl"
        if not deduped_file.exists():
            deduper = NewsDeduplicator(threshold=DEDUPE_THRESHOLD)
            deduper.dedupe_file(raw_file, deduped_file)
        unprocessed.append(deduped_file)
    return unprocessed

def process_news_stream():
    log("ğŸš€ å¯åŠ¨ Agent1ï¼šæµå¼äº‹ä»¶ä¸çœŸå®å®ä½“æå–...")
    files = get_unprocessed_news_files()
    if not files:
        log("ğŸ“­ æ— å¯å¤„ç†æ–°é—»æ–‡ä»¶")
        return

    processed_ids = set()
    if PROCESSED_IDS_FILE.exists():
        with open(PROCESSED_IDS_FILE, "r") as f:
            processed_ids = set(line.strip() for line in f if line.strip())

    total_processed = 0
    with open(PROCESSED_IDS_FILE, "a") as id_log:
        for file_path in files:
            log(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news = json.loads(line)
                        news_id = news["id"]
                        if news_id in processed_ids:
                            continue

                        title = news.get("title", "")
                        content = news.get("content", "")
                        source = news.get("source", "unknown")

                        extracted = llm_extract_events(title, content)
                        # é˜²æ­¢ API é™æµ
                        time.sleep(5)   
                        if extracted:
                            all_entities = []
                            for ev in extracted:
                                all_entities.extend(ev["entities"])
                            if all_entities:
                                update_entities(all_entities, source)
                                update_abstract_map(extracted, source)
                                total_processed += 1

                        id_log.write(news_id + "\n")
                        processed_ids.add(news_id)

                    except Exception as e:
                        log(f"âš ï¸ å¤„ç†å•æ¡æ–°é—»å¤±è´¥: {e}")
                        continue

             
    log(f"âœ… å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡å«æœ‰æ•ˆå®ä½“çš„æ–°é—»")

# ======================
# å…¥å£
# ======================

if __name__ == "__main__":
    process_news_stream()