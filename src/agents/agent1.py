# src/agents/agent1.py
"""
æ™ºèƒ½ä½“1ï¼šæµå¼æ–°é—»å»é‡ + LLMé©±åŠ¨çš„çœŸå®ä¸–ç•Œå®ä½“ä¸äº‹ä»¶æå–å™¨

æ ¸å¿ƒåŸåˆ™ï¼š
- å®ä½“ = èƒ½ç­¾ç½²åˆåŒã€è¢«èµ·è¯‰ã€å‘å¸ƒå…¬å‘Šã€æ‹¥æœ‰é“¶è¡Œè´¦æˆ·çš„ä¸»ä½“
  ï¼ˆè‡ªç„¶äººã€å…¬å¸ã€æ”¿åºœæœºæ„ã€å›½å®¶ã€åœ°åŒºã€å›½é™…ç»„ç»‡ï¼‰
- æ’é™¤ï¼šæŠ€æœ¯æœ¯è¯­ã€æŠ½è±¡æ¦‚å¿µã€æƒ…ç»ªè¯ã€æ³›ç§°
- æå–å³è‡ªåŠ¨å†™å…¥ entities.jsonï¼Œæ— éœ€äººå·¥å®¡æ ¸
- æ¯ä¸ªäº‹ä»¶ç”Ÿæˆå”¯ä¸€æ‘˜è¦ï¼Œå¹¶å…³è”å®ä½“ä¸äº‹ä»¶æè¿°
- è‡ªåŠ¨æ›´æ–°çŸ¥è¯†å›¾è°±ï¼Œç»´æŠ¤å®ä½“-äº‹ä»¶å…³ç³»ç½‘ç»œ
"""

import os
import sys
import json
import re
import time
import hashlib
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple
from datetime import datetime, timezone
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from dotenv import load_dotenv
from ..utils.tool_function import tools
tools = tools()
load_dotenv(dotenv_path=tools.CONFIG_DIR / ".env.local")
from .api_client import LLMAPIPool
from ..utils.entity_updater import update_entities, update_abstract_map
from .agent3 import refresh_graph
API_POOL = None

# ---------- é…ç½®åŠ è½½ï¼šä¼˜å…ˆ config.yamlï¼Œå†å…è®¸ç¯å¢ƒå˜é‡è¦†ç›– ----------
def _load_agent1_settings():
    defaults = {
        "max_workers": 4,
        "rate_limit_per_sec": 1.5,
        "dedupe_threshold": 3,
    }
    cfg_path = tools.CONFIG_DIR / "config.yaml"
    try:
        import yaml  # å±€éƒ¨å¯¼å…¥ï¼Œé¿å…ç¡¬ä¾èµ–
        if cfg_path.exists():
            data = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
            cfg = data.get("agent1_config", {})
            if isinstance(cfg, dict):
                for k, v in cfg.items():
                    if k in defaults:
                        defaults[k] = v
    except Exception:
        pass
    return defaults

_agent1_settings = _load_agent1_settings()
MAX_WORKERS = int(_agent1_settings["max_workers"])
RATE_LIMIT_PER_SEC = float(_agent1_settings["rate_limit_per_sec"])
tools.DEDUPE_THRESHOLD = int(_agent1_settings["dedupe_threshold"])

def init_api_pool():
    global API_POOL
    if API_POOL is None:
        API_POOL = LLMAPIPool()


class RateLimiter:
    """ç®€å•çš„çº¿ç¨‹å®‰å…¨ä»¤ç‰Œæ¡¶ï¼ˆå›ºå®šé€Ÿç‡ï¼‰ï¼Œæ§åˆ¶ LLM QPS"""
    def __init__(self, rate_per_sec: float):
        self.interval = 1.0 / rate_per_sec if rate_per_sec > 0 else 0
        self._lock = threading.Lock()
        self._next = 0.0

    def acquire(self):
        if self.interval <= 0:
            return
        with self._lock:
            now = time.time()
            if now < self._next:
                time.sleep(self._next - now)
            self._next = max(self._next, now) + self.interval


# ======================
# æ–°é—»å»é‡å™¨
# ======================

class NewsDeduplicator:
    def __init__(self, threshold: int = 3):
        self.threshold = threshold
        self.seen_hashes: Set[int] = set()

    @staticmethod
    def _news_key(news: Dict) -> str:
        """æ„é€ ç”¨äºå»é‡çš„å”¯ä¸€é”®ï¼ŒåŒ…å« source å‰ç¼€ï¼Œå…¼å®¹å¤šæ•°æ®æºã€‚"""
        return f"{news.get('source', 'unknown')}:{news.get('id')}"

    def is_duplicate(self, text: str) -> bool:
        h = tools.simhash(text)
        for seen_h in self.seen_hashes:
            if tools.hamming_distance(h, seen_h) <= self.threshold:
                return True
        self.seen_hashes.add(h)
        return False

    def dedupe_file(self, input_path: Path, output_path: Path, processed_ids: Optional[Set[str]] = None):
        """
        å¯¹å•ä¸ªåŸå§‹æ–‡ä»¶åšå»é‡ï¼š
        - å…ˆç”¨ processed_idsï¼ˆå…¨å±€å·²å¤„ç† IDï¼Œå¦‚ blockbeats:323066ï¼‰è¿‡æ»¤å†å²å·²å¤„ç†æ–°é—»
        - å†ç»“åˆå·²æœ‰å»é‡æ–‡ä»¶ & simhash å»æ‰æœ¬æ‰¹å†…/è·¨æ‰¹çš„é‡å¤å†…å®¹
        """
        tools.log(f"ğŸ” å»é‡ä¸­: {input_path.name}")

        # å…ˆåŠ è½½â€œå…¨å±€å·²å¤„ç† IDâ€ï¼Œé¿å…è€æ–°é—»å†æ¬¡è¿›å…¥å»é‡ç»“æœ
        seen_ids: Set[str] = set(processed_ids or set())
        if processed_ids:
            tools.log(f"ğŸ” å·²æœ‰å†å² processed_ids æ•°é‡: {len(processed_ids)}")

        # å†åŠ è½½å·²æœ‰å»é‡ç»“æœæ–‡ä»¶ä¸­çš„ IDï¼Œå®ç°è·¨æ‰¹æ¬¡çš„æœ¬åœ°å»é‡
        if output_path.exists():
            with open(output_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        item = json.loads(line)
                        seen_ids.add(self._news_key(item))
                    except Exception as e:
                        tools.log(f"âš ï¸ è¯»å–å†å²å»é‡æ–‡ä»¶æ—¶è·³è¿‡æ— æ•ˆè¡Œ: {e}")
        
        kept, skipped_id, skipped_sim = 0, 0, 0
        with open(input_path, "r", encoding="utf-8") as fin, \
             open(output_path, "a", encoding="utf-8") as fout:
            for line in fin:
                try:
                    news = json.loads(line)
                    key = self._news_key(news)

                    # 1) æŒ‰å…¨å±€ ID å»é‡ï¼ˆåŒ…æ‹¬ processed_ids å’Œå·²æœ‰å»é‡æ–‡ä»¶ä¸­çš„ IDï¼‰
                    if key in seen_ids:
                        skipped_id += 1
                        continue

                    # 2) æ„é€ æ–‡æœ¬ï¼ŒæŒ‰å†…å®¹ç›¸ä¼¼åº¦å»é‡
                    raw_text = (news.get("title", "") + " " + news.get("content", "")).strip()
                    if not raw_text:
                        continue
                    if self.is_duplicate(raw_text):
                        skipped_sim += 1
                        continue
                    fout.write(line)
                    seen_ids.add(key)
                    kept += 1
                except Exception as e:
                    tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
        tools.log(f"âœ… å»é‡å®Œæˆ: ä¿ç•™ {kept} æ¡, æŒ‰ ID è·³è¿‡ {skipped_id} æ¡, æŒ‰ç›¸ä¼¼åº¦è·³è¿‡ {skipped_sim} æ¡")

# ======================
# LLM ç»“æ„åŒ–æå–å™¨ï¼ˆå«ç²¾å‡†æç¤ºè¯ï¼‰
# ======================

def llm_extract_events(title: str, content: str, max_retries=2) -> List[Dict]:
    # åˆå§‹åŒ– API æ± ï¼ˆå•ä¾‹ï¼‰
    init_api_pool()
    if API_POOL is None:
        tools.log("[LLMè¯·æ±‚] âŒ API æ± æœªåˆå§‹åŒ–")
        return []

    prompt = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡‘èä¸æ³•å¾‹ä¿¡æ¯ç»“æ„åŒ–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–°é—»ä¸­æå–æ‰€æœ‰**çœŸå®å­˜åœ¨çš„ã€å…·æœ‰æ³•å¾‹äººæ ¼æˆ–è¡Œæ”¿èŒèƒ½çš„å®ä½“**ã€‚

ã€å®ä½“å®šä¹‰ã€‘
âœ… å¿…é¡»æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
- è‡ªç„¶äººï¼ˆå¦‚ Elon Muskã€Cathie Woodã€Warren Buffettï¼‰
- æ³¨å†Œå…¬å¸ï¼ˆå¦‚ Apple Inc.ã€Goldman Sachsã€ä¸­å›½å·¥å•†é“¶è¡Œã€Volkswagen AGï¼‰
- æ”¿åºœæœºæ„æˆ–éƒ¨é—¨ï¼ˆå¦‚ ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šã€ä¸­å›½äººæ°‘é“¶è¡Œã€æ¬§ç›Ÿå§”å‘˜ä¼šã€æ—¥æœ¬é‡‘èå…ï¼‰
- ä¸»æƒå›½å®¶æˆ–æ˜ç¡®è¡Œæ”¿åŒºï¼ˆå¦‚ ç¾å›½ã€æ–°åŠ å¡ã€åŠ åˆ©ç¦å°¼äºšå·ã€é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºã€å¾·æ„å¿—è”é‚¦å…±å’Œå›½ï¼‰
- å›½é™…ç»„ç»‡ï¼ˆå¦‚ å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡ã€ä¸–ç•Œé“¶è¡Œã€è”åˆå›½ã€é‡‘èç¨³å®šç†äº‹ä¼šï¼‰
- é‡è¦äº§å“/å“ç‰Œ/å‹å·ï¼ˆç”±æ˜ç¡®ä¸»ä½“ç”Ÿäº§/æä¾›çš„å…·ä½“äº§å“æˆ–å“ç‰Œï¼Œå¦‚ iPhone 15 Proã€Tesla Model 3ã€ChatGPTã€Windows 11ã€Redmi 12Cï¼‰

âŒ ä»¥ä¸‹å†…å®¹**ä¸å¾—**è§†ä¸ºå®ä½“ï¼š
- æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚ â€œå¸‚åœºæ³¢åŠ¨â€ã€â€œç³»ç»Ÿæ€§é£é™©â€ã€â€œèµ„æœ¬æµåŠ¨â€ï¼‰
- æŠ€æœ¯æˆ–é‡‘èæœ¯è¯­ï¼ˆå¦‚ â€œæœŸæƒå®šä»·â€ã€â€œèµ„äº§è´Ÿå€ºè¡¨â€ã€â€œé‡åŒ–å®½æ¾â€ï¼‰
- é‡‘èå·¥å…·æˆ–èµ„äº§åç§°ï¼ˆå¦‚ â€œæ ‡æ™®500æŒ‡æ•°â€ã€â€œ10å¹´æœŸç¾å€ºâ€ã€â€œé»„é‡‘æœŸè´§â€ã€â€œBTCâ€ï¼‰â€”â€”é™¤éæŒ‡ä»£å…¶å‘è¡Œæ–¹ã€ç®¡ç†æ–¹æˆ–å…³è”æ³•äººï¼ˆå¦‚ â€œæ ‡æ™®é“ç¼æ–¯æŒ‡æ•°å…¬å¸â€ï¼‰
- æ³›ç§°ï¼ˆå¦‚ â€œæŠ•èµ„è€…â€ã€â€œç›‘ç®¡æœºæ„â€ã€â€œæŸé“¶è¡Œâ€ã€â€œå¤§å‹ç§‘æŠ€å…¬å¸â€ã€â€œæ™ºèƒ½æ‰‹æœºâ€ï¼‰
- æƒ…ç»ª/è¡Œæƒ…æè¿°ï¼ˆå¦‚ â€œæš´æ¶¨â€ã€â€œæŠ›å”®æ½®â€ã€â€œç»æµè¡°é€€æ‹…å¿§â€ï¼‰

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. åˆ¤æ–­æ–°é—»æ˜¯å¦åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªç‹¬ç«‹äº‹ä»¶ã€‚
2. å¯¹æ¯ä¸ªäº‹ä»¶ï¼Œè¾“å‡ºï¼š
   - ä¸€ä¸ªç®€æ´ã€å®¢è§‚ã€æ— æƒ…ç»ªçš„ä¸­æ–‡æ‘˜è¦ï¼ˆä½œä¸ºäº‹ä»¶å”¯ä¸€æ ‡è¯†ï¼‰
   - æ‰€æœ‰ç¬¦åˆä¸Šè¿°å®šä¹‰çš„å®ä½“ï¼ˆå…¨ç§°ä¼˜å…ˆï¼Œé¿å…ç¼©å†™ï¼›è‹¥åŸæ–‡ä½¿ç”¨è‹±æ–‡åä¸”æ— é€šç”¨ä¸­æ–‡è¯‘åï¼Œåˆ™ä¿ç•™è‹±æ–‡ï¼›äº§å“/å“ç‰Œåç§°ä¿ç•™åŸæ–‡æˆ–é€šç”¨è¯‘åï¼‰
   - æ‰€æœ‰ç¬¦åˆä¸Šè¿°å®šä¹‰çš„å®ä½“çš„åŸå§‹è¯­è¨€è¡¨è¿°ï¼ˆä¿ç•™æ–°é—»ä¸­å®ä½“çš„åŸå§‹è¯­è¨€å½¢å¼ï¼›åŸå§‹è¯­è¨€å®ä½“æ•°ç»„çš„ç´¢å¼•ä¸å®ä½“æ•°ç»„ç´¢å¼•ä¸€ä¸€å¯¹åº”ï¼‰
   - è¯¥äº‹ä»¶çš„æœ¬è´¨æè¿°ï¼ˆä¸€å¥è¯è¯´æ˜â€œè°å¯¹è°åšäº†ä»€ä¹ˆâ€ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡æœ¬ï¼š
{{
  "events": [
    {{
      "abstract": "ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šæ¨è¿Ÿå¯¹VanEckæ¯”ç‰¹å¸ETFç”³è¯·çš„å†³å®š",
      "entities": ["ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼š", "VanEck"],
      "entities_original": ["SEC", "VanEck"],
      "event_summary": "ç›‘ç®¡æœºæ„å»¶é•¿äº†å¯¹æŸèµ„äº§ç®¡ç†å…¬å¸æ¯”ç‰¹å¸ETFç”³è¯·çš„å®¡æŸ¥æœŸ"
    }}
  ]
}}

ã€æ–°é—»ã€‘
æ ‡é¢˜ï¼š{title}
æ­£æ–‡ï¼š{content}"""

    # è°ƒç”¨ API æ± 
    raw_content = API_POOL.call(
        prompt=prompt,
        max_tokens=1500,
        timeout=55,      # é¿å¼€ 60s ä»£ç†è¶…æ—¶
        retries=max_retries
    )

    if not raw_content:
        return []

    # æ¸…ç† Markdown åŒ…è£¹
    try:
        if raw_content.startswith("```json"):
            raw_content = raw_content.split("```json", 1)[1].split("```")[0]
        elif raw_content.startswith("```"):
            raw_content = raw_content.split("```", 1)[1].split("```")[0]

        data = json.loads(raw_content)
        events = data.get("events", [])
        result = []
        for item in events:
            abstract = item.get("abstract", "").strip()
            # ç¡®ä¿entitieså’Œentities_originalä¸€ä¸€å¯¹åº”ï¼Œä¸”éƒ½æœ‰æ•ˆ
            entities_raw = item.get("entities", [])
            entities_original_raw = item.get("entities_original", [])
            entities = []
            entities_original = []
            
            # éå†å¹¶è¿‡æ»¤ï¼Œç¡®ä¿ç´¢å¼•å¯¹åº”
            for ent, ent_original in zip(entities_raw, entities_original_raw):
                if tools.is_valid_entity(ent) and tools.is_valid_entity(ent_original):
                    entities.append(ent)
                    entities_original.append(ent_original)
            summary = item.get("event_summary", "").strip()
            if abstract and entities and summary:
                result.append({
                    "abstract": abstract,
                    "entities": entities,
                    "entities_original": entities_original,
                    "event_summary": summary
                })
        return result
    except Exception as e:
        tools.log(f"[LLMè·å–] âŒ LLM è¿”å›å†…å®¹è§£æå¤±è´¥: {e}")
        return []
    


# ======================
# ä¸»å¤„ç†æµç¨‹
# ======================

def get_unprocessed_news_files() -> List[Path]:
    """
    ä»…ä½¿ç”¨ tmp ç›®å½•çš„å»é‡ä¸å¤„ç†ã€‚
    tmp ç”¨äºæ–°æŠ“å–çš„å¾…å¤„ç†æ•°æ®ï¼Œå¤„ç†å®Œæˆåä¼šåˆ é™¤å¯¹åº” raw/dedupedã€‚
    """
    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r", encoding="utf-8", errors="ignore") as f:
            processed_ids = set(line.strip() for line in f if line.strip())
    
    unprocessed: List[Path] = []
    # åªå¤„ç† tmp/raw_news -> tmp/deduped_news
    raw_dir = tools.RAW_NEWS_TMP_DIR
    dedup_dir = tools.DEDUPED_NEWS_TMP_DIR
    raw_dir.mkdir(parents=True, exist_ok=True)
    dedup_dir.mkdir(parents=True, exist_ok=True)

    for raw_file in sorted(raw_dir.glob("*.jsonl")):
        deduped_file = dedup_dir / f"{raw_file.stem}_deduped.jsonl"
        if not deduped_file.exists():
            deduper = NewsDeduplicator(threshold=tools.DEDUPE_THRESHOLD)
            deduper.dedupe_file(raw_file, deduped_file, processed_ids)
        unprocessed.append(deduped_file)
    return unprocessed

def process_news_stream():
    tools.log(f"ğŸš€ å¯åŠ¨ Agent1ï¼šå¹¶å‘å®ä½“æå– | workers={MAX_WORKERS}, rate={RATE_LIMIT_PER_SEC}/s")
    files = get_unprocessed_news_files()
    if not files:
        tools.log("ğŸ“­ æ— å¯å¤„ç†æ–°é—»æ–‡ä»¶")
        return

    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r", encoding="utf-8", errors="ignore") as f:
            processed_ids = set(line.strip() for line in f if line.strip())

    limiter = RateLimiter(RATE_LIMIT_PER_SEC)
    total_processed = 0

    def build_published_at(ts: Optional[str]) -> Optional[str]:
        if not ts:
            return None
        try:
            return ts if isinstance(ts, str) else str(ts)
        except Exception:
            return None

    def extract_task(global_id: str, title: str, content: str, source: str, published_at: Optional[str]) -> Tuple[str, str, Optional[str], List[Dict]]:
        try:
            limiter.acquire()
            extracted = llm_extract_events(title, content)
            return global_id, source, published_at, extracted
        except Exception as e:
            tools.log(f"âš ï¸ ä»»åŠ¡ {global_id} æå–å¤±è´¥: {e}")
            return global_id, source, published_at, []

    with open(tools.PROCESSED_IDS_FILE, "a") as id_log:
        for file_path in files:
            tools.log(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            futures = []
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor, \
                 open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news = json.loads(line)
                        raw_id = str(news.get("id", "")).strip()
                        source = news.get("source", "unknown").strip().lower()

                        if not raw_id or not source:
                            tools.log("âš ï¸ è·³è¿‡æ—  ID æˆ–æ—  source çš„æ–°é—»")
                            continue

                        global_id = f"{source}:{raw_id}"
                        if global_id in processed_ids:
                            continue

                        title = news.get("title", "")
                        content = news.get("content", "")
                        MAX_CONTENT_CHARS = 2000
                        if isinstance(content, str) and len(content) > MAX_CONTENT_CHARS:
                            content = content[:MAX_CONTENT_CHARS] + "â€¦â€¦ã€åæ–‡å·²æˆªæ–­ã€‘"

                        published_at = build_published_at(news.get("timestamp"))
                        futures.append(
                            executor.submit(
                                extract_task,
                                global_id,
                                title,
                                content,
                                source,
                                published_at
                            )
                        )
                    except Exception as e:
                        tools.log(f"âš ï¸ è§£ææ–°é—»è¡Œå¤±è´¥: {e}")

                for fut in as_completed(futures):
                    try:
                        global_id, source, published_at, extracted = fut.result()
                        if not extracted:
                            tools.log(f"â³ æ–°é—» {global_id}ï¼šLLM æœªè¿”å›æœ‰æ•ˆäº‹ä»¶ï¼Œä¿ç•™é‡è¯•æœºä¼š")
                            continue

                        all_entities = []
                        all_entities_original = []
                        for ev in extracted:
                            all_entities.extend(ev["entities"])
                            all_entities_original.extend(ev["entities_original"])

                        if all_entities and len(all_entities) == len(all_entities_original):
                            update_entities(all_entities, all_entities_original, source, published_at)
                            update_abstract_map(extracted, source, published_at)
                            total_processed += 1
                            id_log.write(global_id + "\n")
                            processed_ids.add(global_id)
                        else:
                            tools.log(f"ğŸ” æ–°é—» {global_id}ï¼šLLM è¿”å›äº‹ä»¶ä½†æ— æœ‰æ•ˆå®ä½“ï¼Œæš‚ä¸æ ‡è®°")
                    except Exception as e:
                        tools.log(f"âš ï¸ å¤„ç†æå–ç»“æœå¤±è´¥: {e}")

            try:
                # ä»…å¤„ç† tmp ç›®å½•ä¸‹çš„ raw/deduped æ–‡ä»¶
                raw_dir = tools.RAW_NEWS_TMP_DIR
                raw_file_name = file_path.stem.replace("_deduped", "") + ".jsonl"
                raw_file_path = raw_dir / raw_file_name
                if raw_file_path.exists():
                    raw_file_path.unlink()
                    tools.log(f"ğŸ—‘ï¸ åˆ é™¤åŸå§‹æ–°é—»æ–‡ä»¶: {raw_file_path}")
                if file_path.exists():
                    file_path.unlink()
                    tools.log(f"ğŸ—‘ï¸ åˆ é™¤å»é‡æ–°é—»æ–‡ä»¶: {file_path}")
            except Exception as e:
                tools.log(f"âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")

    tools.log(f"âœ… å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡å«æœ‰æ•ˆå®ä½“çš„æ–°é—»")
    
    # # åœ¨æ‰€æœ‰æ–°é—»å¤„ç†å®Œæˆåç»Ÿä¸€åˆ·æ–°çŸ¥è¯†å›¾è°±
    # if total_processed > 0:
    #     try:
    #         with tools._refresh_lock:
    #             threading.Thread(target=refresh_graph, daemon=True).start()
    #             tools.log("ğŸ”„ å·²å¯åŠ¨çŸ¥è¯†å›¾è°±åˆ·æ–°çº¿ç¨‹")
    #     except Exception as e:
    #         tools.log(f"âš ï¸ å¯åŠ¨çŸ¥è¯†å›¾è°±åˆ·æ–°å¤±è´¥: {e}")
    # else:
    #     tools.log("ğŸ“­ æœªå¤„ç†ä»»ä½•æ–°é—»ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±åˆ·æ–°")


# ======================
# å…¥å£
# ======================

if __name__ == "__main__":
    process_news_stream()