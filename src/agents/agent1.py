# src/agents/agent1.py
"""
æ™ºèƒ½ä½“1ï¼šæ–°é—»å…³è”è¯ä¸äº‹ä»¶ç±»å‹æå–å™¨ï¼ˆæ”¯æŒäººå·¥å®¡æ ¸å¼€å…³ï¼‰
"""

import pandas as pd
import json
from pathlib import Path
from typing import List, Dict, Optional, Set
import re
import warnings
from datetime import datetime, timezone
warnings.filterwarnings("ignore", category=FutureWarning)

DATA_DIR = Path(__file__).parent.parent.parent / "data"
ENTITIES_FILE = DATA_DIR / "crypto_entities.json"
PENDING_ENTITIES_FILE = DATA_DIR / "pending_entities.json"
STOP_WORDS_FILE = DATA_DIR / "stop_words.txt"

def load_stop_words() -> Set[str]:
    """ä»ç»Ÿä¸€åœç”¨è¯æ–‡ä»¶åŠ è½½ï¼ˆæ”¯æŒæ³¨é‡Šå’Œç©ºè¡Œï¼‰"""
    stop_words = set()
    if STOP_WORDS_FILE.exists():
        with open(STOP_WORDS_FILE, "r", encoding="utf-8") as f:
            for line in f:
                word = line.strip()
                if word and not word.startswith("#"):
                    stop_words.add(word)
    return stop_words

STOP_WORDS = load_stop_words()

def is_valid_candidate(entity: str) -> bool:
    """åˆ¤æ–­ä¸€ä¸ªå€™é€‰è¯æ˜¯å¦å€¼å¾—è¿›å…¥å¾…å®¡æ ¸æ± """
    word = entity.strip()
    if not word:
        return False
    if len(word) == 1:
        return False
    if word in STOP_WORDS:
        return False
    if word.isdigit():
        return False
    if re.match(r'^[0-9+\-\.%]+$', word):  # çº¯æ•°å­—/ç¬¦å·
        return False
    # æ’é™¤çº¯æ ‡ç‚¹æˆ–ç‰¹æ®Šå­—ç¬¦
    if not any(c.isalnum() for c in word):
        return False
    return True

EVENT_KEYWORDS = {
    "regulation": ["ç›‘ç®¡", "åˆè§„", "SEC", "ç½šæ¬¾", "ç¦ä»¤", "ç‰Œç…§", "æ³•å¾‹"],
    "hack": ["é»‘å®¢", "è¢«ç›—", "æ¼æ´", "æ”»å‡»", "å®‰å…¨äº‹ä»¶"],
    "listing": ["ä¸Šçº¿", "ä¸Šæ¶", "äº¤æ˜“å¯¹", "æ”¯æŒ"],
    "partnership": ["åˆä½œ", "æˆ˜ç•¥åˆä½œ", "è”ç›Ÿ", "é›†æˆ"],
    "upgrade": ["å‡çº§", "ä¸»ç½‘", "ç¡¬åˆ†å‰", "æŠ€æœ¯æ›´æ–°"],
    "market": ["æš´è·Œ", "æš´æ¶¨", "è¡Œæƒ…", "å¸‚å€¼", "ä»·æ ¼"],
    "adoption": ["é‡‡ç”¨", "æ”¯ä»˜", "é›†æˆåˆ°", "ä¼ä¸šé‡‡ç”¨"]
}

def load_crypto_entities() -> Set[str]:
    if not ENTITIES_FILE.exists():
        default_data = {
            "crypto_assets": ["BTC", "ETH", "SOL", "USDT", "æ¯”ç‰¹å¸", "ä»¥å¤ªåŠ"],
            "organizations": ["Binance", "SEC", "ç¾è”å‚¨"],
            "concepts": ["ETF", "å‡åŠ", "DeFi"]
        }
        DATA_DIR.mkdir(exist_ok=True)
        with open(ENTITIES_FILE, "w", encoding="utf-8") as f:
            json.dump(default_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ†• é¦–æ¬¡è¿è¡Œï¼šå·²åˆ›å»ºé»˜è®¤å®ä½“åº“ {ENTITIES_FILE}")

    with open(ENTITIES_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    all_entities = set()
    for group in data.values():
        all_entities.update(group)
    return all_entities

def save_pending_entities(candidates: Set[str]):
    """ä¿å­˜é€šè¿‡åˆç­›çš„å€™é€‰å®ä½“"""
    if not candidates:
        return

    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    if PENDING_ENTITIES_FILE.exists():
        with open(PENDING_ENTITIES_FILE, "r", encoding="utf-8") as f:
            pending = json.load(f)
    else:
        pending = {}

    now = datetime.now(timezone.utc).isoformat()
    for ent in candidates:
        if ent not in pending:
            pending[ent] = {
                "first_seen": now,
                "status": "pending",
                "source_contexts": []
            }

    with open(PENDING_ENTITIES_FILE, "w", encoding="utf-8") as f:
        json.dump(pending, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ åˆç­›åæ–°å¢ {len(candidates)} ä¸ªå€™é€‰å®ä½“åˆ°å¾…å®¡æ ¸æ–‡ä»¶")

def approve_pending_entities():
    """
    äº¤äº’å¼å®¡æ ¸å¾…æ‰¹å‡†å®ä½“ã€‚
    æ”¯æŒï¼š
      1. åŠ å…¥åœç”¨è¯åº“ï¼ˆè¿½åŠ åˆ° stop_words.txtï¼‰
      2. åŠ å…¥å·²æœ‰åˆ†ç±»ï¼ˆç¼–å·é€‰æ‹©ï¼‰
      3. åˆ›å»ºæ–°åˆ†ç±»
    """
    if not PENDING_ENTITIES_FILE.exists():
        print("ğŸ“­ å¾…å®¡æ ¸æ–‡ä»¶ä¸å­˜åœ¨")
        return

    with open(PENDING_ENTITIES_FILE, "r", encoding="utf-8") as f:
        pending = json.load(f)

    pending_entities = {
        ent: info for ent, info in pending.items()
        if info.get("status") == "pending"
    }

    if not pending_entities:
        print("âœ… æ‰€æœ‰å¾…å®¡å®ä½“å·²å¤„ç†å®Œæ¯•ï¼")
        return

    # åŠ è½½ä¸»çŸ¥è¯†åº“
    with open(ENTITIES_FILE, "r", encoding="utf-8") as f:
        main_data = json.load(f)

    categories = list(main_data.keys())
    total = len(pending_entities)

    # åŠ è½½å½“å‰åœç”¨è¯ï¼ˆç”¨äºå»é‡ï¼‰
    current_stop_words = load_stop_words()
    new_stop_words_to_add = set()
    approved_updates = {}  # entity -> target_category or "__STOP__"

    for i, (entity, info) in enumerate(pending_entities.items(), 1):
        print(f"\n{'='*50}")
        print(f"ğŸ” [{i}/{total}] å®¡æ ¸å®ä½“: '{entity}'")
        print("[1] åŠ å…¥åœç”¨è¯åº“ï¼ˆæ°¸ä¹…å¿½ç•¥ï¼‰")
        print("[2] åŠ å…¥å·²æœ‰åˆ†ç±»")
        print("[3] åˆ›å»ºæ–°åˆ†ç±»")
        print("[q] é€€å‡ºå®¡æ ¸")

        choice = input("è¯·é€‰æ‹© (1/2/3/q): ").strip().lower()
        if choice == 'q':
            print("â¹ï¸ å®¡æ ¸å·²é€€å‡ºã€‚")
            break
        elif choice == '1':
            if entity in current_stop_words:
                print(f"â„¹ï¸ '{entity}' å·²åœ¨åœç”¨è¯åº“ä¸­")
            else:
                new_stop_words_to_add.add(entity)
                approved_updates[entity] = "__STOP__"
                print(f"ğŸ—‘ï¸ '{entity}' å°†è¢«åŠ å…¥åœç”¨è¯åº“")
        elif choice == '2':
            print("\nå·²æœ‰åˆ†ç±»:")
            for idx, cat in enumerate(categories, 1):
                print(f"  [{idx}] {cat}")
            while True:
                try:
                    sel = input("è¯·é€‰æ‹©ç¼–å·: ").strip()
                    if not sel:
                        continue
                    idx = int(sel) - 1
                    if 0 <= idx < len(categories):
                        target_cat = categories[idx]
                        approved_updates[entity] = target_cat
                        print(f"âœ… '{entity}' å°†åŠ å…¥åˆ†ç±» '{target_cat}'")
                        break
                    else:
                        print("âš ï¸ ç¼–å·è¶…å‡ºèŒƒå›´ï¼Œè¯·é‡è¯•")
                except ValueError:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        elif choice == '3':
            while True:
                new_cat = input("è¯·è¾“å…¥æ–°åˆ†ç±»åï¼ˆå¦‚ protocolsï¼‰: ").strip()
                if new_cat and re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', new_cat):
                    if new_cat not in main_data:
                        main_data[new_cat] = []
                        categories.append(new_cat)
                    approved_updates[entity] = new_cat
                    print(f"ğŸ†• åˆ›å»ºæ–°åˆ†ç±» '{new_cat}' å¹¶æ·»åŠ  '{entity}'")
                    break
                else:
                    print("âš ï¸ åˆ†ç±»åéœ€ä¸ºåˆæ³• Python æ ‡è¯†ç¬¦ï¼ˆå­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼Œä¸èƒ½ä»¥æ•°å­—å¼€å¤´ï¼‰")
        else:
            print("âš ï¸ æ— æ•ˆé€‰é¡¹ï¼Œè·³è¿‡æ­¤å®ä½“")
            continue

    if not approved_updates:
        print("\nâ„¹ï¸ æœªåšä»»ä½•ä¿®æ”¹")
        return

    # --- æ›´æ–°ä¸»çŸ¥è¯†åº“ï¼ˆéåœç”¨è¯é¡¹ï¼‰---
    for entity, target in approved_updates.items():
        if target != "__STOP__":
            if entity not in main_data[target]:
                main_data[target].append(entity)

    # å»é‡ + æ’åº
    for key in main_data:
        main_data[key] = sorted(list(set(main_data[key])))

    with open(ENTITIES_FILE, "w", encoding="utf-8") as f:
        json.dump(main_data, f, ensure_ascii=False, indent=2)

    # --- è¿½åŠ æ–°åœç”¨è¯åˆ°æ–‡ä»¶ ---
    if new_stop_words_to_add:
        with open(STOP_WORDS_FILE, "a", encoding="utf-8") as f:
            for word in sorted(new_stop_words_to_add):
                f.write("\n" + word)
        print(f"\nğŸ’¾ å·²å°† {len(new_stop_words_to_add)} ä¸ªæ–°è¯è¿½åŠ åˆ°åœç”¨è¯åº“: {STOP_WORDS_FILE}")

    # --- æ¸…ç† pending æ–‡ä»¶ ---
    remaining = {ent: info for ent, info in pending.items() if ent not in approved_updates}
    with open(PENDING_ENTITIES_FILE, "w", encoding="utf-8") as f:
        json.dump(remaining, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å®¡æ ¸å®Œæˆï¼å…±å¤„ç† {len(approved_updates)} ä¸ªå®ä½“ã€‚")
    print(f"   - ä¸»çŸ¥è¯†åº“å·²æ›´æ–°: {ENTITIES_FILE}")
    print(f"   - å¾…å®¡æ–‡ä»¶å‰©ä½™ {len(remaining)} é¡¹")

def extract_entities_from_text(text: str, known_entities: Set[str]) -> List[str]:
    if not isinstance(text, str):
        return []
    found = set()
    for entity in known_entities:
        if entity in text:
            found.add(entity)
    return sorted(found)

def classify_event_type(title: str, content: str) -> Optional[str]:
    full_text = (title + " " + content) if isinstance(content, str) else title
    if not isinstance(full_text, str):
        return None
    scores = {}
    for event_type, keywords in EVENT_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw in full_text)
        if score > 0:
            scores[event_type] = score
    return max(scores, key=scores.get) if scores else None

class Agent1EntityExtractor:
    def __init__(self, auto_update: bool = False):
        """
        :param auto_update: æ˜¯å¦è‡ªåŠ¨å°†æ–°å®ä½“å†™å…¥ä¸»çŸ¥è¯†åº“ï¼ˆå¦åˆ™å†™å…¥ pending æ–‡ä»¶ï¼‰
        """
        self.auto_update = auto_update
        self.known_entities = load_crypto_entities()

    def discover_new_entities(self, df: pd.DataFrame, min_freq: int = 2) -> Set[str]:
        from collections import Counter
        candidate_counter = Counter()

        for _, row in df.iterrows():
            text = f"{row['title']} {row.get('content', '')}"
            if not isinstance(text, str):
                continue

            # $WIF
            for match in re.findall(r'\$[A-Za-z0-9]{2,10}', text):
                candidate_counter[match.upper().lstrip('$')] += 1

            # å¤§å†™ä»£å¸ç¬¦å·
            for match in re.findall(r'\b[A-Z]{3,6}\b', text):
                if match not in {"USD", "API", "NFT", "ETF", "SEC", "OKX"}:
                    candidate_counter[match] += 1

            # ä¸­æ–‡é¡¹ç›®å
            for match in re.findall(r'[\u4e00-\u9fa5]{2,4}', text):
                candidate_counter[match] += 1

        valid_candidates = {
            ent for ent, cnt in candidate_counter.items()
            if cnt >= min_freq and ent not in self.known_entities and is_valid_candidate(ent)
        }
        return valid_candidates

    def process(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty:
            return df

        required_cols = ['title', 'content']
        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"è¾“å…¥DataFrameç¼ºå°‘å¿…è¦åˆ—: {col}")

        result_df = df.copy()

        result_df['entities'] = result_df.apply(
            lambda row: extract_entities_from_text(
                str(row['title']) + " " + str(row.get('content', '')),
                self.known_entities
            ),
            axis=1
        )

        result_df['event_type'] = result_df.apply(
            lambda row: classify_event_type(
                str(row['title']), str(row.get('content', ''))
            ),
            axis=1
        )

        # ğŸ”‘ å…³é”®ï¼šæ ¹æ® auto_update å†³å®šå¦‚ä½•å¤„ç†æ–°å®ä½“
        new_entities = self.discover_new_entities(df, min_freq=2)
        if new_entities:
            if self.auto_update:
                self._save_entities_to_main(new_entities)
            else:
                save_pending_entities(new_entities)

        print(f"ğŸ§  æ™ºèƒ½ä½“1å¤„ç†å®Œæˆï¼šå…±å¤„ç† {len(result_df)} æ¡æ–°é—»")
        print(f"   - å¹³å‡æ¯æ¡æ–°é—»æå– {result_df['entities'].apply(len).mean():.1f} ä¸ªå®ä½“")
        print(f"   - è¯†åˆ«å‡º {result_df['event_type'].notna().sum()} æ¡å¸¦äº‹ä»¶ç±»å‹çš„æ–°é—»")
        
        return result_df

    def _save_entities_to_main(self, new_entities: Set[str]):
        """å°†æ–°å®ä½“åˆå¹¶åˆ°ä¸»çŸ¥è¯†åº“ï¼ˆä»…å½“ auto_update=True æ—¶è°ƒç”¨ï¼‰"""
        with open(ENTITIES_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        for ent in new_entities:
            if ent.isupper() and len(ent) <= 6:
                data["crypto_assets"].append(ent)
            elif any(kw in ent for kw in ["äº¤æ˜“æ‰€", "å¸", "Coin"]):
                data["crypto_assets"].append(ent)
            else:
                data["concepts"].append(ent)

        for key in data:
            data[key] = sorted(list(set(data[key])))

        with open(ENTITIES_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ… è‡ªåŠ¨æ–°å¢ {len(new_entities)} ä¸ªå®ä½“åˆ°ä¸»çŸ¥è¯†åº“: {sorted(new_entities)}")

if __name__ == "__main__":
    approve_pending_entities()