# src/agents/agent1.py
"""
æ™ºèƒ½ä½“1ï¼šæµå¼æ–°é—»å»é‡ + LLMé©±åŠ¨çš„çœŸå®ä¸–ç•Œå®ä½“ä¸äº‹ä»¶æå–å™¨

æ ¸å¿ƒåŸåˆ™ï¼š
- å®ä½“ = èƒ½ç­¾ç½²åˆåŒã€è¢«èµ·è¯‰ã€å‘å¸ƒå…¬å‘Šã€æ‹¥æœ‰é“¶è¡Œè´¦æˆ·çš„ä¸»ä½“
  ï¼ˆè‡ªç„¶äººã€å…¬å¸ã€æ”¿åºœæœºæ„ã€å›½å®¶ã€åœ°åŒºã€å›½é™…ç»„ç»‡ï¼‰
- æ’é™¤ï¼šä»£å¸åç§°ã€æŠ€æœ¯æœ¯è¯­ã€æŠ½è±¡æ¦‚å¿µã€æƒ…ç»ªè¯ã€æ³›ç§°
- æå–å³è‡ªåŠ¨å†™å…¥ entities.jsonï¼Œæ— éœ€äººå·¥å®¡æ ¸
- æ¯ä¸ªäº‹ä»¶ç”Ÿæˆå”¯ä¸€æ‘˜è¦ï¼Œå¹¶å…³è”å®ä½“ä¸äº‹ä»¶æè¿°
- è‡ªåŠ¨æ›´æ–°çŸ¥è¯†å›¾è°±ï¼Œç»´æŠ¤å®ä½“-äº‹ä»¶å…³ç³»ç½‘ç»œ
"""

import os
import sys
import json
import re
import time
import hashlib
from pathlib import Path
from typing import List, Dict, Set, Optional
from datetime import datetime, timezone
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

from dotenv import load_dotenv
from ..utils.tool_function import tools
tools = tools()
from .api_client import LLMAPIPool
from .kg_interface import refresh_graph  # å¯¼å…¥çŸ¥è¯†å›¾è°±åˆ·æ–°åŠŸèƒ½
API_POOL = None

def init_api_pool():
    global API_POOL
    if API_POOL is None:
        API_POOL = LLMAPIPool()


# ======================
# æ–°é—»å»é‡å™¨
# ======================

class NewsDeduplicator:
    def __init__(self, threshold: int = 3):
        self.threshold = threshold
        self.seen_hashes: Set[int] = set()

    @staticmethod
    def _news_key(news: Dict) -> str:
        """æ„é€ ç”¨äºå»é‡çš„å”¯ä¸€é”®ï¼ŒåŒ…å« source å‰ç¼€ï¼Œå…¼å®¹å¤šæ•°æ®æºã€‚"""
        return f"{news.get('source', 'unknown')}:{news.get('id')}"

    def is_duplicate(self, text: str) -> bool:
        h = tools.simhash(text)
        for seen_h in self.seen_hashes:
            if tools.hamming_distance(h, seen_h) <= self.threshold:
                return True
        self.seen_hashes.add(h)
        return False

    def dedupe_file(self, input_path: Path, output_path: Path, processed_ids: Optional[Set[str]] = None):
        """
        å¯¹å•ä¸ªåŸå§‹æ–‡ä»¶åšå»é‡ï¼š
        - å…ˆç”¨ processed_idsï¼ˆå…¨å±€å·²å¤„ç† IDï¼Œå¦‚ blockbeats:323066ï¼‰è¿‡æ»¤å†å²å·²å¤„ç†æ–°é—»
        - å†ç»“åˆå·²æœ‰å»é‡æ–‡ä»¶ & simhash å»æ‰æœ¬æ‰¹å†…/è·¨æ‰¹çš„é‡å¤å†…å®¹
        """
        tools.log(f"ğŸ” å»é‡ä¸­: {input_path.name}")

        # å…ˆåŠ è½½â€œå…¨å±€å·²å¤„ç† IDâ€ï¼Œé¿å…è€æ–°é—»å†æ¬¡è¿›å…¥å»é‡ç»“æœ
        seen_ids: Set[str] = set(processed_ids or set())
        if processed_ids:
            tools.log(f"ğŸ” å·²æœ‰å†å² processed_ids æ•°é‡: {len(processed_ids)}")

        # å†åŠ è½½å·²æœ‰å»é‡ç»“æœæ–‡ä»¶ä¸­çš„ IDï¼Œå®ç°è·¨æ‰¹æ¬¡çš„æœ¬åœ°å»é‡
        if output_path.exists():
            with open(output_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        item = json.loads(line)
                        seen_ids.add(self._news_key(item))
                    except Exception as e:
                        tools.log(f"âš ï¸ è¯»å–å†å²å»é‡æ–‡ä»¶æ—¶è·³è¿‡æ— æ•ˆè¡Œ: {e}")
        
        kept, skipped_id, skipped_sim = 0, 0, 0
        with open(input_path, "r", encoding="utf-8") as fin, \
             open(output_path, "a", encoding="utf-8") as fout:
            for line in fin:
                try:
                    news = json.loads(line)
                    key = self._news_key(news)

                    # 1) æŒ‰å…¨å±€ ID å»é‡ï¼ˆåŒ…æ‹¬ processed_ids å’Œå·²æœ‰å»é‡æ–‡ä»¶ä¸­çš„ IDï¼‰
                    if key in seen_ids:
                        skipped_id += 1
                        continue

                    # 2) æ„é€ æ–‡æœ¬ï¼ŒæŒ‰å†…å®¹ç›¸ä¼¼åº¦å»é‡
                    raw_text = (news.get("title", "") + " " + news.get("content", "")).strip()
                    if not raw_text:
                        continue
                    if self.is_duplicate(raw_text):
                        skipped_sim += 1
                        continue
                    fout.write(line)
                    seen_ids.add(key)
                    kept += 1
                except Exception as e:
                    tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
        tools.log(f"âœ… å»é‡å®Œæˆ: ä¿ç•™ {kept} æ¡, æŒ‰ ID è·³è¿‡ {skipped_id} æ¡, æŒ‰ç›¸ä¼¼åº¦è·³è¿‡ {skipped_sim} æ¡")

# ======================
# LLM ç»“æ„åŒ–æå–å™¨ï¼ˆå«ç²¾å‡†æç¤ºè¯ï¼‰
# ======================

def llm_extract_events(title: str, content: str, max_retries=2) -> List[Dict]:
    # åˆå§‹åŒ– API æ± ï¼ˆå•ä¾‹ï¼‰
    init_api_pool()
    if API_POOL is None:
        tools.log("[LLMè¯·æ±‚] âŒ API æ± æœªåˆå§‹åŒ–")
        return []

    prompt = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡‘èä¸æ³•å¾‹ä¿¡æ¯ç»“æ„åŒ–ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–°é—»ä¸­æå–æ‰€æœ‰**çœŸå®å­˜åœ¨çš„ã€å…·æœ‰æ³•å¾‹äººæ ¼æˆ–è¡Œæ”¿èŒèƒ½çš„å®ä½“**ã€‚

ã€å®ä½“å®šä¹‰ã€‘
âœ… å¿…é¡»æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
- æ˜¯è‡ªç„¶äººï¼ˆå¦‚ Elon Muskã€Cathie Woodï¼‰
- æ˜¯æ³¨å†Œå…¬å¸ï¼ˆå¦‚ Binanceã€Coinbaseã€Teslaï¼‰
- æ˜¯æ”¿åºœæœºæ„æˆ–éƒ¨é—¨ï¼ˆå¦‚ ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šã€ä¸­å›½äººæ°‘é“¶è¡Œã€æ¬§ç›Ÿå§”å‘˜ä¼šï¼‰
- æ˜¯ä¸»æƒå›½å®¶æˆ–æ˜ç¡®è¡Œæ”¿åŒºï¼ˆå¦‚ ç¾å›½ã€æ–°åŠ å¡ã€åŠ åˆ©ç¦å°¼äºšå·ã€é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºï¼‰
- æ˜¯å›½é™…ç»„ç»‡ï¼ˆå¦‚ å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡ã€è”åˆå›½ï¼‰

âŒ ä»¥ä¸‹å†…å®¹**ä¸å¾—**è§†ä¸ºå®ä½“ï¼š
- æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚ â€œå»ä¸­å¿ƒåŒ–â€ã€â€œæµåŠ¨æ€§â€ã€â€œå¸‚åœºæƒ…ç»ªâ€ï¼‰
- æŠ€æœ¯æœ¯è¯­ï¼ˆå¦‚ â€œæ™ºèƒ½åˆçº¦â€ã€â€œé›¶çŸ¥è¯†è¯æ˜â€ã€â€œPoSâ€ï¼‰
- ä»£å¸/èµ„äº§åç§°ï¼ˆå¦‚ â€œBTCâ€ã€â€œä»¥å¤ªåŠâ€ã€â€œSolanaâ€ï¼‰â€”â€”é™¤éæŒ‡ä»£å…¶åŸºé‡‘ä¼šæˆ–å¼€å‘å…¬å¸ï¼ˆå¦‚ â€œä»¥å¤ªåŠåŸºé‡‘ä¼šâ€ï¼‰
- æ³›ç§°ï¼ˆå¦‚ â€œæŠ•èµ„è€…â€ã€â€œç›‘ç®¡æœºæ„â€ã€â€œæŸäº¤æ˜“æ‰€â€ï¼‰
- æƒ…ç»ª/è¡Œæƒ…æè¿°ï¼ˆå¦‚ â€œç‰›å¸‚â€ã€â€œæš´è·Œâ€ã€â€œåˆ©å¥½â€ï¼‰

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. åˆ¤æ–­æ–°é—»æ˜¯å¦åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªç‹¬ç«‹äº‹ä»¶ã€‚
2. å¯¹æ¯ä¸ªäº‹ä»¶ï¼Œè¾“å‡ºï¼š
   - ä¸€ä¸ªç®€æ´ã€å®¢è§‚ã€æ— æƒ…ç»ªçš„ä¸­æ–‡æ‘˜è¦ï¼ˆä½œä¸ºäº‹ä»¶å”¯ä¸€æ ‡è¯†ï¼‰
   - æ‰€æœ‰ç¬¦åˆä¸Šè¿°å®šä¹‰çš„å®ä½“ï¼ˆå…¨ç§°ä¼˜å…ˆï¼Œé¿å…ç¼©å†™ï¼Œä½¿ç”¨ä¸­æ–‡æˆ–è‹±æ–‡è¡¨è¿°ï¼‰
   - æ‰€æœ‰ç¬¦åˆä¸Šè¿°å®šä¹‰çš„å®ä½“çš„åŸå§‹è¯­è¨€è¡¨è¿°ï¼ˆä¿ç•™æ–°é—»ä¸­å®ä½“çš„åŸå§‹è¯­è¨€å½¢å¼ï¼ŒåŸå§‹è¯­è¨€å®ä½“æ•°ç»„çš„ç´¢å¼•ä¸å®ä½“æ•°ç»„ç´¢å¼•å¯¹åº”ï¼‰
   - è¯¥äº‹ä»¶çš„æœ¬è´¨æè¿°ï¼ˆä¸€å¥è¯è¯´æ˜â€œè°å¯¹è°åšäº†ä»€ä¹ˆâ€ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼Œä¸è¦ä»»ä½•é¢å¤–æ–‡æœ¬ï¼š
{{
  "events": [
    {{
      "abstract": "ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šæ¨è¿Ÿå¯¹æ¯”ç‰¹å¸ETFçš„æœ€ç»ˆå†³å®š",
      "entities": ["ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼š", "VanEck"],
      "entities_original": ["ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼š", "VanEck"],
      "event_summary": "ç›‘ç®¡æœºæ„å»¶é•¿äº†å¯¹æŸèµ„äº§ç®¡ç†å…¬å¸æ¯”ç‰¹å¸ETFç”³è¯·çš„å®¡æŸ¥æœŸ"
    }}
  ]
}}

ã€æ–°é—»ã€‘
æ ‡é¢˜ï¼š{title}
æ­£æ–‡ï¼š{content}"""

    # è°ƒç”¨ API æ± 
    raw_content = API_POOL.call(
        prompt=prompt,
        max_tokens=1500,
        timeout=55,      # é¿å¼€ 60s ä»£ç†è¶…æ—¶
        retries=max_retries
    )

    if not raw_content:
        return []

    # æ¸…ç† Markdown åŒ…è£¹
    try:
        if raw_content.startswith("```json"):
            raw_content = raw_content.split("```json", 1)[1].split("```")[0]
        elif raw_content.startswith("```"):
            raw_content = raw_content.split("```", 1)[1].split("```")[0]

        data = json.loads(raw_content)
        events = data.get("events", [])
        result = []
        for item in events:
            abstract = item.get("abstract", "").strip()
            # ç¡®ä¿entitieså’Œentities_originalä¸€ä¸€å¯¹åº”ï¼Œä¸”éƒ½æœ‰æ•ˆ
            entities_raw = item.get("entities", [])
            entities_original_raw = item.get("entities_original", [])
            entities = []
            entities_original = []
            
            # éå†å¹¶è¿‡æ»¤ï¼Œç¡®ä¿ç´¢å¼•å¯¹åº”
            for ent, ent_original in zip(entities_raw, entities_original_raw):
                if tools.is_valid_entity(ent) and tools.is_valid_entity(ent_original):
                    entities.append(ent)
                    entities_original.append(ent_original)
            summary = item.get("event_summary", "").strip()
            if abstract and entities and summary:
                result.append({
                    "abstract": abstract,
                    "entities": entities,
                    "entities_original": entities_original,
                    "event_summary": summary
                })
        return result
    except Exception as e:
        tools.log(f"[LLMè·å–] âŒ LLM è¿”å›å†…å®¹è§£æå¤±è´¥: {e}")
        return []
    
# ======================
# è‡ªåŠ¨æ›´æ–°çŸ¥è¯†åº“
# ======================

def update_entities(entities: List[str], entities_original: List[str], source: str, published_at: Optional[str] = None):
    """è‡ªåŠ¨å†™å…¥ä¸»å®ä½“åº“

    æ—¶é—´åˆ»ä½¿ç”¨æ–°é—»çš„å‘å¸ƒæ—¶é—´ï¼ˆè‹¥æä¾›ï¼‰ï¼Œå¦åˆ™å›é€€åˆ°å½“å‰æ—¶é—´ã€‚
    æ”¯æŒå®ä½“çš„åŸå§‹è¯­è¨€è¡¨è¿°ï¼Œentitieså’Œentities_originalæ•°ç»„çš„ç´¢å¼•å¯¹åº”ã€‚
    """
    now = datetime.now(timezone.utc).isoformat()
    # å¦‚æœæä¾›äº†å‘å¸ƒæ—¶é—´ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨è¯¥æ—¶é—´ï¼›å¦åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
    base_ts = published_at or now
    existing = {}
    if tools.ENTITIES_FILE.exists():
        with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
            existing = json.load(f)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    needs_update = False
    
    # ç¡®ä¿ä¸¤ä¸ªæ•°ç»„é•¿åº¦ä¸€è‡´
    for ent, ent_original in zip(entities, entities_original):
        if ent not in existing:
            existing[ent] = {
                "first_seen": base_ts,
                "sources": [source],
                "original_forms": [ent_original]  # æ–°å¢ï¼šä¿å­˜åŸå§‹è¯­è¨€è¡¨è¿°
            }
            needs_update = True
        else:
            # å¦‚æœå·²æœ‰ first_seenï¼Œä¸”æ–°é—»æ—¶é—´æ›´æ—©ï¼Œåˆ™æ›´æ–°ä¸ºæ›´æ—©çš„æ—¶é—´
            try:
                old_ts = existing[ent].get("first_seen")
                if old_ts and base_ts and base_ts < old_ts:
                    existing[ent]["first_seen"] = base_ts
                    needs_update = True
            except Exception:
                # å¼‚å¸¸æ—¶ä¸å¼ºåˆ¶æ›´æ–°ï¼Œé¿å…ç ´åå·²æœ‰æ•°æ®
                pass

            if source not in existing[ent]["sources"]:
                existing[ent]["sources"].append(source)
                needs_update = True
            
            # æ›´æ–°åŸå§‹è¯­è¨€è¡¨è¿°ï¼ˆå»é‡ï¼‰
            if "original_forms" not in existing[ent]:
                existing[ent]["original_forms"] = []
            if ent_original not in existing[ent]["original_forms"]:
                existing[ent]["original_forms"].append(ent_original)
                needs_update = True
    
    if needs_update:
        with open(tools.ENTITIES_FILE, "w", encoding="utf-8") as f:
            json.dump(existing, f, ensure_ascii=False, indent=2)
        
        # å¼‚æ­¥æ›´æ–°çŸ¥è¯†å›¾è°±ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        try:
            import threading
            threading.Thread(target=refresh_graph, daemon=True).start()
        except Exception as e:
            tools.log(f"âš ï¸ å¼‚æ­¥æ›´æ–°çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")

def update_abstract_map(extracted_list: List[Dict], source: str, published_at: Optional[str] = None):
    abstract_map = {}
    if tools.ABSTRACT_MAP_FILE.exists():
        with open(tools.ABSTRACT_MAP_FILE, "r", encoding="utf-8") as f:
            abstract_map = json.load(f)
    
    now = datetime.now(timezone.utc).isoformat()
    base_ts = published_at or now
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    needs_update = False
    for item in extracted_list:
        key = item["abstract"]
        if key not in abstract_map:
            abstract_map[key] = {
                "entities": item["entities"],
                "event_summary": item["event_summary"],
                "sources": [source],
                "first_seen": base_ts
            }
            needs_update = True
        else:
            # first_seen å–æœ€æ—©çš„å‘å¸ƒæ—¶é—´
            try:
                old_ts = abstract_map[key].get("first_seen")
                if old_ts and base_ts and base_ts < old_ts:
                    abstract_map[key]["first_seen"] = base_ts
                    needs_update = True
            except Exception:
                pass

            s_set = set(abstract_map[key]["sources"])
            if source not in s_set:
                s_set.add(source)
                abstract_map[key]["sources"] = sorted(s_set)
                needs_update = True
    
    if needs_update:
        with open(tools.ABSTRACT_MAP_FILE, "w", encoding="utf-8") as f:
            json.dump(abstract_map, f, ensure_ascii=False, indent=2)
        
        # å¼‚æ­¥æ›´æ–°çŸ¥è¯†å›¾è°±ï¼ˆä¸é˜»å¡ä¸»æµç¨‹ï¼‰
        try:
            import threading
            threading.Thread(target=refresh_graph, daemon=True).start()
        except Exception as e:
            tools.log(f"âš ï¸ å¼‚æ­¥æ›´æ–°çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")

# ======================
# ä¸»å¤„ç†æµç¨‹
# ======================

def get_unprocessed_news_files() -> List[Path]:
    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r") as f:
            processed_ids = set(line.strip() for line in f if line.strip())
    
    unprocessed = []
    for raw_file in sorted(tools.RAW_NEWS_DIR.glob("*.jsonl")):
        deduped_file = tools.DEDUPED_NEWS_DIR / f"{raw_file.stem}_deduped.jsonl"
        if not deduped_file.exists():
            deduper = NewsDeduplicator(threshold=tools.DEDUPE_THRESHOLD)
            # å…ˆç”¨ processed_ids è¿‡æ»¤â€œå†å²å·²å¤„ç†æ–°é—»â€ï¼Œå†å†™å…¥å»é‡æ–‡ä»¶
            deduper.dedupe_file(raw_file, deduped_file, processed_ids)
        unprocessed.append(deduped_file)
    return unprocessed

def process_news_stream():
    tools.log("ğŸš€ å¯åŠ¨ Agent1ï¼šæµå¼äº‹ä»¶ä¸çœŸå®å®ä½“æå–...")
    files = get_unprocessed_news_files()
    if not files:
        tools.log("ğŸ“­ æ— å¯å¤„ç†æ–°é—»æ–‡ä»¶")
        return

    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r") as f:
            processed_ids = set(line.strip() for line in f if line.strip())

    total_processed = 0
    with open(tools.PROCESSED_IDS_FILE, "a") as id_log:
        for file_path in files:
            tools.log(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news = json.loads(line)
                        raw_id = str(news.get("id", "")).strip()
                        source = news.get("source", "unknown").strip().lower()
                        
                        if not raw_id or not source:
                            tools.log("âš ï¸ è·³è¿‡æ—  ID æˆ–æ—  source çš„æ–°é—»")
                            continue

                        global_id = f"{source}:{raw_id}"  # ğŸ‘ˆ å…³é”®ï¼šå¸¦å‰ç¼€çš„å…¨å±€å”¯ä¸€ ID

                        if global_id in processed_ids:
                            continue

                        title = news.get("title", "")
                        content = news.get("content", "")

                        # ä¸ºé¿å…è¶…é•¿æ­£æ–‡å¯¼è‡´ LLM è¶…å‡º token æˆ–è¿”å›å¼‚å¸¸ JSONï¼Œè¿™é‡Œå¯¹æ­£æ–‡åšé•¿åº¦æˆªæ–­
                        MAX_CONTENT_CHARS = 2000  # å¯è°ƒï¼Œæ¯”å¦‚ 1500 / 3000
                        if isinstance(content, str) and len(content) > MAX_CONTENT_CHARS:
                            content = content[:MAX_CONTENT_CHARS] + "â€¦â€¦ã€åæ–‡å·²æˆªæ–­ã€‘"

                        extracted = llm_extract_events(title, content)

                        if extracted:
                            all_entities = []
                            all_entities_original = []
                            for ev in extracted:
                                all_entities.extend(ev["entities"])
                                all_entities_original.extend(ev["entities_original"])
                            if all_entities and len(all_entities) == len(all_entities_original):
                                # ä¼˜å…ˆä½¿ç”¨æ–°é—»è‡ªèº«çš„æ—¶é—´æˆ³ï¼ˆç”±é‡‡é›†å™¨æä¾›ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨å½“å‰æ—¶é—´
                                ts = news.get("timestamp")
                                # éƒ¨åˆ†æ—§æ•°æ®å¯èƒ½æ˜¯ datetime å¯¹è±¡æˆ–å…¶ä»–ç±»å‹ï¼Œç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²
                                published_at = None
                                if ts:
                                    try:
                                        published_at = (
                                            ts if isinstance(ts, str) else str(ts)
                                        )
                                    except Exception:
                                        published_at = None

                                update_entities(all_entities, all_entities_original, source, published_at)
                                update_abstract_map(extracted, source, published_at)
                                total_processed += 1

                                id_log.write(global_id + "\n")  # ğŸ‘ˆ å†™å…¥å¸¦å‰ç¼€çš„ ID
                                processed_ids.add(global_id)
                            else:
                                tools.log(f"ğŸ” æ–°é—» {global_id}ï¼šLLM è¿”å›äº‹ä»¶ä½†æ— æœ‰æ•ˆå®ä½“ï¼Œæš‚ä¸æ ‡è®°")
                        else:
                            tools.log(f"â³ æ–°é—» {global_id}ï¼šLLM æœªè¿”å›æœ‰æ•ˆäº‹ä»¶ï¼Œä¿ç•™é‡è¯•æœºä¼š")

                    except Exception as e:
                        tools.log(f"âš ï¸ å¤„ç†å•æ¡æ–°é—»å¤±è´¥: {e}")
            
            # å¤„ç†å®Œæ–‡ä»¶ååˆ é™¤å¯¹åº”çš„raw_newsæ–‡ä»¶å’Œè¯¥deduped_newsæ–‡ä»¶
            try:
                # æ‰¾åˆ°å¯¹åº”çš„raw_newsæ–‡ä»¶ï¼ˆå»æ‰"_deduped"åç¼€ï¼‰
                raw_file_name = file_path.stem.replace("_deduped", "") + ".jsonl"
                raw_file_path = tools.RAW_NEWS_DIR / raw_file_name
                
                # åˆ é™¤raw_newsæ–‡ä»¶
                if raw_file_path.exists():
                    raw_file_path.unlink()
                    tools.log(f"ğŸ—‘ï¸ åˆ é™¤åŸå§‹æ–°é—»æ–‡ä»¶: {raw_file_path.name}")
                
                # åˆ é™¤deduped_newsæ–‡ä»¶
                if file_path.exists():
                    file_path.unlink()
                    tools.log(f"ğŸ—‘ï¸ åˆ é™¤å»é‡æ–°é—»æ–‡ä»¶: {file_path.name}")
            except Exception as e:
                tools.log(f"âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")

    tools.log(f"âœ… å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡å«æœ‰æ•ˆå®ä½“çš„æ–°é—»")


# ======================
# å…¥å£
# ======================

if __name__ == "__main__":
    process_news_stream()