# src/agents/agent1.py
"""
æ™ºèƒ½ä½“1ï¼šæ–°é—»å…³è”è¯ä¸äº‹ä»¶ç±»å‹æå–å™¨ï¼ˆæ”¯æŒäººå·¥å®¡æ ¸å¼€å…³ï¼‰
"""
from dotenv import load_dotenv
import os
import time
import pandas as pd
import json
from pathlib import Path
from typing import List, Dict, Optional, Set
import re
import warnings
from datetime import datetime, timezone
warnings.filterwarnings("ignore", category=FutureWarning)

DATA_DIR = Path(__file__).parent.parent.parent / "data"
ENTITIES_FILE = DATA_DIR / "crypto_entities.json"
PENDING_ENTITIES_FILE = DATA_DIR / "pending_entities.json"
EVENT_KEYWORDS_FILE = DATA_DIR / "event_keywords.json"
STOP_WORDS_FILE = DATA_DIR / "stop_words.txt"

def load_stop_words() -> Set[str]:
    """ä»ç»Ÿä¸€åœç”¨è¯æ–‡ä»¶åŠ è½½ï¼ˆæ”¯æŒæ³¨é‡Šå’Œç©ºè¡Œï¼‰"""
    stop_words = set()
    if STOP_WORDS_FILE.exists():
        with open(STOP_WORDS_FILE, "r", encoding="utf-8") as f:
            for line in f:
                word = line.strip()
                if word and not word.startswith("#"):
                    stop_words.add(word)
    return stop_words

STOP_WORDS = load_stop_words()

def is_valid_candidate(entity: str) -> bool:
    """åˆ¤æ–­ä¸€ä¸ªå€™é€‰è¯æ˜¯å¦å€¼å¾—è¿›å…¥å¾…å®¡æ ¸æ± """
    word = entity.strip()
    if not word:
        return False
    if len(word) == 1:
        return False
    if word in STOP_WORDS:
        return False
    if word.isdigit():
        return False
    if re.match(r'^[0-9+\-\.%]+$', word):  # çº¯æ•°å­—/ç¬¦å·
        return False
    # æ’é™¤çº¯æ ‡ç‚¹æˆ–ç‰¹æ®Šå­—ç¬¦
    if not any(c.isalnum() for c in word):
        return False
    return True

def load_event_keywords() -> Dict[str, List[str]]:
    """åŠ è½½äº‹ä»¶å…³é”®è¯åº“"""
    if EVENT_KEYWORDS_FILE.exists():
        with open(EVENT_KEYWORDS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    else:
        return {}

EVENT_KEYWORDS = load_event_keywords()

def load_crypto_entities() -> Set[str]:
    if not ENTITIES_FILE.exists():
        default_data = {
            "crypto_assets": ["BTC", "ETH", "SOL", "USDT", "æ¯”ç‰¹å¸", "ä»¥å¤ªåŠ"],
            "organizations": ["Binance", "SEC", "ç¾è”å‚¨"],
            "concepts": ["ETF", "å‡åŠ", "DeFi"]
        }
        DATA_DIR.mkdir(exist_ok=True)
        with open(ENTITIES_FILE, "w", encoding="utf-8") as f:
            json.dump(default_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ†• é¦–æ¬¡è¿è¡Œï¼šå·²åˆ›å»ºé»˜è®¤å®ä½“åº“ {ENTITIES_FILE}")

    with open(ENTITIES_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    all_entities = set()
    for group in data.values():
        all_entities.update(group)
    return all_entities

def save_pending_entities(candidates: Set[str]):
    """ä¿å­˜é€šè¿‡åˆç­›çš„å€™é€‰å®ä½“"""
    if not candidates:
        return

    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    if PENDING_ENTITIES_FILE.exists():
        with open(PENDING_ENTITIES_FILE, "r", encoding="utf-8") as f:
            pending = json.load(f)
    else:
        pending = {}

    now = datetime.now(timezone.utc).isoformat()
    for ent in candidates:
        if ent not in pending:
            pending[ent] = {
                "first_seen": now,
                "status": "pending",
                "source_contexts": []
            }

    with open(PENDING_ENTITIES_FILE, "w", encoding="utf-8") as f:
        json.dump(pending, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ åˆç­›åæ–°å¢ {len(candidates)} ä¸ªå€™é€‰å®ä½“åˆ°å¾…å®¡æ ¸æ–‡ä»¶")

def _add_concept_to_event_keywords(entity: str, event_keywords: dict) -> dict:
    """
    äº¤äº’å¼è¯¢é—®æ˜¯å¦å°† actions ç±»å®ä½“åŠ å…¥äº‹ä»¶å…³é”®è¯åº“
    è¿”å›æ›´æ–°åçš„ event_keywords å­—å…¸
    """
    print(f"\nğŸ’¡ æ£€æµ‹åˆ° '{entity}' è¢«åŠ å…¥ 'actions'ï¼Œæ˜¯å¦ä¹Ÿä½œä¸ºäº‹ä»¶å…³é”®è¯ï¼Ÿ")
    print("[1] åŠ å…¥ç°æœ‰äº‹ä»¶ç±»å‹")
    print("[2] åˆ›å»ºæ–°äº‹ä»¶ç±»å‹")
    print("[3] ä¸åŠ å…¥äº‹ä»¶å…³é”®è¯åº“")

    while True:
        choice = input("è¯·é€‰æ‹© (1/2/3): ").strip()
        if choice == "3":
            return event_keywords
        elif choice == "1":
            print("\nç°æœ‰äº‹ä»¶ç±»å‹:")
            event_types = list(event_keywords.keys())
            for i, et in enumerate(event_types, 1):
                print(f"  [{i}] {et} â†’ {', '.join(event_keywords[et][:3])}...")
            try:
                idx = int(input("é€‰æ‹©ç¼–å·: ").strip()) - 1
                if 0 <= idx < len(event_types):
                    target_type = event_types[idx]
                    if entity not in event_keywords[target_type]:
                        event_keywords[target_type].append(entity)
                        print(f"âœ… '{entity}' å·²åŠ å…¥äº‹ä»¶ç±»å‹ '{target_type}'")
                    else:
                        print(f"â„¹ï¸ '{entity}' å·²åœ¨ '{target_type}' ä¸­")
                    return event_keywords
                else:
                    print("âš ï¸ ç¼–å·è¶…å‡ºèŒƒå›´")
            except ValueError:
                print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        elif choice == "2":
            while True:
                new_type = input("è¾“å…¥æ–°äº‹ä»¶ç±»å‹åç§°ï¼ˆå¦‚ 'governance'ï¼‰: ").strip()
                if new_type and re.match(r'^[a-z_][a-z0-9_]*$', new_type):
                    if new_type in event_keywords:
                        print(f"âš ï¸ äº‹ä»¶ç±»å‹ '{new_type}' å·²å­˜åœ¨")
                        continue
                    event_keywords[new_type] = [entity]
                    print(f"ğŸ†• åˆ›å»ºæ–°äº‹ä»¶ç±»å‹ '{new_type}' å¹¶æ·»åŠ å…³é”®è¯ '{entity}'")
                    return event_keywords
                else:
                    print("âš ï¸ äº‹ä»¶ç±»å‹åéœ€ä¸ºå°å†™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼Œä¸”ä¸èƒ½ä»¥æ•°å­—å¼€å¤´")
        else:
            print("âš ï¸ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡è¯•")

def approve_pending_entities():
    """
    äº¤äº’å¼å®¡æ ¸å¾…æ‰¹å‡†å®ä½“ã€‚
    æ”¯æŒï¼š
      1. åŠ å…¥åœç”¨è¯åº“ï¼ˆè¿½åŠ åˆ° stop_words.txtï¼‰
      2. åŠ å…¥å·²æœ‰åˆ†ç±»ï¼ˆç¼–å·é€‰æ‹©ï¼‰
      3. åˆ›å»ºæ–°åˆ†ç±»
    """
    if not PENDING_ENTITIES_FILE.exists():
        print("ğŸ“­ å¾…å®¡æ ¸æ–‡ä»¶ä¸å­˜åœ¨")
        return

    with open(PENDING_ENTITIES_FILE, "r", encoding="utf-8") as f:
        pending = json.load(f)

    pending_entities = {
        ent: info for ent, info in pending.items()
        if info.get("status") == "pending"
    }

    if not pending_entities:
        print("âœ… æ‰€æœ‰å¾…å®¡å®ä½“å·²å¤„ç†å®Œæ¯•ï¼")
        return

    # åŠ è½½ä¸»çŸ¥è¯†åº“
    with open(ENTITIES_FILE, "r", encoding="utf-8") as f:
        main_data = json.load(f)

    categories = list(main_data.keys())
    total = len(pending_entities)

    # åŠ è½½å½“å‰åœç”¨è¯ï¼ˆç”¨äºå»é‡ï¼‰
    current_stop_words = load_stop_words()
    new_stop_words_to_add = set()
    approved_updates = {}  # entity -> target_category or "__STOP__"

    for i, (entity, info) in enumerate(pending_entities.items(), 1):
        print(f"\n{'='*50}")
        print(f"ğŸ” [{i}/{total}] å®¡æ ¸å®ä½“: '{entity}'")
        print("[1] åŠ å…¥åœç”¨è¯åº“ï¼ˆæ°¸ä¹…å¿½ç•¥ï¼‰")
        print("[2] åŠ å…¥å·²æœ‰åˆ†ç±»")
        print("[3] åˆ›å»ºæ–°åˆ†ç±»")
        print("[q] é€€å‡ºå®¡æ ¸")

        choice = input("è¯·é€‰æ‹© (1/2/3/q): ").strip().lower()
        if choice == 'q':
            print("â¹ï¸ å®¡æ ¸å·²é€€å‡ºã€‚")
            break
        elif choice == '1':
            if entity in current_stop_words:
                print(f"â„¹ï¸ '{entity}' å·²åœ¨åœç”¨è¯åº“ä¸­")
            else:
                new_stop_words_to_add.add(entity)
                approved_updates[entity] = "__STOP__"
                print(f"ğŸ—‘ï¸ '{entity}' å°†è¢«åŠ å…¥åœç”¨è¯åº“")
        elif choice == '2':
            print("\nå·²æœ‰åˆ†ç±»:")
            for idx, cat in enumerate(categories, 1):
                print(f"  [{idx}] {cat}")
            while True:
                try:
                    sel = input("è¯·é€‰æ‹©ç¼–å·: ").strip()
                    if not sel:
                        continue
                    idx = int(sel) - 1
                    if 0 <= idx < len(categories):
                        target_cat = categories[idx]
                        approved_updates[entity] = target_cat
                        print(f"âœ… '{entity}' å°†åŠ å…¥åˆ†ç±» '{target_cat}'")
                        if target_cat == "actions":
                            # åªæœ‰å½“ EVENT_KEYWORDS_FILE å­˜åœ¨æˆ–å¯åŠ è½½æ—¶æ‰å¤„ç†
                            try:
                                with open(EVENT_KEYWORDS_FILE, "r", encoding="utf-8") as f:
                                    current_event_kw = json.load(f)
                            except Exception:
                                current_event_kw = {}

                            updated_event_kw = _add_concept_to_event_keywords(entity, current_event_kw)

                            # å¦‚æœæœ‰ä¿®æ”¹ï¼Œç«‹å³ä¿å­˜å›æ–‡ä»¶
                            if updated_event_kw != current_event_kw:
                                with open(EVENT_KEYWORDS_FILE, "w", encoding="utf-8") as f:
                                    json.dump(updated_event_kw, f, ensure_ascii=False, indent=2)
                                print(f"ğŸ’¾ äº‹ä»¶å…³é”®è¯åº“å·²æ›´æ–°: {EVENT_KEYWORDS_FILE}")
                        break
                    else:
                        print("âš ï¸ ç¼–å·è¶…å‡ºèŒƒå›´ï¼Œè¯·é‡è¯•")
                except ValueError:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
        elif choice == '3':
            while True:
                new_cat = input("è¯·è¾“å…¥æ–°åˆ†ç±»åï¼ˆå¦‚ protocolsï¼‰: ").strip()
                if new_cat and re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', new_cat):
                    if new_cat not in main_data:
                        main_data[new_cat] = []
                        categories.append(new_cat)
                    approved_updates[entity] = new_cat
                    print(f"ğŸ†• åˆ›å»ºæ–°åˆ†ç±» '{new_cat}' å¹¶æ·»åŠ  '{entity}'")
                    break
                else:
                    print("âš ï¸ åˆ†ç±»åéœ€ä¸ºåˆæ³• Python æ ‡è¯†ç¬¦ï¼ˆå­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ï¼Œä¸èƒ½ä»¥æ•°å­—å¼€å¤´ï¼‰")
        else:
            print("âš ï¸ æ— æ•ˆé€‰é¡¹ï¼Œè·³è¿‡æ­¤å®ä½“")
            continue

    if not approved_updates:
        print("\nâ„¹ï¸ æœªåšä»»ä½•ä¿®æ”¹")
        return

    # --- æ›´æ–°ä¸»çŸ¥è¯†åº“ï¼ˆéåœç”¨è¯é¡¹ï¼‰---
    for entity, target in approved_updates.items():
        if target != "__STOP__":
            if entity not in main_data[target]:
                main_data[target].append(entity)

    # å»é‡ + æ’åº
    for key in main_data:
        main_data[key] = sorted(list(set(main_data[key])))

    with open(ENTITIES_FILE, "w", encoding="utf-8") as f:
        json.dump(main_data, f, ensure_ascii=False, indent=2)

    # --- è¿½åŠ æ–°åœç”¨è¯åˆ°æ–‡ä»¶ ---
    if new_stop_words_to_add:
        with open(STOP_WORDS_FILE, "a", encoding="utf-8") as f:
            for word in sorted(new_stop_words_to_add):
                f.write("\n" + word)
        print(f"\nğŸ’¾ å·²å°† {len(new_stop_words_to_add)} ä¸ªæ–°è¯è¿½åŠ åˆ°åœç”¨è¯åº“: {STOP_WORDS_FILE}")

    # --- æ¸…ç† pending æ–‡ä»¶ ---
    remaining = {ent: info for ent, info in pending.items() if ent not in approved_updates}
    with open(PENDING_ENTITIES_FILE, "w", encoding="utf-8") as f:
        json.dump(remaining, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å®¡æ ¸å®Œæˆï¼å…±å¤„ç† {len(approved_updates)} ä¸ªå®ä½“ã€‚")
    print(f"   - ä¸»çŸ¥è¯†åº“å·²æ›´æ–°: {ENTITIES_FILE}")
    print(f"   - å¾…å®¡æ–‡ä»¶å‰©ä½™ {len(remaining)} é¡¹")

def extract_entities_from_text(text: str, known_entities: Set[str]) -> List[str]:
    if not isinstance(text, str):
        return []
    found = set()
    for entity in known_entities:
        if entity in text:
            found.add(entity)
    return sorted(found)

def classify_event_type(title: str, content: str) -> Optional[str]:
    full_text = (title + " " + content) if isinstance(content, str) else title
    if not isinstance(full_text, str):
        return None
    EVENT_KEYWORDS = load_event_keywords()
    scores = {}
    for event_type, keywords in EVENT_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw in full_text)
        if score > 0:
            scores[event_type] = score
    return max(scores, key=scores.get) if scores else None

def load_entity_categories() -> Dict[str, Set[str]]:
    """åŠ è½½å®Œæ•´çš„å®ä½“åˆ†ç±»å­—å…¸ï¼ˆç”¨äºå¤–éƒ¨æ¨¡å—å¦‚ TradingAgent ä½¿ç”¨ï¼‰"""
    if not ENTITIES_FILE.exists():
        load_crypto_entities()  # è§¦å‘åˆå§‹åŒ–
    
    with open(ENTITIES_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    return {category: set(entities) for category, entities in data.items()}

class Agent1EntityExtractor:
    def __init__(self, auto_update: bool = False):
        """
        :param auto_update: æ˜¯å¦è‡ªåŠ¨å°†æ–°å®ä½“å†™å…¥ä¸»çŸ¥è¯†åº“ï¼ˆå¦åˆ™å†™å…¥ pending æ–‡ä»¶ï¼‰
        """
        self.auto_update = auto_update
        self.known_entities = load_crypto_entities()

    def discover_new_entities(
        self, 
        df: pd.DataFrame, 
        min_freq: int = 2
    ) -> tuple[Set[str], Dict[str, List[str]]]:  # â† è¿”å›ä¸¤ä¸ªå€¼
        from collections import Counter
        candidate_counter = Counter()
        # ç”¨äºæ”¶é›†ä¸Šä¸‹æ–‡ï¼šentity -> [title1, title2, ...]
        context_map: Dict[str, List[str]] = {}

        for _, row in df.iterrows():
            title = str(row['title'])
            content = str(row.get('content', ''))
            text = f"{title} {content}"

            if not isinstance(text, str):
                continue

            found_in_row = set()

            # $WIF
            for match in re.findall(r'\$[A-Za-z0-9]{2,10}', text):
                ent = match.upper().lstrip('$')
                if len(ent) >= 2:
                    found_in_row.add(ent)

            # å¤§å†™ä»£å¸ç¬¦å·
            for match in re.findall(r'\b[A-Z]{3,6}\b', text):
                if match not in {"USD", "API", "NFT", "ETF", "SEC", "OKX"}:
                    found_in_row.add(match)

            # ä¸­æ–‡é¡¹ç›®å
            for match in re.findall(r'[\u4e00-\u9fa5]{2,4}', text):
                found_in_row.add(match)

            # æ›´æ–°è®¡æ•° & ä¸Šä¸‹æ–‡
            for ent in found_in_row:
                candidate_counter[ent] += 1
                if ent not in context_map:
                    context_map[ent] = []
                context_map[ent].append(title)  # æˆ–è€…å­˜æ•´ä¸ª textï¼Ÿ

        # ç­›é€‰æœ‰æ•ˆå€™é€‰
        valid_candidates = {
            ent for ent, cnt in candidate_counter.items()
            if cnt >= min_freq 
            and ent not in self.known_entities 
            and is_valid_candidate(ent) 
            and ent not in EVENT_KEYWORDS
        }

        # åªä¿ç•™ valid_candidates çš„ä¸Šä¸‹æ–‡
        filtered_context_map = {
            ent: context_map[ent] for ent in valid_candidates if ent in context_map
        }

        return valid_candidates, filtered_context_map  # âœ… è¿”å›ä¸¤ä¸ªå€¼

    def process(self, df: pd.DataFrame) -> pd.DataFrame:
        if df.empty:
            return df

        required_cols = ['title', 'content']
        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"è¾“å…¥DataFrameç¼ºå°‘å¿…è¦åˆ—: {col}")

        result_df = df.copy()

        # 1. åˆç­›å®ä½“ï¼ˆåŸºäºè§„åˆ™/è¯å…¸ï¼‰
        result_df['entities'] = result_df.apply(
            lambda row: extract_entities_from_text(
                str(row['title']) + " " + str(row.get('content', '')),
                self.known_entities
            ),
            axis=1
        )

        # 2. äº‹ä»¶ç±»å‹åˆ†ç±»
        result_df['event_type'] = result_df.apply(
            lambda row: classify_event_type(
                str(row['title']), str(row.get('content', ''))
            ),
            axis=1
        )

        # ğŸ”‘ 3. å‘ç°æ–°å®ä½“å¹¶æ”¶é›†ä¸Šä¸‹æ–‡ï¼ˆç”¨äº LLM äºŒç­›ï¼‰
        new_entities, context_map = self.discover_new_entities(result_df, min_freq=2)

         # ğŸ¤– 4. LLM äºŒç­›ï¼ˆä»…éè‡ªåŠ¨æ¨¡å¼ï¼‰
        final_valid_entities = set(self.known_entities)
        if new_entities and not self.auto_update:
            filtered_new, _ = llm_second_pass_filter(new_entities, context_map)
            final_valid_entities.update(filtered_new)
            save_pending_entities(filtered_new)

        elif new_entities and self.auto_update:
            # è‡ªåŠ¨æ¨¡å¼ï¼šä½¿ç”¨ LLM çš„åˆ†ç±»ç»“æœ
            filtered_new, category_map = llm_second_pass_filter(new_entities, context_map)
            final_valid_entities.update(filtered_new)
            self._save_entities_to_main(filtered_new, category_map)

        # âœ… 5. ã€å…³é”®ã€‘ç”¨æœ€ç»ˆæœ‰æ•ˆå®ä½“è¿‡æ»¤æ¯æ¡æ–°é—»çš„ entities åˆ—
        result_df['entities'] = result_df['entities'].apply(
            lambda ents: [e for e in ents if e in final_valid_entities]
        )

        print(f"ğŸ§  æ™ºèƒ½ä½“1å¤„ç†å®Œæˆï¼šå…±å¤„ç† {len(result_df)} æ¡æ–°é—»")
        print(f"   - å¹³å‡æ¯æ¡æ–°é—»æå– {result_df['entities'].apply(len).mean():.1f} ä¸ªå®ä½“")
        print(f"   - è¯†åˆ«å‡º {result_df['event_type'].notna().sum()} æ¡å¸¦äº‹ä»¶ç±»å‹çš„æ–°é—»")
        
        return result_df

    def _save_entities_to_main(self, new_entities: Set[str], category_map: Dict[str, str]):
        """å°†æ–°å®ä½“æŒ‰ LLM é¢„æµ‹çš„ç±»åˆ«åˆå¹¶åˆ°ä¸»çŸ¥è¯†åº“"""
        with open(ENTITIES_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        # ç¡®ä¿æ‰€æœ‰ç›®æ ‡ç±»åˆ«å­˜åœ¨
        for cat in set(category_map.values()):
            if cat not in data:
                data[cat] = []

        # æŒ‰ç±»åˆ«æ·»åŠ 
        for ent in new_entities:
            cat = category_map.get(ent, "concepts")
            if cat not in data:
                cat = "concepts"
            if ent not in data[cat]:
                data[cat].append(ent)

        # å»é‡ + æ’åº
        for key in data:
            data[key] = sorted(list(set(data[key])))

        with open(ENTITIES_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ… è‡ªåŠ¨æ–°å¢ {len(new_entities)} ä¸ªå®ä½“åˆ°ä¸»çŸ¥è¯†åº“ï¼ˆæŒ‰ LLM åˆ†ç±»ï¼‰:")
        for ent in sorted(new_entities):
            print(f"   - '{ent}' â†’ {category_map.get(ent, 'concepts')}")



def llm_second_pass_filter(
    candidates: Set[str], 
    context_map: Dict[str, List[str]]
) -> tuple[Set[str], Dict[str, str]]:  # â† è¿”å› (æœ‰æ•ˆå®ä½“é›†åˆ, å®ä½“â†’ç±»åˆ«æ˜ å°„)
    """
    ä½¿ç”¨ DeepSeek API å¯¹åˆç­›å€™é€‰å®ä½“è¿›è¡ŒäºŒæ¬¡è¿‡æ»¤ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰ã€‚
    - ä¸€æ¬¡æ€§å‘é€æ‰€æœ‰å€™é€‰å®ä½“
    - è¾“å‡ºæ ¼å¼ï¼š{"entity1": {"is_valid": true, "category": "..."}, ...}
    - è‹¥æœªè®¾ç½® API Key æˆ–è°ƒç”¨å¤±è´¥ï¼Œåˆ™è·³è¿‡ LLM è¿‡æ»¤ï¼Œè¿”å›åŸé›†åˆï¼ˆå®‰å…¨é™çº§ï¼‰
    """
    try:
        from openai import OpenAI
    except ImportError:
        print("âš ï¸ openai åº“æœªå®‰è£…ï¼Œè·³è¿‡ LLM äºŒç­›")
        return candidates, {e: "concepts" for e in candidates}

    # ğŸ”‘ åŠ è½½ API Key
    AGENT_DIR = Path(__file__).parent
    ENV_PATH = AGENT_DIR / ".env.local"
    load_dotenv(ENV_PATH, override=True)
    api_key = os.getenv("DEEPSEEK_API_KEY")
    if not api_key:
        print("âš ï¸ æœªè®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼Œè·³è¿‡ LLM äºŒç­›")
        return candidates

    client = OpenAI(
        api_key=api_key,
        base_url="https://api.deepseek.com"
    )

    # ğŸ§  æ„é€  entitie å­—ç¬¦ä¸²
    sorted_entities = sorted(candidates)
    entities_str = ", ".join(f'"{e}"' for e in sorted_entities)
    contexts_lines = []
    for entity in sorted_entities:
        ctxs = context_map.get(entity, [])
        ctx_str = "\n".join(f"- {ctx}" for ctx in ctxs[:3])  # æœ€å¤š3æ¡ä¸Šä¸‹æ–‡
        contexts_lines.append(f"ã€{entity}ã€‘\n{ctx_str or 'ï¼ˆæ— ä¸Šä¸‹æ–‡ï¼‰'}")
    contexts_str = "\n\n".join(contexts_lines)
    # ğŸ’¬ æç¤ºè¯
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸ä¸åŒºå—é“¾é¢†åŸŸåˆ†æå¸ˆã€‚è¯·ä¸¥æ ¼è¯„ä¼°ä»¥ä¸‹è¯è¯­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„é¢†åŸŸå®ä½“ã€‚

**æœ‰æ•ˆå®ä½“åŒ…æ‹¬**ï¼š
- åŠ å¯†èµ„äº§ï¼ˆå¦‚ BTCã€ä»¥å¤ªåŠã€SOLã€$WIFï¼‰
- é¡¹ç›®/åè®®ï¼ˆå¦‚ Uniswapã€Arbitrumã€Baseé“¾ï¼‰
- ç»„ç»‡/å…¬å¸ï¼ˆå¦‚ Binanceã€Coinbaseã€SECï¼‰
- æŠ€æœ¯æ¦‚å¿µï¼ˆå¦‚ å‡åŠã€ç©ºæŠ•ã€è´¨æŠ¼ã€MEVï¼‰
- äººå/æ˜µç§° ï¼ˆå¦‚ éº»å‰å¤§å“¥ã€CZï¼‰
- è¡Œä¸º ï¼ˆå¦‚ åˆ†çº¢ã€åˆä½œï¼‰
- äº‹ä»¶ç±»å‹å…³é”®è¯ï¼ˆå¦‚ åˆ†å‰ã€é»‘å®¢æ”»å‡»ã€ç›‘ç®¡å¤„ç½šï¼‰

**æ— æ•ˆå†…å®¹åŒ…æ‹¬**ï¼š
- æ™®é€šåŠ¨è¯/å½¢å®¹è¯ï¼ˆå¦‚ ä¸Šæ¶¨ã€æš´è·Œã€åˆ©å¥½ã€å®£å¸ƒï¼‰
- æ—¶é—´è¯ï¼ˆå¦‚ ä»Šå¤©ã€æ˜¨æ—¥ï¼‰
- æ³›æ³›è¯æ±‡ï¼ˆå¦‚ å¸‚åœºã€æŠ•èµ„è€…ã€æ¶ˆæ¯ï¼‰
- çº¯æ•°å­—æˆ–ç¬¦å·

è¯è¯­: {entities_str}
å‡ºç°ä¸Šä¸‹æ–‡: {contexts_str}

è¯·ä»…è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "entity_name1": {{
    "is_valid": true,
    "category": "crypto_assets|organizations|concepts|persons|actions|events|other"
  }},
  "entity_name2": {{
    "is_valid": false,
    "category": "other"
  }}
}}

ä¸è¦è§£é‡Šï¼Œä¸è¦é¢å¤–æ–‡æœ¬ã€‚"""

    total = len(candidates)
    print(f"ğŸ¤– å¯åŠ¨ DeepSeek LLM æ‰¹é‡äºŒç­›ï¼šå…± {total} ä¸ªå€™é€‰å®ä½“")

    try:
        response = client.chat.completions.create(
            # model="deepseek-chat",
            # max_tokens=8192,
            model="deepseek-reasoner",
            extra_body={"thinking": {"type": "enabled"}},
            messages=[{"role": "user", "content": prompt}],
            max_tokens=64000,
            timeout=600,
            stream=False
        )
        content = response.choices[0].message.content.strip()

        # ğŸ§¹ æ¸…ç† Markdown åŒ…è£¹
        if content.startswith("```json"):
            content = content.split("```json", 1)[1].split("```")[0]
        elif content.startswith("```"):
            content = content.split("```", 1)[1].split("```")[0]

        # ğŸ“¦ è§£æ JSON
        result_dict = json.loads(content)
        print(f"  [DEBUG] DeepSeek è¿”å›åŸå§‹ç»“æœï¼ˆå‰3é¡¹ï¼‰: {dict(list(result_dict.items())[:3])}")

        valid_entities = set()
        invalid_entities = set()
        for entity in sorted_entities:
            entry = result_dict.get(entity)
            if isinstance(entry, dict):
                is_valid = entry.get("is_valid")
                if is_valid is True:
                    valid_entities.add(entity)
                    print(f"  âœ… '{entity}' â†’ æœ‰æ•ˆ ({entry.get('category')})")
                else:
                    invalid_entities.add(entity)
                    print(f"  âŒ '{entity}' â†’ æ— æ•ˆ")
            else:
                # LLM æ ¼å¼é”™è¯¯ï¼Œä½†ä¸ºå®‰å…¨èµ·è§ä¿ç•™ï¼ˆæˆ–å¯é€‰æ‹©ä¸¢å¼ƒï¼‰
                print(f"  âš ï¸ '{entity}' æ ¼å¼å¼‚å¸¸ï¼Œä¿ç•™ï¼ˆå®‰å…¨ç­–ç•¥ï¼‰")
                valid_entities.add(entity)

        # æ£€æŸ¥æ˜¯å¦æœ‰å®ä½“æœªè¢« LLM è¿”å›
        missing_entities = set(sorted_entities) - set(result_dict.keys())
        if missing_entities:
            print(f"  âš ï¸ LLM æœªè¿”å› {len(missing_entities)} ä¸ªå®ä½“ï¼Œè‡ªåŠ¨ä¿ç•™: {sorted(missing_entities)}")
            valid_entities.update(missing_entities)

        if invalid_entities:
            # åŠ è½½å½“å‰åœç”¨è¯ï¼ˆç”¨äºå»é‡ï¼‰
            current_stop_words = load_stop_words()
            new_invalid_words = invalid_entities - current_stop_words

            if new_invalid_words:
                STOP_WORDS_FILE.parent.mkdir(exist_ok=True)
                with open(STOP_WORDS_FILE, "a", encoding="utf-8") as f:
                    for word in sorted(new_invalid_words):
                        f.write("\n" + word)
                print(f"ğŸ§¹ å·²å°† {len(new_invalid_words)} ä¸ªæ— æ•ˆè¯è¿½åŠ åˆ°åœç”¨è¯åº“: {STOP_WORDS_FILE}")
                print(f"   æ–°å¢è¯: {sorted(new_invalid_words)}")
            else:
                print("â„¹ï¸ æ— æ•ˆè¯å‡å·²å­˜åœ¨äºåœç”¨è¯åº“ï¼Œæ— éœ€æ›´æ–°")

        category_map = {}
        for entity in sorted_entities:
            entry = result_dict.get(entity)
            if isinstance(entry, dict) and entry.get("is_valid") is True:
                cat = entry.get("category", "concepts")  # é»˜è®¤ fallback
                # ç¡®ä¿ category æ˜¯ä¸»çŸ¥è¯†åº“ä¸­å·²æœ‰çš„ keyï¼Œå¦åˆ™å½’å…¥ concepts
                if cat not in ["crypto_assets", "organizations", "concepts", "persons", "actions", "events"]:
                    cat = "concepts"
                category_map[entity] = cat

        # å¯¹ missing_entitiesï¼Œä¹Ÿç»™é»˜è®¤ç±»åˆ«ï¼ˆæ¯”å¦‚ conceptsï¼‰
        for ent in missing_entities:
            category_map[ent] = "concepts"

        print(f"âœ… DeepSeek LLM äºŒç­›å®Œæˆï¼š{len(valid_entities)}/{total} ä¸ªå®ä½“é€šè¿‡")
        return valid_entities, category_map 

    except Exception as e:
        print(f"âŒ DeepSeek æ‰¹é‡è°ƒç”¨å¤±è´¥: {e}")
        print("âš ï¸ å®‰å…¨é™çº§ï¼šä¿ç•™æ‰€æœ‰å€™é€‰å®ä½“ï¼Œç±»åˆ«è®¾ä¸º concepts")
        return candidates, {e: "concepts" for e in candidates}

if __name__ == "__main__":
    approve_pending_entities()