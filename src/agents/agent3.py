import json
import time
import threading
from datetime import datetime
from difflib import SequenceMatcher
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Any
from ..utils.tool_function import tools
from .api_client import LLMAPIPool

try:
    import yaml
except ImportError:
    yaml = None


class RateLimiter:
    """ç®€å•çº¿ç¨‹å®‰å…¨ä»¤ç‰Œæ¡¶ï¼Œæ§åˆ¶å…¨å±€QPS"""
    def __init__(self, rate_per_sec: float):
        self.interval = 1.0 / rate_per_sec if rate_per_sec > 0 else 0
        self._lock = threading.Lock()
        self._next = 0.0

    def acquire(self):
        if self.interval <= 0:
            return
        with self._lock:
            now = time.time()
            if now < self._next:
                time.sleep(self._next - now)
            self._next = max(self._next, now) + self.interval

class KnowledgeGraph:
    """
    å‹ç¼©çŸ¥è¯†å›¾è°±ç³»ç»Ÿï¼Œç”¨äºç®¡ç†å®ä½“å’Œäº‹ä»¶ï¼Œæ”¯æŒé‡å¤æ£€æµ‹å’Œæ›´æ–°ã€‚
    """
    
    def __init__(self):
        self.tools = tools()
        self.entities_file = self.tools.ENTITIES_FILE
        self.events_file = self.tools.EVENTS_FILE
        self.abstract_map_file = self.tools.ABSTRACT_MAP_FILE
        self.entities_tmp_file = self.tools.ENTITIES_TMP_FILE
        self.abstract_tmp_file = self.tools.ABSTRACT_TMP_FILE
        self.kg_file = self.tools.KNOWLEDGE_GRAPH_FILE
        self.merge_rules_file = self.tools.CONFIG_DIR / "entity_merge_rules.json" # è§„åˆ™æ–‡ä»¶è·¯å¾„
        self.merge_rules = {} # å†…å­˜ä¸­çš„è§„åˆ™ç¼“å­˜
        self.settings = self._load_agent3_settings()
        self.graph = {
            "entities": {},  # å®ä½“ID -> å®ä½“ä¿¡æ¯
            "events": {},   # äº‹ä»¶æ‘˜è¦ -> äº‹ä»¶ä¿¡æ¯
            "edges": []     # è¾¹åˆ—è¡¨ï¼Œè¿æ¥å®ä½“å’Œäº‹ä»¶
        }
        self.llm_pool = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self._tmp_loaded = []  # è®°å½•å·²åŠ è½½çš„tmpæ–‡ä»¶ï¼Œåˆ·æ–°å®Œæˆåæ¸…ç†
        self._load_merge_rules() # åˆå§‹åŒ–æ—¶åŠ è½½è§„åˆ™
        
    def _init_llm_pool(self):
        """åˆå§‹åŒ–LLM APIæ± """
        if self.llm_pool is None:
            try:
                self.llm_pool = LLMAPIPool()
                self.tools.log("[çŸ¥è¯†å›¾è°±] LLM APIæ± åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ åˆå§‹åŒ–LLM APIæ± å¤±è´¥: {e}")
                self.llm_pool = None
    
    def load_data(self) -> bool:
        """ä»æ–‡ä»¶åŠ è½½å®ä½“å’Œäº‹ä»¶æ•°æ®"""
        try:
            if self.entities_file.exists():
                with open(self.entities_file, 'r', encoding='utf-8') as f:
                    self.graph['entities'] = json.load(f)
            else:
                self.graph['entities'] = {}
                
            if self.abstract_map_file.exists():
                with open(self.abstract_map_file, 'r', encoding='utf-8') as f:
                    abstract_map = json.load(f)
                    # è½¬æ¢abstract_mapä¸ºäº‹ä»¶æ ¼å¼
                    self.graph['events'] = {
                        abstract: {
                            "abstract": abstract,
                            "entities": data["entities"],
                            "event_summary": data.get("event_summary", ""),
                            "sources": data.get("sources", []),
                            "first_seen": data.get("first_seen", "")
                        }
                        for abstract, data in abstract_map.items()
                    }
            else:
                self.graph['events'] = {}
            
            # é¢å¤–åŠ è½½ tmp æ–°å¢æ•°æ®ï¼ˆæœªåˆå¹¶çš„æ–°å¢å®ä½“/äº‹ä»¶ï¼‰
            self._load_tmp_entities()
            self._load_tmp_events()
                
            self._build_edges()
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] æ•°æ®åŠ è½½æˆåŠŸ: {len(self.graph['entities'])} å®ä½“, {len(self.graph['events'])} äº‹ä»¶")
            return True
        except Exception as e:
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def _build_edges(self):
        """æ„å»ºå®ä½“å’Œäº‹ä»¶ä¹‹é—´çš„è¾¹"""
        self.graph['edges'] = []
        for abstract, event in self.graph['events'].items():
            for entity in event.get('entities', []):
                if entity in self.graph['entities']:
                    self.graph['edges'].append({
                        "from": entity,
                        "to": abstract,
                        "type": "involved_in"
                    })
    
    def build_graph(self) -> bool:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        return self.load_data()
    
    def _load_merge_rules(self):
        """åŠ è½½å®ä½“åˆå¹¶è§„åˆ™"""
        if self.merge_rules_file.exists():
            try:
                with open(self.merge_rules_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.merge_rules = data.get("merge_rules", {})
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] å·²åŠ è½½ {len(self.merge_rules)} æ¡å®ä½“åˆå¹¶è§„åˆ™")
            except Exception as e:
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½åˆå¹¶è§„åˆ™å¤±è´¥: {e}")
        else:
            self.merge_rules = {}

    def _load_tmp_entities(self):
        """åŠ è½½å¹¶åˆå¹¶ tmp å®ä½“æ•°æ®"""
        if self.entities_tmp_file.exists():
            try:
                with open(self.entities_tmp_file, "r", encoding="utf-8") as f:
                    tmp_entities = json.load(f)
                    for name, data in tmp_entities.items():
                        if name in self.graph['entities']:
                            self._merge_entity_record(self.graph['entities'][name], data)
                        else:
                            self.graph['entities'][name] = data
                self._tmp_loaded.append(self.entities_tmp_file)
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] å·²åŠ è½½ tmp å®ä½“ {len(tmp_entities)} æ¡")
            except Exception as e:
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½ tmp å®ä½“å¤±è´¥: {e}")

    def _load_tmp_events(self):
        """åŠ è½½å¹¶åˆå¹¶ tmp äº‹ä»¶æ•°æ®"""
        if self.abstract_tmp_file.exists():
            try:
                with open(self.abstract_tmp_file, "r", encoding="utf-8") as f:
                    tmp_events = json.load(f)
                    for abstract, data in tmp_events.items():
                        if abstract in self.graph['events']:
                            self._merge_event_record(self.graph['events'][abstract], data)
                        else:
                            self.graph['events'][abstract] = {
                                "abstract": abstract,
                                "entities": data.get("entities", []),
                                "event_summary": data.get("event_summary", ""),
                                "sources": data.get("sources", []),
                                "first_seen": data.get("first_seen", "")
                            }
                self._tmp_loaded.append(self.abstract_tmp_file)
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] å·²åŠ è½½ tmp äº‹ä»¶ {len(tmp_events)} æ¡")
            except Exception as e:
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½ tmp äº‹ä»¶å¤±è´¥: {e}")

    def _cleanup_tmp_files(self):
        """åˆ·æ–°å®Œæˆåæ¸…ç†å·²åŠ è½½çš„ tmp æ–‡ä»¶"""
        for path in self._tmp_loaded:
            try:
                path.unlink(missing_ok=True)
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] ğŸ—‘ï¸ å·²æ¸…ç† tmp æ–‡ä»¶: {path}")
            except Exception as e:
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ æ— æ³•åˆ é™¤ tmp æ–‡ä»¶ {path}: {e}")
        self._tmp_loaded = []

    def _save_merge_rules(self):
        """ä¿å­˜å®ä½“åˆå¹¶è§„åˆ™"""
        try:
            data = {
                "merge_rules": self.merge_rules,
                "last_updated": time.strftime("%Y-%m-%dT%H:%M:%S")
            }
            with open(self.merge_rules_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] å·²ä¿å­˜åˆå¹¶è§„åˆ™åº“ (å…± {len(self.merge_rules)} æ¡)")
        except Exception as e:
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ ä¿å­˜åˆå¹¶è§„åˆ™å¤±è´¥: {e}")

    def _merge_entity_record(self, target: Dict[str, Any], source: Dict[str, Any]):
        """å°†sourceå®ä½“ä¿¡æ¯åˆå¹¶åˆ°targetï¼Œä¸åˆ é™¤èŠ‚ç‚¹"""
        if not source:
            return
        # sources
        primary_sources = set()
        for s in target.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        for s in source.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        target['sources'] = list(primary_sources)

        # original_forms
        primary_forms = set()
        for f in target.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)
        for f in source.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)
        target['original_forms'] = list(primary_forms)

        # first_seen å–æœ€æ—©
        primary_first = target.get('first_seen', '')
        source_first = source.get('first_seen', '')
        if source_first and (not primary_first or source_first < primary_first):
            target['first_seen'] = source_first

    def _merge_event_record(self, target: Dict[str, Any], source: Dict[str, Any]):
        """å°†sourceäº‹ä»¶ä¿¡æ¯åˆå¹¶åˆ°targetï¼Œä¸åˆ é™¤èŠ‚ç‚¹"""
        if not source:
            return
        # sources
        primary_sources = set()
        for s in target.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        for s in source.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        target['sources'] = list(primary_sources)

        # entities union
        ents = set(target.get('entities', []))
        ents.update(source.get('entities', []))
        target['entities'] = list(ents)

        # first_seen å–æœ€æ—©
        primary_first = target.get('first_seen', '')
        source_first = source.get('first_seen', '')
        if source_first and (not primary_first or source_first < primary_first):
            target['first_seen'] = source_first

        # event_summary è‹¥ç¼ºå¤±åˆ™è¡¥
        if not target.get('event_summary') and source.get('event_summary'):
            target['event_summary'] = source['event_summary']

    def _load_agent3_settings(self) -> Dict[str, Any]:
        """
        åŠ è½½agent3ç›¸å…³é…ç½®ï¼Œè‹¥æ— é…ç½®åˆ™ä½¿ç”¨é»˜è®¤å€¼ã€‚
        """
        defaults = {
            "entity_batch_size": 80,
            "event_batch_size": 15,
            "event_bucket_days": 7,
            "event_bucket_entity_overlap": 1,
            "event_bucket_max_size": 40,
            "event_precluster_similarity": 0.82,
            "event_precluster_limit": 300,
            "entity_precluster_similarity": 0.93,
            "entity_precluster_limit": 500,
            "max_summary_chars": 360,
            "entity_max_workers": 3,
            "event_max_workers": 3,
            "rate_limit_per_sec": 1.0,
            "entity_evidence_per_entity": 2,
            "entity_evidence_max_chars": 400,
        }
        config_file = self.tools.CONFIG_DIR / "config.yaml"
        if yaml and config_file.exists():
            try:
                with open(config_file, "r", encoding="utf-8") as f:
                    data = yaml.safe_load(f) or {}
                    cfg = data.get("agent3_config", {})
                    if isinstance(cfg, dict):
                        for k, v in cfg.items():
                            if k in defaults:
                                defaults[k] = v
            except Exception as e:
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½agent3é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        else:
            if not yaml:
                self.tools.log("[çŸ¥è¯†å›¾è°±] âš ï¸ æœªå®‰è£… PyYAMLï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return defaults

    def _string_similarity(self, a: str, b: str) -> float:
        """å­—ç¬¦ä¸²ç›¸ä¼¼åº¦(0-1)"""
        return SequenceMatcher(None, a.lower(), b.lower()).ratio()

    def _is_chinese(self, text: str) -> bool:
        return any('\u4e00' <= ch <= '\u9fff' for ch in text)

    def _entity_type(self, name: str) -> str:
        """
        è½»é‡ç±»å‹åˆ¤å®šï¼šgeo/org/person/product/unknown
        ä»…ç”¨äºé˜»æ–­è·¨ç±»å‹åˆå¹¶ï¼Œå®å¯ unknownã€‚
        """
        n = name or ""
        geo_kw = ["å¸‚", "çœ", "å·", "å¿", "åŒº", "é•‡", "ä¹¡", "éƒ¡", "å²›", "åºœ", "é“", "è‡ªæ²»", "å…±å’Œå›½", "ç‹å›½", "ç‰¹åŒº", "è‡ªæ²»åŒº"]
        org_kw = ["å…¬å¸", "é›†å›¢", "é“¶è¡Œ", "æ”¿åºœ", "éƒ¨", "å±€", "ç½²", "é™¢", "å…", "è¡Œ", "å…š", "æœºæ„", "æ³•é™¢", "æ£€å¯Ÿé™¢", "å§”å‘˜ä¼š", "ç»„ç»‡", "è”ç›Ÿ", "ç†äº‹ä¼š", "åä¼š", "åŸºé‡‘ä¼š", "å¤§å­¦", "å­¦é™¢", "å­¦æ ¡", "å·¥å‚", "å‚", "æŠ¥", "ç”µè§†", "æ–°é—»", "æ—¥æŠ¥", "æ™šæŠ¥", "å‘¨æŠ¥", "ç¤¾"]
        product_kw = ["ç³»åˆ—", "ç‰ˆ", "å‹", "å‹å·", "Pro", "Ultra"]
        if any(k in n for k in geo_kw):
            return "geo"
        if any(k in n for k in org_kw):
            return "org"
        if " " in n or "Â·" in n:
            return "person"
        if any(k in n for k in product_kw):
            return "product"
        return "unknown"

    def _valid_entity_group(self, group: List[str]) -> bool:
        """è·¨ç±»å‹åˆå¹¶æ‹¦æˆªï¼šè‹¥æ··åˆ geo/org/person/product åˆ™æ‹’ç»"""
        types = set()
        for name in group:
            t = self._entity_type(name)
            if t != "unknown":
                types.add(t)
        # å¦‚æœæ£€æµ‹åˆ°å¤šç§å·²çŸ¥ç±»å‹åˆ™è§†ä¸ºé«˜é£é™©
        if len(types) > 1:
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ è·¨ç±»å‹åˆå¹¶è¢«é˜»æ­¢: {group} | types={types}")
            return False
        return True

    def _collect_entity_evidence(self, entities_batch: List[str]) -> Dict[str, List[str]]:
        """
        ä¸ºå®ä½“æ‰¹æ¬¡æ”¶é›†ç›¸å…³äº‹ä»¶æ‘˜è¦+å®ä½“ï¼Œå‡å°‘å¹»è§‰ã€‚
        """
        per_entity = int(self.settings.get("entity_evidence_per_entity", 2))
        max_chars = int(self.settings.get("entity_evidence_max_chars", 400))
        evidence: Dict[str, List[str]] = {e: [] for e in entities_batch}
        for abstract, event in self.graph['events'].items():
            ents = event.get('entities', [])
            summary = self._trim_text(event.get('event_summary', "") or abstract, max_chars)
            for e in ents:
                if e in evidence and len(evidence[e]) < per_entity:
                    evidence[e].append(f"{abstract} | {', '.join(ents)} | {summary}")
        return evidence

    def _trim_text(self, text: str, max_chars: int) -> str:
        """æ§åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿"""
        if not text or max_chars <= 0:
            return text
        if len(text) <= max_chars:
            return text
        return text[:max_chars] + "..."

    def _parse_time(self, ts: str) -> float:
        """å°½é‡è§£ææ—¶é—´æˆ³ï¼Œå¤±è´¥è¿”å›0"""
        if not ts:
            return 0
        try:
            # æ”¯æŒISOå­—ç¬¦ä¸²
            return datetime.fromisoformat(ts.replace("Z", "+00:00")).timestamp()
        except Exception:
            try:
                return time.mktime(time.strptime(ts, "%Y-%m-%d %H:%M:%S"))
            except Exception:
                return 0

    def _bucket_events_by_time_and_entity(
        self,
        window_days: int,
        min_entity_overlap: int,
        max_bucket_size: int
    ) -> List[Dict[str, Any]]:
        """
        æŒ‰æ—¶é—´çª—å£ä¸å®ä½“äº¤é›†åˆ†æ¡¶äº‹ä»¶ï¼Œå‡å°‘è·¨æœŸ/è·¨ä¸»ä½“æ··æ‚ã€‚
        """
        events_items = list(self.graph["events"].items())
        window_sec = window_days * 86400

        # é¢„æ’åºï¼Œæ—¶é—´ç¼ºå¤±æ”¾æœ«å°¾
        def _sort_key(item):
            ts = self._parse_time(item[1].get("first_seen", ""))
            return ts if ts > 0 else float("inf")

        events_items.sort(key=_sort_key)

        buckets: List[Dict[str, Any]] = []
        for abstract, event in events_items:
            entities = set(event.get("entities", []))
            ts = self._parse_time(event.get("first_seen", ""))
            placed = False
            for bucket in buckets:
                if len(bucket["keys"]) >= max_bucket_size:
                    continue
                # æ—¶é—´çª—å£åˆ¤å®š
                if bucket["min_time"] and ts and ts < bucket["min_time"] - window_sec:
                    continue
                if bucket["max_time"] and ts and ts > bucket["max_time"] + window_sec:
                    continue
                # å®ä½“äº¤é›†åˆ¤å®š
                if min_entity_overlap > 0:
                    if not entities or len(bucket["entities"].intersection(entities)) < min_entity_overlap:
                        continue
                # å‘½ä¸­ï¼ŒåŠ å…¥æ¡¶
                bucket["keys"].append(abstract)
                bucket["entities"].update(entities)
                if ts:
                    bucket["min_time"] = min(bucket["min_time"] or ts, ts)
                    bucket["max_time"] = max(bucket["max_time"] or ts, ts)
                placed = True
                break
            if not placed:
                buckets.append({
                    "keys": [abstract],
                    "entities": set(entities),
                    "min_time": ts if ts else None,
                    "max_time": ts if ts else None
                })
        self.tools.log(f"[çŸ¥è¯†å›¾è°±] äº‹ä»¶åˆ†æ¡¶å®Œæˆï¼Œå…± {len(buckets)} ä¸ªæ¡¶")
        return buckets

    def _precluster_entities_by_string(self, entities: List[str], threshold: float, limit: int) -> List[List[str]]:
        """
        åŸºäºå­—ç¬¦ä¸²ç›¸ä¼¼åº¦çš„è½»é‡é¢„èšç±»ï¼Œé¿å…LLMè¿‡é‡è¾“å…¥ã€‚
        """
        if len(entities) == 0 or len(entities) > limit:
            return []
        res = []
        used = set()
        for i, ent in enumerate(entities):
            if ent in used:
                continue
            group = [ent]
            used.add(ent)
            for other in entities[i+1:]:
                if other in used:
                    continue
                if self._string_similarity(ent, other) >= threshold:
                    group.append(other)
                    used.add(other)
            if len(group) > 1:
                res.append(group)
        if res:
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] æœ¬åœ°å®ä½“é¢„èšç±»å‘ç° {len(res)} ç»„å¯èƒ½é‡å¤")
        return res

    def _precluster_events_by_string(
        self,
        events_map: Dict[str, Dict[str, Any]],
        keys: List[str],
        threshold: float,
        limit: int,
        max_summary_chars: int
    ) -> List[List[str]]:
        """
        åŒæ¡¶äº‹ä»¶çš„å­—ç¬¦ä¸²è¿‘ä¼¼èšç±»ï¼Œå‡å°‘LLMè´Ÿæ‹…ã€‚
        """
        if len(keys) == 0 or len(keys) > limit:
            return []

        def norm_text(k: str) -> str:
            evt = events_map.get(k, {})
            summary = self._trim_text(evt.get("event_summary", "") or "", max_summary_chars)
            return (k + " " + summary).lower()

        texts = {k: norm_text(k) for k in keys}
        res = []
        used = set()
        for i, key in enumerate(keys):
            if key in used:
                continue
            base = texts.get(key, "")
            group = [key]
            used.add(key)
            for other in keys[i+1:]:
                if other in used:
                    continue
                if self._string_similarity(base, texts.get(other, "")) >= threshold:
                    group.append(other)
                    used.add(other)
            if len(group) > 1:
                res.append(group)
        if res:
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] æœ¬åœ°äº‹ä»¶é¢„èšç±»å‘ç° {len(res)} ç»„å¯èƒ½é‡å¤")
        return res

    def _apply_merge_rules(self) -> bool:
        """
        åº”ç”¨æœ¬åœ°åˆå¹¶è§„åˆ™
        è¿”å›: æ˜¯å¦æœ‰æ›´æ–°
        """
        updated = False
        if not self.merge_rules:
            return False
            
        # éå†ç°æœ‰å®ä½“ï¼Œçœ‹æ˜¯å¦åŒ¹é…è§„åˆ™
        # æ³¨æ„ï¼šéœ€è¦åœ¨éå†æ—¶å¤„ç†ï¼Œé¿å…å­—å…¸å¤§å°å˜åŒ–é—®é¢˜ï¼Œé€šå¸¸æ”¶é›†åå†å¤„ç†
        to_merge = []
        for entity in list(self.graph['entities'].keys()):
            if entity in self.merge_rules:
                target = self.merge_rules[entity]
                # åªæœ‰å½“ç›®æ ‡å®ä½“ä¹Ÿå­˜åœ¨ï¼Œæˆ–è€…ç›®æ ‡å°±æ˜¯æˆ‘ä»¬æƒ³è¦ç»Ÿä¸€åˆ°çš„åç§°æ—¶ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºå¦‚æœç›®æ ‡åœ¨åº“é‡Œæˆ–æˆ‘ä»¬å†³å®šæ”¹åï¼‰
                # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾è§„åˆ™æ˜¯ A -> Bï¼Œå¦‚æœ A å­˜åœ¨ï¼Œå°±å°è¯•åˆå¹¶åˆ° Bã€‚
                # å¦‚æœ B ä¸åœ¨åº“é‡Œï¼Œå°±æŠŠ A é‡å‘½åä¸º Bã€‚
                if target != entity:
                    to_merge.append((target, entity))
        
        for primary, duplicate in to_merge:
            # å¦‚æœç›®æ ‡å®ä½“ä¸å­˜åœ¨ï¼Œå…ˆé‡å‘½å
            if primary not in self.graph['entities'] and duplicate in self.graph['entities']:
                self.graph['entities'][primary] = self.graph['entities'][duplicate]
                del self.graph['entities'][duplicate]
                # æ›´æ–°äº‹ä»¶å¼•ç”¨
                for abstract, event in self.graph['events'].items():
                    entities = event.get('entities', [])
                    if duplicate in entities:
                        event['entities'] = [primary if e == duplicate else e for e in entities]
                self.tools.log(f"[çŸ¥è¯†å›¾è°±][è§„åˆ™] é‡å‘½åå®ä½“: {duplicate} -> {primary}")
                updated = True
            elif primary in self.graph['entities'] and duplicate in self.graph['entities']:
                # å¦‚æœéƒ½å­˜åœ¨ï¼Œåˆ™åˆå¹¶
                self._merge_entities(primary, duplicate)
                updated = True
                
        return updated

    def compress_with_llm(self) -> Dict[str, List[str]]:
        """
        ä½¿ç”¨LLMåˆ†æå‹ç¼©çŸ¥è¯†å›¾è°±ï¼Œè¾“å‡ºé‡å¤çš„å®ä½“å’Œäº‹ä»¶æŠ½è±¡ã€‚
        åˆ†æ‰¹å¤„ç†ä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é•¿ã€‚
        é›†æˆè§„åˆ™ä¼˜å…ˆç­–ç•¥ã€‚
        """
        # 0. é¦–å…ˆåº”ç”¨æœ¬åœ°è§„åˆ™
        rule_applied = self._apply_merge_rules()
        if rule_applied:
            self._save_data() # è§„åˆ™åº”ç”¨åå…ˆä¿å­˜ä¸€æ¬¡çŠ¶æ€
            
        self._init_llm_pool()
        if self.llm_pool is None:
            self.tools.log("[çŸ¥è¯†å›¾è°±] âŒ LLMä¸å¯ç”¨ï¼Œè·³è¿‡å‹ç¼©")
            return {"duplicate_entities": [], "duplicate_events": []}
        
        # å¹¶å‘ä¸é€Ÿç‡æ§åˆ¶
        entity_workers = int(self.settings.get("entity_max_workers", 3))
        event_workers = int(self.settings.get("event_max_workers", 3))
        rate_limit = float(self.settings.get("rate_limit_per_sec", 1.0))
        limiter = RateLimiter(rate_limit) if rate_limit > 0 else None

        all_duplicate_entities = []
        all_duplicate_events = []
        
        # 1. å¤„ç†å®ä½“ (åˆ†æ‰¹)
        # æ’åºä»¥å¢åŠ ç›¸ä¼¼å®ä½“ç›¸é‚»çš„æ¦‚ç‡
        entities_list = sorted(list(self.graph['entities'].keys()))
        BATCH_SIZE_ENT = int(self.settings.get("entity_batch_size", 80))
        precluster_entities = self._precluster_entities_by_string(
            entities_list,
            threshold=float(self.settings.get("entity_precluster_similarity", 0.93)),
            limit=int(self.settings.get("entity_precluster_limit", 500))
        )
        if precluster_entities:
            all_duplicate_entities.extend(precluster_entities)
        
        entity_batches = []
        for i in range(0, len(entities_list), BATCH_SIZE_ENT):
            entity_batches.append((i // BATCH_SIZE_ENT, entities_list[i:i+BATCH_SIZE_ENT]))

        def _run_entity_batch(idx: int, batch: List[str]) -> List[List[str]]:
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] å¤„ç†å®ä½“æ‰¹æ¬¡ {idx+1}/{len(entity_batches)} (å¤§å°: {len(batch)})")
            prompt = self._prepare_entity_compression_prompt_strict(batch)
            response = self._call_llm_limited(prompt, timeout=90, limiter=limiter)
            return self._parse_entity_response(response) if response else []

        new_rules_count = 0
        if entity_batches:
            with ThreadPoolExecutor(max_workers=entity_workers) as executor:
                futures = [executor.submit(_run_entity_batch, idx, batch) for idx, batch in entity_batches]
                for fut in as_completed(futures):
                    batch_dupes = fut.result() or []
                    if batch_dupes:
                        for group in batch_dupes:
                            if len(group) >= 2 and self._valid_entity_group(group):
                                primary, dupes = self._choose_primary_entity(group)
                                for duplicate in dupes:
                                    if duplicate not in self.merge_rules:
                                        self.merge_rules[duplicate] = primary
                                        new_rules_count += 1
                                all_duplicate_entities.append([primary] + dupes)
                    else:
                        all_duplicate_entities.extend(batch_dupes)
        if new_rules_count > 0:
            self._save_merge_rules()
                
        # 2. å¤„ç†äº‹ä»¶ (åˆ†æ‰¹)
        events_list = sorted(list(self.graph['events'].keys()))
        if not events_list:
            return {
                "duplicate_entities": all_duplicate_entities, 
                "duplicate_events": all_duplicate_events
            }

        BATCH_SIZE_EVT = int(self.settings.get("event_batch_size", 15))
        bucket_days = int(self.settings.get("event_bucket_days", 7))
        bucket_overlap = int(self.settings.get("event_bucket_entity_overlap", 1))
        bucket_max_size = int(self.settings.get("event_bucket_max_size", 40))
        evt_similarity = float(self.settings.get("event_precluster_similarity", 0.82))
        evt_limit = int(self.settings.get("event_precluster_limit", 300))
        max_summary_chars = int(self.settings.get("max_summary_chars", 360))

        buckets = self._bucket_events_by_time_and_entity(bucket_days, bucket_overlap, bucket_max_size)

        def _run_event_bucket(idx: int, bucket: Dict[str, Any]) -> List[List[str]]:
            bucket_keys = bucket.get("keys", [])
            bucket_events = {k: self.graph['events'][k] for k in bucket_keys if k in self.graph['events']}
            local_dupes: List[List[str]] = []

            pre_clusters = self._precluster_events_by_string(
                bucket_events,
                bucket_keys,
                threshold=evt_similarity,
                limit=evt_limit,
                max_summary_chars=max_summary_chars
            )
            if pre_clusters:
                local_dupes.extend(pre_clusters)

            if len(bucket_keys) <= 1:
                self.tools.log(f"[çŸ¥è¯†å›¾è°±] è·³è¿‡äº‹ä»¶æ¡¶ {idx+1}/{len(buckets)}ï¼ˆä»…1æ¡ï¼Œæ— éœ€å»é‡ï¼‰")
                return local_dupes

            total_batches = (len(bucket_keys) - 1) // BATCH_SIZE_EVT + 1
            for i in range(0, len(bucket_keys), BATCH_SIZE_EVT):
                batch_keys = bucket_keys[i:i+BATCH_SIZE_EVT]
                batch_events = {
                    k: {
                        **bucket_events.get(k, {}),
                        "event_summary": self._trim_text(
                            bucket_events.get(k, {}).get("event_summary", "") or "",
                            max_summary_chars
                        )
                    }
                    for k in batch_keys
                }
                self.tools.log(
                    f"[çŸ¥è¯†å›¾è°±] å¤„ç†äº‹ä»¶æ¡¶ {idx+1}/{len(buckets)} çš„æ‰¹æ¬¡ {i//BATCH_SIZE_EVT + 1}/{total_batches} (å¤§å°: {len(batch_keys)})"
                )
                prompt = self._prepare_event_compression_prompt(batch_events)
                response = self._call_llm_limited(prompt, timeout=120, limiter=limiter)
                if response:
                    batch_dupes = self._parse_event_response(response)
                    local_dupes.extend(batch_dupes)
            return local_dupes

        if buckets:
            with ThreadPoolExecutor(max_workers=event_workers) as executor:
                futures = [executor.submit(_run_event_bucket, idx, bucket) for idx, bucket in enumerate(buckets)]
                for fut in as_completed(futures):
                    batch_dupes = fut.result() or []
                    all_duplicate_events.extend(batch_dupes)

        return {
            "duplicate_entities": all_duplicate_entities, 
            "duplicate_events": all_duplicate_events
        }

    def _call_llm(self, prompt: str, timeout: int) -> Optional[str]:
        """ç»Ÿä¸€LLMè°ƒç”¨"""
        return self.llm_pool.call(
            prompt=prompt,
            max_tokens=4000,
            timeout=timeout,
            retries=2
        )

    def _call_llm_limited(self, prompt: str, timeout: int, limiter: Optional[RateLimiter]) -> Optional[str]:
        """å¸¦å…¨å±€QPSé™åˆ¶çš„LLMè°ƒç”¨"""
        if limiter:
            limiter.acquire()
        return self._call_llm(prompt, timeout)

    def _choose_primary_entity(self, group: List[str]) -> (str, List[str]):
        """
        é€‰æ‹©ä¸»å®ä½“ï¼šä¼˜å…ˆä¸­æ–‡ï¼Œå…¶æ¬¡ first_seen æœ€æ—©ï¼Œå†å…¶æ¬¡åç§°é•¿åº¦ã€‚
        è¿”å› (primary, duplicates)
        """
        if not group:
            return "", []
        best = None
        best_key = None
        for name in group:
            info = self.graph['entities'].get(name, {})
            is_cn = self._is_chinese(name)
            ts = self._parse_time(info.get('first_seen', ''))
            ts_key = ts if ts > 0 else float('inf')
            key = (0 if is_cn else 1, ts_key, len(name))
            if best_key is None or key < best_key:
                best = name
                best_key = key
        duplicates = [n for n in group if n != best]
        return best, duplicates

    def _prepare_entity_compression_prompt_strict(self, entities_batch: List[str]) -> str:
        evidence_map = self._collect_entity_evidence(entities_batch)
        evidence_lines = []
        for ent, evs in evidence_map.items():
            if evs:
                for ev in evs:
                    evidence_lines.append(f"{ent} <= {ev}")

        prompt = f"""ä½ æ˜¯ä¸€åçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ä»»åŠ¡ï¼šä»…åœ¨æœ‰å……åˆ†è¯æ®æ—¶è®¤å®šå®ä½“ä¸ºåŒä¸€ä¸»ä½“ï¼ˆåˆ«å/ç¼©å†™/ä¸­è‹±æ–‡/æ³•å®šå…¨ç§°å·®å¼‚ï¼‰ã€‚ä¸è¦å› ä¸ºè¡Œä¸šç›¸ä¼¼ã€ä¸Šä¸‹çº§å…³ç³»æˆ–åœ°åŸŸç›¸ä¼¼è€Œåˆå¹¶ã€‚

ã€é«˜é£é™©è¯¯åˆ¤ç¤ºä¾‹ã€‘
- å®ä½“å…·æœ‰ç‰¹åŒ–èŒèƒ½æˆ–åŠŸæ•ˆæˆ–ç”¨é€”ï¼Œä¸å¯åˆå¹¶
- è¡Œä½¿èŒèƒ½çš„ç»„ç»‡ã€æœºæ„ä¸å…¶ä¸‹è¾–çš„æ›´å…·ä½“èŒèƒ½çš„ç»„ç»‡ã€æœºæ„ä¸å¯åˆå¹¶
- â€œå¤§å­¦â€ ä¸ â€œè”ç›Ÿ/åä¼š/éƒ¨é—¨/å¤®è¡Œâ€ ä¸æ˜¯åŒä¸€ä¸»ä½“
- ä¸åŒå›½å®¶/åœ°åŒºçš„åŒåæœºæ„ï¼Œä¸å¯åˆå¹¶
- ä¸Šå¸‚å…¬å¸ vs å­å…¬å¸/æ§è‚¡è‚¡ä¸œï¼Œä¸å¯åˆå¹¶
- æ”¿åºœéƒ¨é—¨ vs ä¸Šçº§æ”¿åºœï¼Œä¸å¯åˆå¹¶
- å›½å®¶/çœå·/åŸå¸‚/åŒºå¿ ä¹‹é—´ä¸å¾—äº’å¹¶ï¼Œä¹Ÿä¸å¾—è·¨å›½åˆå¹¶
- å…¬å¸/æœºæ„ â‰  äº§å“/å“ç‰Œ/å‹å·ï¼Œä¸å¾—äº’å¹¶
- äººå â‰  å…¬å¸/æœºæ„/åœ°ç†/äº§å“ï¼Œä¸å¾—äº’å¹¶
- åª’ä½“åç§° â‰  åœ°ç‚¹/æ”¿åºœ/ä¼ä¸š/äººå
- ä½“è‚²ä¿±ä¹éƒ¨/èµ›äº‹ â‰  åŸå¸‚/å›½å®¶/æ”¿åºœ/ä¸ªäºº

ã€å®ä½“åˆ—è¡¨ã€‘
{json.dumps(entities_batch, ensure_ascii=False, indent=2)}

ã€è¯æ®ï¼ˆéƒ¨åˆ†ç›¸å…³äº‹ä»¶æ‘˜è¦ï¼Œä¾›å‚è€ƒï¼Œé¿å…å¹»è§‰ï¼‰ã€‘
æ ¼å¼: å®ä½“ <= æ‘˜è¦ | å‚ä¸å®ä½“ | æè¿°
{chr(10).join(evidence_lines) if evidence_lines else "ï¼ˆæ— å¯ç”¨äº‹ä»¶ï¼Œè°¨æ…åˆå¹¶ï¼‰"}

ã€è¦æ±‚ã€‘
- ä¸»å®ä½“ä¼˜å…ˆæ›´é€šç”¨ï¼ˆæˆ–æ›´å€¾å‘ä¸­å›½äººè¡¨è¾¾ï¼‰ã€æ›´è¯¦ç»†ã€æ›´ç²¾ç¡®ã€æ›´å­¦æœ¯ï¼ˆâ€œæ›´XXâ€æŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰
- åªè¾“å‡ºç¡®å®šä¸ºåŒä¸€ä¸»ä½“çš„ç»„åˆï¼›ä¸ç¡®å®šå°±è¿”å›ç©ºã€‚
- ä¼˜å…ˆä¸¥æ ¼åŒ¹é…ï¼šåŒåã€æ˜æ˜¾è¯‘åã€ç¼©å†™å±•å¼€ã€‚
- ä¸è¦æ”¹å†™åç§°æ ¼å¼ï¼ˆä¸è¦æ·»åŠ ä¹¦åå·/å¼•å·/æ‹¬å·ç­‰æ ‡ç‚¹ï¼‰ã€‚
- åœ°ç†/æœºæ„ç±»åˆå¹¶éœ€åŒä¸€å›½å®¶/è¡Œæ”¿å±‚çº§ï¼›äººåä¸å¾—ä¸éäººååˆå¹¶ï¼›å…¬å¸ä¸å¾—ä¸äº§å“/å“ç‰Œåˆå¹¶ã€‚
- å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼š
{{
  "duplicate_entities": [
    ["ä¸»å®ä½“", "åˆ«åæˆ–é‡å¤"],
    ["ä¸»å®ä½“2", "åˆ«å2", "åˆ«å3"]
  ]
}}
å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å› {{ "duplicate_entities": [] }}ã€‚åªè¾“å‡ºJSONã€‚
"""
        return prompt

    def _prepare_event_compression_prompt(self, events_batch: Dict) -> str:
        prompt = f"""ä½ æ˜¯ä¸€åçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ä»»åŠ¡ï¼šä»…åœ¨æè¿°â€œåŒä¸€å…·ä½“äº‹å®â€æ—¶æ‰è§†ä¸ºé‡å¤äº‹ä»¶ã€‚ä¸è¦åˆå¹¶ä¸åŒä¸»ä½“ã€ä¸Šä¸‹æ¸¸å…³è”æˆ–æ—¶é—´ä¸åŒçš„ç›¸ä¼¼äº‹ä»¶ã€‚

ã€æ‹’ç»åˆå¹¶çš„æƒ…å†µç¤ºä¾‹ã€‘
- è¡Œä¸šç›¸ä¼¼ä½†ä¸»ä½“ä¸åŒçš„äº‹ä»¶
- ä¸Šæ¸¸/ä¸‹æ¸¸/ç›‘ç®¡/è”ç›Ÿå…³ç³» â‰  åŒä¸€äº‹ä»¶
- æ—¶é—´é—´éš”æ˜æ˜¾ä¸åŒçš„å¤šæ¬¡äº‹ä»¶
- äº‹ä»¶å…·æœ‰è¿ç»­å‘ç”Ÿæˆ–ä¸€å‰ä¸€åçš„å…³ç³»
- å¯¹äºç›¸åŒå®ä½“ï¼Œä¸åŒæ—¶é—´ç‚¹å‘ç”Ÿçš„äº‹ä»¶

ã€äº‹ä»¶åˆ—è¡¨ã€‘
æ ¼å¼: æ‘˜è¦ | å‚ä¸å®ä½“ | äº‹ä»¶æè¿°
"""
        for abstract, event in events_batch.items():
            entities = event.get('entities', [])
            summary = event.get('event_summary', '')
            prompt += f"{abstract} | {', '.join(entities)} | {summary}\n"

        prompt += """
ã€ä»»åŠ¡ã€‘
æ‰¾å‡ºè¯­ä¹‰ä¸Šé«˜åº¦é‡å ã€æè¿°åŒä¸€äº‹å®çš„äº‹ä»¶ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼š
{
  "duplicate_events": [
    ["äº‹ä»¶æ‘˜è¦1", "äº‹ä»¶æ‘˜è¦2"],
    ["äº‹ä»¶æ‘˜è¦3", "äº‹ä»¶æ‘˜è¦4", "äº‹ä»¶æ‘˜è¦5"]
  ]
}
å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å› { "duplicate_events": [] }ã€‚åªè¾“å‡ºJSONã€‚
"""
        return prompt

    def _parse_entity_response(self, raw_content: str) -> List[List[str]]:
        try:
            data = self._extract_json(raw_content)
            res = data.get("duplicate_entities", [])
            return res if isinstance(res, list) else []
        except Exception:
            return []

    def _parse_event_response(self, raw_content: str) -> List[List[str]]:
        try:
            data = self._extract_json(raw_content)
            res = data.get("duplicate_events", [])
            return res if isinstance(res, list) else []
        except Exception:
            return []

    def _extract_json(self, text: str) -> Dict:
        if "```json" in text:
            text = text.split("```json", 1)[1].split("```")[0]
        elif "```" in text:
            text = text.split("```", 1)[1].split("```")[0]
        return json.loads(text)
    
    # æ—§æ–¹æ³•ä¿ç•™æˆ–åˆ é™¤ï¼ˆè¿™é‡Œæ›¿æ¢æ—§çš„ _prepare_compression_prompt å’Œ _parse_llm_responseï¼‰
    
    def update_entities_and_events(self, duplicates: Dict[str, List[List[str]]]):
        """æ ¹æ®é‡å¤æ£€æµ‹ç»“æœæ›´æ–°å®ä½“åº“å’Œäº‹ä»¶åº“"""
        updated = False
        
        # åˆå¹¶é‡å¤å®ä½“
        for group in duplicates.get("duplicate_entities", []):
            if len(group) < 2:
                continue
            if not self._valid_entity_group(group):
                continue
            primary, dupes = self._choose_primary_entity(group)
            # ç¡®ä¿ä¸»å®ä½“å­˜åœ¨ï¼›è‹¥ä¸å­˜åœ¨ä½†æœ‰é‡å¤å®ä½“å­˜åœ¨ï¼Œå¯äº¤æ¢
            if primary not in self.graph['entities'] and dupes:
                for d in dupes:
                    if d in self.graph['entities']:
                        primary, dupes = d, [x for x in group if x != d]
                        break
            for duplicate in dupes:
                if duplicate in self.graph['entities'] and primary in self.graph['entities']:
                    self._merge_entities(primary, duplicate)
                    updated = True
        
        # åˆå¹¶é‡å¤äº‹ä»¶
        for group in duplicates.get("duplicate_events", []):
            if len(group) < 2:
                continue
            primary = group[0]
            for duplicate in group[1:]:
                if duplicate in self.graph['events']:
                    self._merge_events(primary, duplicate)
                    updated = True
        
        if updated:
            self._save_data()
            self.tools.log("[çŸ¥è¯†å›¾è°±] å®ä½“å’Œäº‹ä»¶æ›´æ–°å®Œæˆ")
        else:
            self.tools.log("[çŸ¥è¯†å›¾è°±] æ— é‡å¤é¡¹éœ€è¦æ›´æ–°")
    
    def _merge_entities(self, primary: str, duplicate: str):
        """åˆå¹¶é‡å¤å®ä½“"""
        if primary not in self.graph['entities'] or duplicate not in self.graph['entities']:
            return
        
        primary_data = self.graph['entities'][primary]
        duplicate_data = self.graph['entities'][duplicate]
        
        # åˆå¹¶sources (ç¡®ä¿è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„tupleæˆ–ç›´æ¥åˆ—è¡¨å¤„ç†)
        primary_sources = set()
        for s in primary_data.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue # æš‚æ—¶å¿½ç•¥å¤æ‚ç»“æ„
            else: primary_sources.add(s)
            
        for s in duplicate_data.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue 
            else: primary_sources.add(s)
            
        # è½¬å›list
        primary_data['sources'] = list(primary_sources)
        
        # åˆå¹¶original_forms
        primary_forms = set()
        for f in primary_data.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)
            
        for f in duplicate_data.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)

        # å°†é‡å¤å®ä½“åä¹Ÿä½œä¸ºä¸»å®ä½“çš„å…¶ä»–è¡¨è¿°è®°å½•ï¼Œé˜²æ­¢ä¸¢å¤±åˆ«å
        primary_forms.add(duplicate)
        primary_forms.add(primary)
            
        primary_data['original_forms'] = list(primary_forms)
        
        # æ›´æ–°first_seenä¸ºæ›´æ—©çš„æ—¶é—´
        primary_first = primary_data.get('first_seen', '')
        duplicate_first = duplicate_data.get('first_seen', '')
        if duplicate_first and (not primary_first or duplicate_first < primary_first):
            primary_data['first_seen'] = duplicate_first
        
        # åˆ é™¤é‡å¤å®ä½“
        del self.graph['entities'][duplicate]
        
        # æ›´æ–°äº‹ä»¶ä¸­çš„å®ä½“å¼•ç”¨
        for abstract, event in self.graph['events'].items():
            entities = event.get('entities', [])
            if duplicate in entities:
                # æ›¿æ¢ä¸ºprimaryï¼Œå¹¶å»é‡
                new_entities = [primary if ent == duplicate else ent for ent in entities]
                # å»é‡
                unique_entities = []
                seen = set()
                for ent in new_entities:
                    if ent not in seen:
                        seen.add(ent)
                        unique_entities.append(ent)
                event['entities'] = unique_entities
        
        self.tools.log(f"[çŸ¥è¯†å›¾è°±] åˆå¹¶å®ä½“: {duplicate} -> {primary}")
    
    def _merge_events(self, primary: str, duplicate: str):
        """åˆå¹¶é‡å¤äº‹ä»¶"""
        if primary not in self.graph['events'] or duplicate not in self.graph['events']:
            return
        
        primary_event = self.graph['events'][primary]
        duplicate_event = self.graph['events'][duplicate]
        
        # åˆå¹¶sources
        primary_sources = set()
        for s in primary_event.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
            
        for s in duplicate_event.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
            
        primary_event['sources'] = list(primary_sources)
        
        # åˆå¹¶entities
        primary_entities = set(primary_event.get('entities', []))
        duplicate_entities = set(duplicate_event.get('entities', []))
        primary_event['entities'] = list(primary_entities.union(duplicate_entities))
        
        # æ›´æ–°first_seen
        primary_first = primary_event.get('first_seen', '')
        duplicate_first = duplicate_event.get('first_seen', '')
        if duplicate_first and (not primary_first or duplicate_first < primary_first):
            primary_event['first_seen'] = duplicate_first
        
        # äº‹ä»¶æè¿°åˆå¹¶ï¼šä¿ç•™æ›´è¯¦ç»†çš„
        if not primary_event.get('event_summary') and duplicate_event.get('event_summary'):
            primary_event['event_summary'] = duplicate_event['event_summary']
        
        # åˆ é™¤é‡å¤äº‹ä»¶
        del self.graph['events'][duplicate]
        
        self.tools.log(f"[çŸ¥è¯†å›¾è°±] åˆå¹¶äº‹ä»¶: {duplicate} -> {primary}")
    
    def _save_data(self):
        """ä¿å­˜æ›´æ–°åçš„æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # ä¿å­˜å®ä½“
            with open(self.entities_file, 'w', encoding='utf-8') as f:
                json.dump(self.graph['entities'], f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜äº‹ä»¶ï¼ˆabstract_mapæ ¼å¼ï¼‰
            abstract_map = {}
            for abstract, event in self.graph['events'].items():
                abstract_map[abstract] = {
                    "entities": event.get('entities', []),
                    "event_summary": event.get('event_summary', ''),
                    "sources": event.get('sources', []),
                    "first_seen": event.get('first_seen', '')
                }
            
            with open(self.abstract_map_file, 'w', encoding='utf-8') as f:
                json.dump(abstract_map, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜çŸ¥è¯†å›¾è°±çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            with open(self.kg_file, 'w', encoding='utf-8') as f:
                json.dump(self.graph, f, ensure_ascii=False, indent=2)
            
            self.tools.log("[çŸ¥è¯†å›¾è°±] æ•°æ®ä¿å­˜å®Œæˆ")
        except Exception as e:
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def append_only_update(self, events_list: List[Dict[str, Any]], default_source: str = "auto_pipeline", allow_append_original_forms: bool = True) -> Dict[str, int]:
        """
        åªè¿½åŠ æ–°æ•°æ®ï¼Œä¸æ”¹åŠ¨å·²æœ‰å®ä½“/äº‹ä»¶çš„æ—§å­—æ®µã€‚
        - å®ä½“å·²å­˜åœ¨ï¼šä¸æ”¹ first_seen/sourcesï¼Œé»˜è®¤ä»…å¯é€‰åœ°è¿½åŠ åŸå§‹è¡¨è¿°
        - äº‹ä»¶å·²å­˜åœ¨ï¼ˆåŒ abstractï¼‰ï¼šè·³è¿‡ï¼Œä¸æ”¹æ—§äº‹ä»¶
        """
        if not events_list:
            return {"added_entities": 0, "added_events": 0}

        if not self.build_graph():
            return {"added_entities": 0, "added_events": 0}

        # å‡†å¤‡ LLM å»é‡æ˜ å°„ï¼ˆä»…æ˜ å°„â€œæ–°å®ä½“â€åˆ°â€œå·²æœ‰å®ä½“â€ï¼Œä¸æ”¹æ—§å®ä½“å­—æ®µï¼‰
        self._init_llm_pool()
        merge_rules = self.merge_rules or {}
        existing_entities = set(self.graph["entities"].keys())

        # æ”¶é›†æ–°å®ä½“é›†åˆ
        new_entities_set = set()
        for ev in events_list:
            ents = ev.get("entities", []) or []
            ents = [merge_rules.get(e, e) for e in ents if e]
            for e in ents:
                if e not in existing_entities:
                    new_entities_set.add(e)

        # æ„å»ºæ˜ å°„ï¼šæ–°å®ä½“ -> å·²æœ‰å®ä½“ï¼ˆLLM åˆ¤æ–­å¯èƒ½åŒåï¼‰
        llm_merge_map: Dict[str, str] = {}
        if self.llm_pool and new_entities_set:
            # åˆ†æ¡¶é™ä½ä¸Šä¸‹æ–‡é•¿åº¦ï¼šæŒ‰é¦–å­—æ¯/å­—ç¬¦åˆ†æ¡¶
            from collections import defaultdict
            from concurrent.futures import ThreadPoolExecutor, as_completed
            buckets = defaultdict(list)
            for ent in new_entities_set:
                prefix = ent[0] if ent else "#"
                buckets[prefix].append(ent)

            llm_lock = threading.Lock()
            max_workers = int(self.settings.get("entity_max_workers", 3)) or 1

            def handle_bucket(prefix: str, bucket_new: List[str]):
                local_map = {}
                # å–åŒå‰ç¼€çš„éƒ¨åˆ†å·²æœ‰å®ä½“åšå¯¹æ¯”ï¼Œé¿å…è¿‡é•¿
                existing_subset = [e for e in existing_entities if e.startswith(prefix)]
                existing_subset = existing_subset[: max(10, min(80, len(existing_subset)))]
                candidates = list(existing_subset) + list(bucket_new)
                if len(candidates) < 2:
                    return local_map
                try:
                    prompt = self._prepare_entity_compression_prompt_strict(candidates)
                    resp = self._call_llm_limited(prompt, timeout=60, limiter=None)
                    groups = self._parse_entity_response(resp) if resp else []
                    for g in groups:
                        if len(g) < 2:
                            continue
                        primary, dupes = self._choose_primary_entity(g)
                        if primary in existing_entities:
                            for d in dupes:
                                if d in new_entities_set:
                                    local_map[d] = primary
                except Exception as e:
                    self.tools.log(f"[çŸ¥è¯†å›¾è°±] è¿½åŠ æ¨¡å¼ LLM å»é‡å¤±è´¥: {e}")
                return local_map

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(handle_bucket, p, b) for p, b in buckets.items()]
                for fut in as_completed(futures):
                    res_map = fut.result() or {}
                    if res_map:
                        with llm_lock:
                            llm_merge_map.update(res_map)

        added_entities = 0
        added_events = 0

        def normalize_entity(name: str) -> str:
            if not name:
                return name
            name = merge_rules.get(name, name)
            return llm_merge_map.get(name, name)

        for ev in events_list:
            ents = ev.get("entities", []) or []
            ents = [normalize_entity(e) for e in ents]
            ents_original = ev.get("entities_original") or ents
            src = ev.get("source", default_source)
            published_at = ev.get("published_at") or ev.get("datetime") or ""

            # è¿½åŠ å®ä½“ï¼ˆä»…å½“ä¸å­˜åœ¨ï¼‰
            for ent, ent_orig in zip(ents, ents_original):
                if not ent:
                    continue
                if ent not in self.graph["entities"]:
                    self.graph["entities"][ent] = {
                        "first_seen": published_at or datetime.utcnow().isoformat(),
                        "sources": [src] if src else [],
                        "original_forms": [ent_orig] if ent_orig else []
                    }
                    added_entities += 1
                else:
                    # ä¸æ”¹æ—§å­—æ®µï¼Œä»…å¯é€‰è¿½åŠ åŸå§‹è¡¨è¿°
                    if allow_append_original_forms and ent_orig:
                        forms = self.graph["entities"][ent].get("original_forms", [])
                        if ent_orig not in forms:
                            forms.append(ent_orig)
                            self.graph["entities"][ent]["original_forms"] = forms

            # è¿½åŠ äº‹ä»¶ï¼ˆä»…å½“æ‘˜è¦ä¸å­˜åœ¨ï¼‰
            abstract = ev.get("abstract")
            if abstract and abstract not in self.graph["events"]:
                self.graph["events"][abstract] = {
                    "abstract": abstract,
                    "entities": ents,
                    "event_summary": ev.get("event_summary", ""),
                    "sources": [src] if src else [],
                    "first_seen": published_at
                }
                added_events += 1

        # é‡å»ºè¾¹å¹¶ä¿å­˜ï¼ˆåªåœ¨æœ‰æ–°å¢æ—¶ï¼‰
        if added_entities or added_events:
            self._build_edges()
            self._save_data()
            self.tools.log(f"[çŸ¥è¯†å›¾è°±] è¿½åŠ æ¨¡å¼å®Œæˆï¼šæ–°å¢å®ä½“ {added_entities}ï¼Œæ–°å¢äº‹ä»¶ {added_events}")
        else:
            self.tools.log("[çŸ¥è¯†å›¾è°±] è¿½åŠ æ¨¡å¼ï¼šæ²¡æœ‰æ–°å¢å®ä½“/äº‹ä»¶")

        return {"added_entities": added_entities, "added_events": added_events}
    
    def refresh_graph(self):
        """åˆ·æ–°çŸ¥è¯†å›¾è°±ï¼šæ„å»ºã€å‹ç¼©ã€æ›´æ–°"""
        self.tools.log("[çŸ¥è¯†å›¾è°±] å¼€å§‹åˆ·æ–°çŸ¥è¯†å›¾è°±")
        
        # æ„å»ºå›¾
        if not self.build_graph():
            self.tools.log("[çŸ¥è¯†å›¾è°±] âŒ æ„å»ºå›¾å¤±è´¥")
            return
        
        # å‹ç¼©ï¼šä½¿ç”¨LLMæ£€æµ‹é‡å¤
        duplicates = self.compress_with_llm()
        
        # æ›´æ–°å®ä½“å’Œäº‹ä»¶
        self.update_entities_and_events(duplicates)
        
        self.tools.log("[çŸ¥è¯†å›¾è°±] çŸ¥è¯†å›¾è°±åˆ·æ–°å®Œæˆ")
        # æ¸…ç†å·²åŠ è½½çš„ä¸´æ—¶æ–‡ä»¶
        self._cleanup_tmp_files()

# å…¨å±€å‡½æ•°ï¼Œä¾›agent1å’Œagent2è°ƒç”¨
def refresh_graph():
    """åˆ·æ–°çŸ¥è¯†å›¾è°±ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰"""
    kg = KnowledgeGraph()
    kg.refresh_graph()

def append_only_update_graph(events_list: List[Dict[str, Any]], default_source: str = "auto_pipeline", allow_append_original_forms: bool = True) -> Dict[str, int]:
    """
    åªè¿½åŠ æ–°äº‹ä»¶/å®ä½“åˆ°ç°æœ‰å›¾è°±ï¼Œä¸ä¿®æ”¹æ—§è®°å½•ã€‚
    """
    kg = KnowledgeGraph()
    return kg.append_only_update(events_list, default_source=default_source, allow_append_original_forms=allow_append_original_forms)

def build_graph() -> bool:
    """æ„å»ºçŸ¥è¯†å›¾è°±"""
    kg = KnowledgeGraph()
    return kg.build_graph()

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    kg = KnowledgeGraph()
    kg.refresh_graph()
