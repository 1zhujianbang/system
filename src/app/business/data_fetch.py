from __future__ import annotations

import asyncio
import json
import time
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Union

import pandas as pd
try:
    import dask.dataframe as dd
    from dask.distributed import Client, as_completed
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False

from ...infra.registry import register_tool
from ...infra.paths import tools as Tools
from ...core import ConfigManager, AsyncExecutor, RateLimiter, get_config_manager, get_llm_pool
from ...domain.data_operations import update_entities, update_abstract_map
from ...adapters.news.fetch_utils import fetch_from_multiple_sources, normalize_news_items
from .extraction import llm_extract_events, NewsDeduplicator, persist_expanded_news_to_tmp
from ...infra.file_utils import safe_unlink_multiple, safe_unlink
from ...domain.data_operations import sanitize_datetime_fields, write_jsonl_file
from ...ports.extraction import NewsItem, FetchConfig
from ...adapters.news.gdelt_adapter import GDELTAdapter

# å¯¼å…¥æ–°é—»APIç®¡ç†å™¨
from ...core import NewsAPIManager


class GDELTDataProcessor:
    """GDELTæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, use_dask: bool = True):
        self._use_dask = use_dask and DASK_AVAILABLE
        self._gdelt_adapter = GDELTAdapter()
        
    async def process_gdelt_data(
        self, 
        config: FetchConfig,
        normalize_entities: bool = True,
        extract_roles: bool = True,
        format_timestamps: bool = True
    ) -> pd.DataFrame:
        """
        å¤„ç†GDELTæ•°æ®çš„ä¸»æ–¹æ³•
        
        Args:
            config: è·å–é…ç½®
            normalize_entities: æ˜¯å¦æ ‡å‡†åŒ–å®ä½“
            extract_roles: æ˜¯å¦æå–è§’è‰²
            format_timestamps: æ˜¯å¦æ ¼å¼åŒ–æ—¶é—´æˆ³
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        # è·å–åŸå§‹æ•°æ®
        raw_data = await self._gdelt_adapter.fetch(config)
        
        if not raw_data.items:
            print("No GDELT data found for the given criteria")
            return pd.DataFrame()
        
        # å°†NewsItemè½¬æ¢ä¸ºDataFrame
        df = self._news_items_to_dataframe(raw_data.items)
        
        # æ•°æ®é¢„å¤„ç†
        df = self._preprocess_data(df)
        
        # å®ä½“æ ‡å‡†åŒ–
        if normalize_entities:
            df = self._normalize_entities(df)
        
        # è§’è‰²æå–
        if extract_roles:
            df = self._extract_roles(df)
        
        # æ—¶é—´æˆ³å¤„ç†
        if format_timestamps:
            df = self._format_timestamps(df)
        
        return df
    
    def _news_items_to_dataframe(self, news_items: List[NewsItem]) -> pd.DataFrame:
        """å°†NewsItemåˆ—è¡¨è½¬æ¢ä¸ºDataFrame"""
        if not news_items:
            return pd.DataFrame()
        
        # æå–raw_dataä¸­çš„ä¿¡æ¯
        records = []
        for item in news_items:
            if item.raw_data:
                record = item.raw_data.copy()
                record['id'] = item.id
                record['title'] = item.title
                record['content'] = item.content
                record['source_name'] = item.source_name
                record['source_url'] = item.source_url
                record['published_at'] = item.published_at
                record['author'] = item.author
                record['category'] = item.category
                record['language'] = item.language
                records.append(record)
        
        if records:
            df = pd.DataFrame(records)
            return df
        else:
            return pd.DataFrame()
    
    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """é¢„å¤„ç†æ•°æ®"""
        if df.empty:
            return df
        
        # æ¸…ç†ç©ºå€¼
        df = df.fillna('')
        
        # ç¡®ä¿å…³é”®åˆ—å­˜åœ¨
        required_columns = [
            'Actor1Name', 'Actor2Name', 'EventCode', 'SQLDATE',
            'GLOBALEVENTID', 'SOURCEURL', 'ActionGeo_FullName'
        ]
        
        for col in required_columns:
            if col not in df.columns:
                df[col] = ''
        
        # å»é™¤é‡å¤è¡Œ
        df = df.drop_duplicates(subset=['GLOBALEVENTID'], keep='first')
        
        return df
    
    def _normalize_entities(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–å®ä½“ï¼ˆå»é‡ActorNameï¼‰"""
        if df.empty:
            return df
        
        print(f"Normalizing entities in {len(df)} records")
        
        # å¯¹Actor1Nameå’ŒActor2Nameè¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
        df['Actor1Name_normalized'] = df['Actor1Name'].apply(self._normalize_actor_name)
        df['Actor2Name_normalized'] = df['Actor2Name'].apply(self._normalize_actor_name)
        
        # å®ç°å»é‡é€»è¾‘ - åˆå¹¶ç›¸åŒå®ä½“çš„è®°å½•
        df = self._deduplicate_entities(df)
        
        return df
    
    def _normalize_actor_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–å‚ä¸è€…åç§°"""
        if pd.isna(name) or name == '':
            return ''
        
        # æ¸…ç†åç§°ï¼šå»é™¤å¤šä½™ç©ºæ ¼ï¼Œç»Ÿä¸€å¤§å°å†™ç­‰
        normalized = str(name).strip().title()
        
        # å¤„ç†å¸¸è§çš„åç§°å˜ä½“ï¼ˆå¦‚ç¼©å†™ã€å…¨ç§°ç­‰ï¼‰
        name_mapping = {
            'USA': 'United States',
            'US': 'United States',
            'UK': 'United Kingdom',
            'UAE': 'United Arab Emirates',
            'USSR': 'Soviet Union',
            'CHN': 'China',
            'JPN': 'Japan',
            'GER': 'Germany',
            'FRA': 'France',
            'ITA': 'Italy',
            'CAN': 'Canada',
            'MEX': 'Mexico',
            'BRA': 'Brazil',
            'IND': 'India',
            'AUS': 'Australia',
            'KOR': 'South Korea',
            'PRK': 'North Korea',
            'SA': 'South Africa',
            'NGA': 'Nigeria',
            'EGY': 'Egypt',
            'TUR': 'Turkey',
            'SAU': 'Saudi Arabia',
            # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šæ˜ å°„
        }
        
        # åº”ç”¨æ˜ å°„
        mapped_name = name_mapping.get(normalized, normalized)
        
        # ç§»é™¤å¤šä½™çš„è¯ç¼€æˆ–æè¿°
        suffixes_to_remove = ['Government', 'Of', 'The', 'Republic', 'State', 'Province', 'City', 'Municipality']
        for suffix in suffixes_to_remove:
            mapped_name = mapped_name.replace(f' {suffix}', '').replace(f'{suffix} ', '')
        
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        mapped_name = ' '.join(mapped_name.split())
        
        return mapped_name
    
    def _deduplicate_entities(self, df: pd.DataFrame) -> pd.DataFrame:
        """å»é‡å®ä½“ï¼Œåˆå¹¶ç›¸åŒå®ä½“çš„è®°å½•"""
        if df.empty:
            return df
        
        # åˆ›å»ºæ ‡å‡†åŒ–çš„Actorç»„åˆé”®ï¼Œç”¨äºè¯†åˆ«é‡å¤é¡¹
        df['actor_pair_normalized'] = df.apply(
            lambda row: tuple(sorted([
                row['Actor1Name_normalized'], 
                row['Actor2Name_normalized']
            ])), axis=1
        )
        
        # å¦‚æœéœ€è¦å®Œå…¨å»é‡ï¼Œå¯ä»¥åŸºäºæ ‡å‡†åŒ–çš„å®ä½“å¯¹è¿›è¡Œåˆ†ç»„
        # è¿™é‡Œæˆ‘ä»¬ä¿ç•™æ¯ä¸ªå®ä½“å¯¹çš„æœ€æ–°è®°å½•
        df_sorted = df.sort_values('SQLDATE', ascending=False)
        df_deduplicated = df_sorted.drop_duplicates(
            subset=['actor_pair_normalized', 'EventCode'], 
            keep='first'
        ).sort_index()  # æ¢å¤åŸå§‹é¡ºåº
        
        print(f"Deduplicated from {len(df)} to {len(df_deduplicated)} records")
        
        return df_deduplicated
    
    def _extract_roles(self, df: pd.DataFrame) -> pd.DataFrame:
        """æå–è§’è‰²ä¿¡æ¯ï¼ˆActor1ä¸ºæ–½äº‹è€…ï¼ŒActor2ä¸ºå—äº‹è€…ï¼‰"""
        if df.empty:
            return df
        
        print(f"Extracting roles in {len(df)} records")
        
        # æ·»åŠ è§’è‰²æ ‡ç­¾
        df['Actor1_role'] = 'actor'  # æ–½äº‹è€…ï¼ˆå‘èµ·è¡ŒåŠ¨çš„å®ä½“ï¼‰
        df['Actor2_role'] = 'target'  # å—äº‹è€…ï¼ˆæ¥å—è¡ŒåŠ¨çš„å®ä½“ï¼‰
        
        # æ ¹æ®äº‹ä»¶ä»£ç ç¡®å®šäº‹ä»¶ç±»å‹å’Œè§’è‰²å…³ç³»
        df['event_type'] = df['EventCode'].apply(self._get_event_type)
        df['actor_target_relationship'] = df.apply(self._determine_relationship, axis=1)
        
        # å¢å¼ºè§’è‰²ä¿¡æ¯
        df['actor_description'] = df['Actor1Name_normalized'].apply(self._describe_actor)
        df['target_description'] = df['Actor2Name_normalized'].apply(self._describe_actor)
        
        return df
    
    def _determine_relationship(self, row) -> str:
        """æ ¹æ®äº‹ä»¶ä»£ç ç¡®å®šæ–½äº‹è€…å’Œå—äº‹è€…çš„å…³ç³»"""
        event_code = str(row['EventCode'])
        if len(event_code) >= 2:
            primary_code = event_code[:2]
        else:
            primary_code = event_code
        
        # æ ¹æ®CAMEOäº‹ä»¶ä»£ç å®šä¹‰å…³ç³»
        relationship_map = {
            '01': 'makes_public_statement_to',      # å‘è¡¨å…¬å¼€å£°æ˜
            '02': 'appeals_to',                    # å‘¼å
            '03': 'expresses_intent_to_cooperate_with',  # è¡¨è¾¾åˆä½œæ„å›¾
            '04': 'consults_with',                  # å’¨è¯¢
            '05': 'engages_in_diplomatic_cooperation_with',  # è¿›è¡Œå¤–äº¤åˆä½œ
            '06': 'engages_in_material_cooperation_with',    # è¿›è¡Œç‰©è´¨åˆä½œ
            '07': 'provides_aid_to',               # æä¾›æ´åŠ©
            '08': 'yields_to',                     # å±ˆæœäº
            '09': 'investigates',                  # è°ƒæŸ¥
            '10': 'demands_of',                    # è¦æ±‚
            '11': 'disapproves_of',                # ä¸èµæˆ
            '12': 'rejects',                       # æ‹’ç»
            '13': 'threatens_with',                # ä»¥...å¨èƒ
            '14': 'protests_against',              # æŠ—è®®
            '15': 'exhibits_force_posture_towards', # å±•ç¤ºæ­¦åŠ›å§¿æ€
            '16': 'reduces_relations_with',         # å‡å°‘ä¸...çš„å…³ç³»
            '17': 'coerces',                       # å¼ºåˆ¶
            '18': 'assaults',                      # æ”»å‡»
            '19': 'fights_with',                   # ä¸...æˆ˜æ–—
            '20': 'uses_unconventional_violence_against'  # å¯¹...ä½¿ç”¨éå¸¸è§„æš´åŠ›
        }
        
        return relationship_map.get(primary_code, f'interacts_with_code_{primary_code}')
    
    def _describe_actor(self, actor_name: str) -> str:
        """æè¿°å‚ä¸è€…ç±»å‹"""
        if pd.isna(actor_name) or actor_name == '':
            return 'unknown'
        
        # æ ¹æ®åç§°æ¨¡å¼åˆ¤æ–­å‚ä¸è€…ç±»å‹
        actor_lower = actor_name.lower()
        
        # å›½å®¶ç±»å‹
        if any(country in actor_lower for country in ['china', 'united states', 'russia', 'france', 'germany', 
                                                     'uk', 'united kingdom', 'japan', 'canada', 'australia']):
            return 'country'
        
        # ç»„ç»‡ç±»å‹
        if any(org in actor_lower for org in ['party', 'government', 'ministry', 'department', 'agency', 
                                             'committee', 'council', 'organization', 'union']):
            return 'organization'
        
        # ä¸ªäººç±»å‹
        if len(actor_name.split()) <= 3:  # å§“åé€šå¸¸è¾ƒçŸ­
            return 'person'
        
        return 'entity'
    
    def _get_event_type(self, event_code: str) -> str:
        """æ ¹æ®äº‹ä»¶ä»£ç ç¡®å®šäº‹ä»¶ç±»å‹"""
        if pd.isna(event_code):
            return 'unknown'
        
        # CAMEOäº‹ä»¶ä»£ç æ˜ å°„
        # è¿™é‡Œåªåˆ—å‡ºä¸€äº›å¸¸è§çš„äº‹ä»¶ç±»å‹ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
        event_mapping = {
            '01': 'make_public_statement',
            '02': 'appeal',
            '03': 'express_intent_to_cooperate',
            '04': 'consult',
            '05': 'engage_in_diplomatic_cooperation',
            '06': 'engage_in_material_cooperation',
            '07': 'provide_aid',
            '08': 'yield',
            '09': 'investigate',
            '10': 'demand',
            '11': 'disapprove',
            '12': 'reject',
            '13': 'threaten',
            '14': 'protest',
            '15': 'exhibit_force_posture',
            '16': 'reduce_relations',
            '17': 'coerce',
            '18': 'assault',
            '19': 'fight',
            '20': 'use_unconventional_mass_violence'
        }
        
        code_str = str(event_code)
        # å–å‰ä¸¤ä½ä½œä¸ºä¸»è¦äº‹ä»¶ç±»å‹
        primary_code = code_str[:2] if len(code_str) >= 2 else code_str
        
        return event_mapping.get(primary_code, f'code_{code_str}')
    
    def _format_timestamps(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ—¶é—´æˆ³æå–å’Œæ ¼å¼åŒ–"""
        if df.empty:
            return df
        
        print(f"Formatting timestamps in {len(df)} records")
        
        # å¤„ç†SQLDATEåˆ—
        if 'SQLDATE' in df.columns:
            df['timestamp_formatted'] = df['SQLDATE'].apply(self._format_sql_date)
            df['date_parsed'] = df['SQLDATE'].apply(self._parse_sql_date)
            df['year'] = df['date_parsed'].apply(lambda x: x.year if x is not None else None)
            df['month'] = df['date_parsed'].apply(lambda x: x.month if x is not None else None)
            df['day'] = df['date_parsed'].apply(lambda x: x.day if x is not None else None)
            df['week'] = df['date_parsed'].apply(lambda x: x.isocalendar()[1] if x is not None else None)
            df['quarter'] = df['date_parsed'].apply(lambda x: x.quarter if x is not None and hasattr(x, 'quarter') else None)
        
        # å¤„ç†å…¶ä»–å¯èƒ½çš„æ—¶é—´åˆ—
        if 'DATEADDED' in df.columns:
            df['dateadded_parsed'] = df['DATEADDED'].apply(self._parse_sql_date)
        
        # å¦‚æœæœ‰MentionTimeDateï¼ˆæ¥è‡ªmentionsè¡¨ï¼‰
        if 'MentionTimeDate' in df.columns:
            df['mention_timestamp'] = df['MentionTimeDate'].apply(self._format_mention_timestamp)
        
        # åˆ›å»ºæ—¶é—´ç‰¹å¾
        df = self._create_temporal_features(df)
        
        return df
    
    def _format_sql_date(self, sql_date: Union[str, int, float]) -> Optional[str]:
        """æ ¼å¼åŒ–SQLDATEä¸ºæ ‡å‡†æ—¶é—´æˆ³æ ¼å¼"""
        if pd.isna(sql_date):
            return None
        
        try:
            # SQLDATEæ ¼å¼é€šå¸¸æ˜¯YYYYMMDD
            date_str = str(int(sql_date)) if isinstance(sql_date, (int, float)) else str(sql_date)
            if len(date_str) == 8:
                dt = datetime.strptime(date_str, '%Y%m%d')
                return dt.strftime('%Y-%m-%d %H:%M:%S')
            elif len(date_str) == 14:  # YYYYMMDDHHMMSS æ ¼å¼
                dt = datetime.strptime(date_str, '%Y%m%d%H%M%S')
                return dt.strftime('%Y-%m-%d %H:%M:%S')
            else:
                return str(sql_date)
        except Exception:
            return str(sql_date)
    
    def _parse_sql_date(self, sql_date: Union[str, int, float]) -> Optional[datetime]:
        """è§£æSQLDATEä¸ºdatetimeå¯¹è±¡"""
        if pd.isna(sql_date):
            return None
        
        try:
            # SQLDATEæ ¼å¼é€šå¸¸æ˜¯YYYYMMDD
            date_str = str(int(sql_date)) if isinstance(sql_date, (int, float)) else str(sql_date)
            if len(date_str) == 8:
                return datetime.strptime(date_str, '%Y%m%d')
            elif len(date_str) == 14:  # YYYYMMDDHHMMSS æ ¼å¼
                return datetime.strptime(date_str, '%Y%m%d%H%M%S')
            else:
                return None
        except Exception:
            return None
    
    def _format_mention_timestamp(self, mention_time: Union[str, int, float]) -> Optional[str]:
        """æ ¼å¼åŒ–MentionTimeDateä¸ºæ ‡å‡†æ—¶é—´æˆ³æ ¼å¼"""
        if pd.isna(mention_time):
            return None
        
        try:
            # MentionTimeDateæ ¼å¼é€šå¸¸æ˜¯YYYYMMDDHHMMSS
            time_str = str(int(mention_time)) if isinstance(mention_time, (int, float)) else str(mention_time)
            if len(time_str) == 14:
                dt = datetime.strptime(time_str, '%Y%m%d%H%M%S')
                return dt.strftime('%Y-%m-%d %H:%M:%S')
            elif len(time_str) == 8:  # å¦‚æœåªæœ‰æ—¥æœŸéƒ¨åˆ†
                dt = datetime.strptime(time_str, '%Y%m%d')
                return dt.strftime('%Y-%m-%d %H:%M:%S')
            else:
                return str(time_str)
        except Exception:
            return str(mention_time)
    
    def _create_temporal_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ—¶é—´ç‰¹å¾"""
        if df.empty:
            return df
        
        # åŸºäºè§£æåçš„æ—¥æœŸåˆ›å»ºæ›´å¤šæ—¶é—´ç‰¹å¾
        if 'date_parsed' in df.columns:
            date_col = df['date_parsed']
            
            # å·¥ä½œæ—¥ç‰¹å¾
            df['day_of_week'] = date_col.apply(lambda x: x.weekday() if x is not None else None)
            df['is_weekend'] = date_col.apply(lambda x: x.weekday() >= 5 if x is not None else None)
            
            # å­£èŠ‚ç‰¹å¾
            df['season'] = date_col.apply(lambda x: self._get_season(x.month) if x is not None else None)
            
            # æ˜¯å¦ä¸ºæœˆåˆ/æœˆæœ«
            df['is_month_start'] = date_col.apply(lambda x: x.day == 1 if x is not None else None)
            df['is_month_end'] = date_col.apply(lambda x: x.day == x.days_in_month if x is not None and hasattr(x, 'days_in_month') else None)
        
        return df
    
    def _get_season(self, month: int) -> str:
        """æ ¹æ®æœˆä»½è·å–å­£èŠ‚"""
        if month in [12, 1, 2]:
            return 'winter'
        elif month in [3, 4, 5]:
            return 'spring'
        elif month in [6, 7, 8]:
            return 'summer'
        else:
            return 'autumn'
    
    async def process_large_gdelt_data(
        self, 
        config: FetchConfig,
        chunk_size: int = 10000
    ) -> Union[pd.DataFrame, dd.DataFrame]:
        """
        å¤„ç†å¤§è§„æ¨¡GDELTæ•°æ®ï¼ˆæ”¯æŒDaskï¼‰
        
        Args:
            config: è·å–é…ç½®
            chunk_size: åˆ†å—å¤§å°
            
        Returns:
            å¤„ç†åçš„DataFrameï¼ˆpandasæˆ–Daskï¼‰
        """
        if self._use_dask:
            return await self._process_with_dask(config, chunk_size)
        else:
            return await self.process_gdelt_data(config)
    
    async def _process_with_dask(
        self, 
        config: FetchConfig,
        chunk_size: int = 10000
    ) -> dd.DataFrame:
        """ä½¿ç”¨Daskå¤„ç†å¤§è§„æ¨¡æ•°æ®"""
        if not DASK_AVAILABLE:
            print("Dask not available, falling back to pandas")
            return await self.process_gdelt_data(config)
        
        # ç”±äºDaskå¤„ç†éœ€è¦ç‰¹å®šçš„æ•°æ®æºï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿåˆ†å—å¤„ç†
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™å¯èƒ½éœ€è¦ä»æ–‡ä»¶ç³»ç»Ÿæˆ–æ•°æ®åº“ç›´æ¥è¯»å–
        
        # å…ˆè·å–æ‰€æœ‰æ•°æ®
        raw_data = await self._gdelt_adapter.fetch(config)
        
        if not raw_data.items:
            return dd.from_pandas(pd.DataFrame(), npartitions=1)
        
        # å°†æ•°æ®è½¬æ¢ä¸ºDataFrame
        df = self._news_items_to_dataframe(raw_data.items)
        
        # å¦‚æœæ•°æ®é‡å¤§ï¼Œä½¿ç”¨Dask
        if len(df) > chunk_size:
            # åˆ›å»ºDask DataFrame
            ddf = dd.from_pandas(df, npartitions=max(1, len(df) // chunk_size))
            
            # åº”ç”¨å¤„ç†å‡½æ•°
            ddf = ddf.map_partitions(self._process_partition)
            
            return ddf
        else:
            # æ•°æ®é‡å°ï¼Œç›´æ¥ä½¿ç”¨pandaså¤„ç†
            processed_df = self._process_partition(df)
            return dd.from_pandas(processed_df, npartitions=1)
    
    def _process_partition(self, partition_df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†æ•°æ®åˆ†åŒº"""
        # é¢„å¤„ç†
        partition_df = self._preprocess_data(partition_df)
        
        # å®ä½“æ ‡å‡†åŒ–
        partition_df = self._normalize_entities(partition_df)
        
        # è§’è‰²æå–
        partition_df = self._extract_roles(partition_df)
        
        # æ—¶é—´æˆ³å¤„ç†
        partition_df = self._format_timestamps(partition_df)
        
        return partition_df
    
    def integrate_gkg_data(
        self, 
        events_df: pd.DataFrame, 
        gkg_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        é›†æˆGKGæ–‡ä»¶ä»¥å¢å¼ºä¸»é¢˜/æƒ…æ„Ÿåˆ†æ
        
        Args:
            events_df: äº‹ä»¶æ•°æ®DataFrame
            gkg_df: GKGæ•°æ®DataFrame
            
        Returns:
            é›†æˆåçš„DataFrame
        """
        if events_df.empty or gkg_df.empty:
            return events_df
        
        print("Integrating GKG data with events data")
        
        # è¿™é‡Œå®ç°GKGæ•°æ®ä¸äº‹ä»¶æ•°æ®çš„é›†æˆé€»è¾‘
        # é€šå¸¸é€šè¿‡æ—¥æœŸæˆ–ä¸»é¢˜åŒ¹é…
        # ç”±äºGKGå’Œäº‹ä»¶æ•°æ®çš„ç»“æ„ä¸åŒï¼Œæˆ‘ä»¬éœ€è¦é€‚å½“çš„è¿æ¥é€»è¾‘
        
        # ç¤ºä¾‹ï¼šæŒ‰æ—¥æœŸè¿æ¥ï¼ˆè¿™å¯èƒ½éœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´ï¼‰
        if 'DATE' in gkg_df.columns and 'SQLDATE' in events_df.columns:
            # å°†äº‹ä»¶æ•°æ®çš„SQLDATEè½¬æ¢ä¸ºGKGæ ¼å¼è¿›è¡ŒåŒ¹é…
            events_df['DATE'] = events_df['SQLDATE'].apply(
                lambda x: int(str(x)[:8]) if pd.notna(x) else None
            )
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.merge(
                events_df,
                gkg_df,
                on='DATE',
                how='left',
                suffixes=('', '_gkg')
            )
            
            return merged_df
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…çš„åˆ—ï¼Œç›´æ¥è¿”å›äº‹ä»¶æ•°æ®
            print("Cannot merge events and GKG data: no matching columns found")
            return events_df


def _load_entity_equivs() -> Dict[str, Set[str]]:

@register_tool(
    name="fetch_news_stream",
    description="ä»æ‰€æœ‰é…ç½®çš„æ•°æ®æºï¼ˆå½“å‰ä»… GNewsï¼‰è·å–æœ€æ–°æ–°é—»",
    category="Data Fetch"
)
async def fetch_news_stream(
    limit: int = 50,
    sources: Optional[List[str]] = None,
    # GNews å¯é€‰å‚æ•°
    category: Optional[str] = None,
    query: Optional[str] = None,
    from_: Optional[str] = None,
    to: Optional[str] = None,
    nullable: Optional[str] = None,
    truncate: Optional[str] = None,
    sortby: Optional[str] = None,
    in_fields: Optional[str] = None,
    page: Optional[int] = None,
    daily_incremental: bool = False,  # æ–°å¢å‚æ•°ï¼šæ˜¯å¦æŒ‰å¤©é€’å¢è¯·æ±‚
) -> List[Dict[str, Any]]:
    """
    è·å–å…¨æ¸ é“æ–°é—»æ•°æ®ã€‚

    Args:
        limit: æ¯ä¸ªæºè·å–çš„æœ€å¤§æ¡æ•°
        sources: æŒ‡å®šæºåˆ—è¡¨ (å¦‚ ["GNews-cn"]), é»˜è®¤ä¸ºæ‰€æœ‰å¯ç”¨æº
        daily_incremental: æ˜¯å¦æŒ‰å¤©é€’å¢è¯·æ±‚æ•°æ®ï¼ˆä»from_å¼€å§‹ï¼Œæ¯å¤©è¯·æ±‚ä¸€æ¬¡ï¼Œç›´åˆ°to_ï¼‰

    Returns:
        æ–°é—»åˆ—è¡¨ (List[Dict])
    """
    tools = Tools()
    tools.log(f"[fetch_news_stream] å¼€å§‹æ‰§è¡Œï¼Œè¯·æ±‚æº: {sources}, limit: {limit}, daily_incremental: {daily_incremental}")
    
    # é…ç½®é©±åŠ¨çš„å¹¶å‘ä¸Šé™ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ï¼‰
    config_manager = get_config_manager()
    concurrency = config_manager.get_concurrency_limit("agent1_config")
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºDEBUGä»¥ä¾¿è·å–æ›´å¤šè°ƒè¯•ä¿¡æ¯
    from ...infra.logging import LoggerManager
    LoggerManager.set_level("DEBUG")

    # åˆå§‹åŒ– API Pool
    api_pool = NewsAPIManager.get_instance()
    available_sources = api_pool.list_available_sources()
    tools.log(f"[fetch_news_stream] å¯ç”¨æ•°æ®æº: {available_sources}")
    
    if sources:
        target_sources = [s for s in sources if s in available_sources]
        tools.log(f"[fetch_news_stream] ç­›é€‰åç›®æ ‡æº: {target_sources}")
    else:
        target_sources = available_sources

    if not target_sources:
        tools.log("Warning: No valid sources to fetch from. APIå¯†é’¥å¯èƒ½æœªé…ç½®ã€‚")
        return []

    # å¦‚æœå¯ç”¨äº†æŒ‰å¤©é€’å¢è¯·æ±‚ï¼Œå¹¶ä¸”æä¾›äº†from_å’Œto_å‚æ•°
    if daily_incremental and from_ and to:
        tools.log(f"[fetch_news_stream] å¯ç”¨æŒ‰å¤©é€’å¢è¯·æ±‚: ä» {from_} åˆ° {to}")
        all_news = []
        
        # å®šä¹‰æ‰€æœ‰å¯ç”¨çš„ç±»åˆ«
        categories = ["general", "world", "nation", "business", "technology", "entertainment", "sports", "science", "health"]
        
        # è§£ææ—¥æœŸ
        try:
            from_date = datetime.fromisoformat(from_.replace("Z", "+00:00")).date()
            to_date = datetime.fromisoformat(to.replace("Z", "+00:00")).date()
            
            # æŒ‰å¤©é€’å¢è¯·æ±‚æ•°æ®
            current_date = from_date
            while current_date <= to_date:
                # æ„é€ å½“å¤©çš„æ—¥æœŸèŒƒå›´
                day_start = f"{current_date.isoformat()}T00:00:00.000Z"
                day_end = f"{current_date.isoformat()}T23:59:59.999Z"
                
                tools.log(f"[fetch_news_stream] è¯·æ±‚ {current_date.isoformat()} çš„æ•°æ®")
                
                # ä¸ºæ¯ä¸ªç±»åˆ«è·å–æ•°æ®
                day_news = []
                for cat in categories:
                    tools.log(f"[fetch_news_stream] è¯·æ±‚ {current_date.isoformat()} çš„ {cat} ç±»åˆ«æ•°æ®")
                    cat_news = await fetch_from_multiple_sources(
                        api_pool=api_pool,
                        source_names=target_sources,
                        concurrency_limit=concurrency,
                        query=query,
                        category=cat,
                        limit=limit,
                        from_=day_start,
                        to=day_end,
                        nullable=nullable,
                        truncate=truncate,
                        sortby=sortby,
                        in_fields=in_fields,
                        page=page,
                    )
                    day_news.extend(cat_news)
                    tools.log(f"[fetch_news_stream] {current_date.isoformat()} çš„ {cat} ç±»åˆ«è·å–åˆ° {len(cat_news)} æ¡æ•°æ®")
                
                all_news.extend(day_news)
                tools.log(f"[fetch_news_stream] {current_date.isoformat()} æ€»å…±è·å–åˆ° {len(day_news)} æ¡æ•°æ®")
                
                # ç§»åŠ¨åˆ°ä¸‹ä¸€å¤©
                current_date = current_date + timedelta(days=1)                
        except Exception as e:
            tools.log(f"[fetch_news_stream] æŒ‰å¤©é€’å¢è¯·æ±‚å‡ºé”™: {e}")
            # å¦‚æœå‡ºé”™ï¼Œå›é€€åˆ°åŸæ¥çš„å®ç°
            return await fetch_from_multiple_sources(
                api_pool=api_pool,
                source_names=target_sources,
                concurrency_limit=concurrency,
                query=query,
                category=category,
                limit=limit,
                from_=from_,
                to=to,
                nullable=nullable,
                truncate=truncate,
                sortby=sortby,
                in_fields=in_fields,
                page=page,
            )
            
        tools.log(f"[fetch_news_stream] æŒ‰å¤©é€’å¢è¯·æ±‚å®Œæˆï¼Œæ€»å…±è·å–åˆ° {len(all_news)} æ¡æ•°æ®")
        return all_news
    else:
        # å¦‚æœæŒ‡å®šäº†ç±»åˆ«ï¼Œåˆ™åªä½¿ç”¨è¯¥ç±»åˆ«ï¼›å¦åˆ™éå†æ‰€æœ‰ç±»åˆ«
        categories = ["general", "world", "nation", "business", "technology", "entertainment", "sports", "science", "health"]
        
        # ä¸ºæ¯ä¸ªç±»åˆ«è·å–æ•°æ®å¹¶åˆå¹¶
        all_news = []
        for cat in categories:
            cat_news = await fetch_from_multiple_sources(
                api_pool=api_pool,
                source_names=target_sources,
                concurrency_limit=concurrency,
                query=query,
                category=cat,
                limit=limit,
                from_=from_,
                to=to,
                nullable=nullable,
                truncate=truncate,
                sortby=sortby,
                in_fields=in_fields,
                page=page,
            )
            all_news.extend(cat_news)
        
        return all_news

def _load_entity_equivs() -> Dict[str, Set[str]]:
    """
    æ„å»ºå®ä½“åŠåŒä¹‰è¯ç´¢å¼•ï¼šå®ä½“åº“ original_forms + åˆå¹¶è§„åˆ™çš„åˆ«åã€‚
    è¿”å› dict: è¯ -> åŒä¹‰é›†åˆï¼ˆåŒ…å«è‡ªèº«ï¼‰ã€‚
    """
    tools = Tools()
    idx: Dict[str, Set[str]] = {}

    def add_forms(key: str, forms: List[str]):
        if not key:
            return
        bucket = idx.setdefault(key, set())
        for f in forms:
            if f:
                bucket.add(f)
        bucket.add(key)

    # å®ä½“åº“ original_forms
    try:
        if tools.ENTITIES_FILE.exists():
            with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
                ents = json.load(f)
            for name, data in ents.items():
                forms = data.get("original_forms", []) if isinstance(data, dict) else []
                if isinstance(forms, list):
                    flat = []
                    for x in forms:
                        if isinstance(x, str):
                            flat.append(x)
                        elif isinstance(x, list):
                            flat.extend([str(i) for i in x])
                    add_forms(name, flat)
                else:
                    add_forms(name, [])
    except Exception:
        pass

    # åˆå¹¶è§„åˆ™ï¼ˆalias -> primaryï¼‰ï¼ŒåŒå‘åŠ å…¥
    try:
        merge_rules_file = tools.CONFIG_DIR / "entity_merge_rules.json"
        if merge_rules_file.exists():
            with open(merge_rules_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            rules = data.get("merge_rules", {}) if isinstance(data, dict) else {}
            inv: Dict[str, List[str]] = {}
            for alias, primary in rules.items():
                add_forms(alias, [primary])
                inv.setdefault(primary, []).append(alias)
            for primary, aliases in inv.items():
                add_forms(primary, aliases)
    except Exception:
        pass

    # å±•å¼€ï¼šç¡®ä¿äº’ç›¸åŒ…å«
    for k, forms in list(idx.items()):
        for f in list(forms):
            if f in idx:
                forms.update(idx[f])
    return idx


def _expand_keywords(keywords: List[str]) -> List[List[str]]:
    """
    å°†æ¯ä¸ªå…³é”®è¯æ‰©å±•ä¸ºåŒä¹‰é›†åˆåˆ—è¡¨ï¼ˆæŒ‰è¾“å…¥é¡ºåºä¿ç•™ç»„ï¼‰ã€‚
    """
    idx = _load_entity_equivs()
    groups: List[List[str]] = []
    for kw in keywords:
        kw_norm = (kw or "").strip()
        if not kw_norm:
            continue
        forms = set([kw_norm])
        if kw_norm in idx:
            forms.update(idx[kw_norm])
        groups.append(list(forms))
    return groups


def _build_boolean_query(groups: List[List[str]]) -> str:
    """
    æ„å»ºgnews.io APIæŸ¥è¯¢
    è§„åˆ™ï¼š
    1. æ¯ä¸ªåˆ†ç»„å†…çš„è¯ç”¨ORè¿æ¥
    2. ä¸åŒåˆ†ç»„ä¹‹é—´ç”¨ANDè¿æ¥  
    """
    if not groups:
        return ""
    
    query_parts = []
    
    for group in groups:
        if not group:
            continue
            
        # æ¸…ç†ç»„å†…çš„è¯
        valid_terms = []
        for term in group:
            if isinstance(term, str):
                term = term.strip()
                if term:
                    valid_terms.append(term)
        
        if not valid_terms:
            continue
            
        # å¤„ç†å•ä¸ªè¯
        if len(valid_terms) == 1:
            term = valid_terms[0]
            query_parts.append(f'"{term}"')
        # å¤„ç†å¤šä¸ªè¯
        else:
            or_terms = []
            for term in valid_terms:
                or_terms.append(f'"{term}"')
            query_parts.append(f"{' OR '.join(or_terms)}")
    
    if not query_parts:
        return ""
    
    base_query = " AND ".join(query_parts)
    return base_query



@register_tool(
    name="search_news_by_keywords",
    description="æŒ‰å…³é”®è¯æœç´¢æ–°é—»ï¼ˆå½“å‰ä»… GNewsï¼‰ï¼Œæ”¯æŒå¯é€‰æ—¶é—´èŒƒå›´ä¸æ’åº",
    category="Data Fetch"
)
async def search_news_by_keywords(
    keywords: List[str],
    apis: Optional[List[str]] = None,
    limit: int = 10,
    category: Optional[str] = None,
    from_: Optional[str] = None,
    to: Optional[str] = None,
    nullable: Optional[str] = None,
    truncate: Optional[str] = None,
    sortby: Optional[str] = None,
    in_fields: Optional[str] = None,
    page: Optional[int] = None,
) -> List[Dict[str, Any]]:
    """
    æ ¹æ®å…³é”®è¯åˆ—è¡¨æœç´¢æ–°é—»ï¼ˆGNewsï¼‰ï¼Œè¿”å›åˆå¹¶åçš„ç»“æœã€‚
    """
    tools = Tools()
    if not keywords:
        return []

    # æ„é€  (A OR A2) AND (B OR B2) æŸ¥è¯¢ä¸²
    groups = _expand_keywords(keywords)
    query_str = _build_boolean_query(groups)
    if not query_str:
        return []

    # åˆå§‹åŒ– API Pool
    api_pool = NewsAPIManager.get_instance()

    # å¹¶å‘ä¸Šé™ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ï¼‰
    config_manager = get_config_manager()
    concurrency = config_manager.get_concurrency_limit("agent2_config")

    available_sources = api_pool.list_available_sources()
    if apis:
        target_sources = [s for s in apis if s in available_sources]
    else:
        target_sources = available_sources

    if not target_sources:
        tools.log("Warning: No valid sources to search from.")
        return []

    # ä½¿ç”¨å…¬å…±å‡½æ•°è·å–æ•°æ®
    return await fetch_from_multiple_sources(
        api_pool=api_pool,
        source_names=target_sources,
        concurrency_limit=concurrency,
        query=query_str,
        category=category,
        limit=limit,
        from_=from_,
        to=to,
        nullable=nullable,
        truncate=truncate,
        sortby=sortby,
        in_fields=in_fields,
        page=page,
    )


async def expand_news_by_entities(entities: List[Dict], limit_per_entity: int = 10, time_window_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    æ ¹æ®å®ä½“åˆ—è¡¨æœç´¢ç›¸å…³æ–°é—»ï¼Œæ”¯æŒä½¿ç”¨åŸå§‹è¯è¿›è¡Œæ£€ç´¢

    Args:
        entities: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«nameå’Œoriginal_formså­—æ®µ
        limit_per_entity: æ¯ä¸ªå®ä½“æœç´¢çš„æ–°é—»æ•°é‡é™åˆ¶
        time_window_days: é»˜è®¤æ£€ç´¢æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢ï¼Œå¦‚æœä¸ºTrueåˆ™ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´

    Returns:
        æœç´¢åˆ°çš„ç›¸å…³æ–°é—»åˆ—è¡¨
    """
    expanded_news = []
    news_id_set = set()  # ç”¨äºå»é‡

    # è·å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨
    news_collectors = []
    api_pool = NewsAPIManager.get_instance()
    available_sources = api_pool.list_available_sources()

    # ä¸æ›´æ–°åçš„APIæ± å…¼å®¹ï¼Œç§»é™¤å¯èƒ½çš„å¤‡ç”¨é€»è¾‘
    for source_name in available_sources:
        try:
            collector = api_pool.get_collector(source_name)
            news_collectors.append(collector)
        except Exception as e:
            Tools().log(f"âš ï¸ æ— æ³•åˆ›å»ºæ–°é—»æ”¶é›†å™¨ {source_name}: {e}")

    if not news_collectors:
        Tools().log("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨")
        return expanded_news

    # ä¸ºæ¯ä¸ªå®ä½“æœç´¢ç›¸å…³æ–°é—»
    for entity in entities:
        entity_name = entity['name']
        original_forms = entity.get('original_forms', [])

        # æ„å»ºä½¿ç”¨ORæ“ä½œç¬¦è¿æ¥çš„æœç´¢æŸ¥è¯¢ï¼šå®ä½“åç§° + æ‰€æœ‰åŸå§‹è¯
        all_terms = [entity_name] + original_forms

        # ç”ŸæˆæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶
        query_batches = []
        current_batch = []
        current_length = 0

        for term in all_terms:
            quoted_term = f'"{term}"'
            term_length = len(quoted_term)

            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯ï¼Œç›´æ¥æ·»åŠ ï¼›å¦åˆ™éœ€è¦è€ƒè™‘ORæ“ä½œç¬¦çš„é•¿åº¦
            if current_batch:
                required_length = current_length + 4 + term_length  # 4æ˜¯" OR "çš„é•¿åº¦
                if required_length > 200:
                    # å¦‚æœæ·»åŠ å½“å‰è¯ä¼šè¶…è¿‡é™åˆ¶ï¼Œä¿å­˜å½“å‰æ‰¹æ¬¡å¹¶å¼€å§‹æ–°æ‰¹æ¬¡
                    query_batches.append(" OR ".join(current_batch))
                    current_batch = [quoted_term]
                    current_length = term_length
                else:
                    current_batch.append(quoted_term)
                    current_length = required_length
            else:
                current_batch.append(quoted_term)
                current_length = term_length

        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            query_batches.append(" OR ".join(current_batch))

        Tools().log(f"ğŸ” ä¸ºå®ä½“ '{entity_name}' æœç´¢ç›¸å…³æ–°é—»ï¼Œä½¿ç”¨ORæŸ¥è¯¢è¿æ¥ {len(original_forms)} ä¸ªåŸå§‹è¯...")
        Tools().log(f"   ğŸ“ ç”Ÿæˆäº† {len(query_batches)} ä¸ªæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶")

        # è·å–æ—¶é—´èŒƒå›´
        time_ranges = get_time_ranges(time_window_days, full_search)

        for collector in news_collectors:
            try:
                # å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰¹æ¬¡å’Œæ—¶é—´èŒƒå›´è¿›è¡Œæœç´¢
                for time_range in time_ranges:
                    start_date = time_range['start']
                    end_date = time_range['end']

                    Tools().log(f"   ğŸ“… æœç´¢æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")

                    for batch_index, batch_query in enumerate(query_batches):
                        Tools().log(f"   ğŸ“ æ‰§è¡ŒæŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1}/{len(query_batches)}: '{batch_query}'")

                        try:
                            # ä½¿ç”¨æœç´¢åŠŸèƒ½è·å–ç›¸å…³æ–°é—»ï¼Œä¼ å…¥æ—¶é—´èŒƒå›´å‚æ•°
                            search_params = {
                                'keyword' if hasattr(collector, 'search_news_by_keyword') else 'query': batch_query,
                                'limit': limit_per_entity // (len(query_batches) * len(time_ranges)) + 1  # å¹³å‡åˆ†é…é™åˆ¶
                            }

                            # å¦‚æœæ”¶é›†å™¨æ”¯æŒæ—¶é—´èŒƒå›´å‚æ•°ï¼Œåˆ™æ·»åŠ 
                            if hasattr(collector, 'search_news_by_keyword'):
                                if 'start_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date
                            elif hasattr(collector, 'search'):
                                if 'start_date' in collector.search.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date

                            # ä½¿ç”¨æ›´æ–°åçš„APIè°ƒç”¨æ–¹æ³•
                            # ä¼˜å…ˆä½¿ç”¨search_news_by_keywordæ–¹æ³•ï¼Œç¡®ä¿ä¸æ›´æ–°åçš„APIæ± å…¼å®¹
                            if hasattr(collector, 'search_news_by_keyword'):
                                news_list = await collector.search_news_by_keyword(**search_params)
                            elif hasattr(collector, 'search'):
                                news_list = await collector.search(**search_params)
                            else:
                                Tools().log(f"âš ï¸ æ”¶é›†å™¨ {collector.__class__.__name__} æ²¡æœ‰æ”¯æŒçš„æœç´¢æ–¹æ³•")
                                continue

                            # ä¸ºæ¯æ¡æ–°é—»æ·»åŠ å®ä½“æ ‡ç­¾å¹¶å»é‡
                            for news in news_list:
                                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦ç”¨äºå»é‡
                                news_id = f"{news.get('url', '')}_{news.get('publishedAt', '')}"
                                if news_id not in news_id_set:
                                    news_id_set.add(news_id)
                                    news['expanded_from_entity'] = entity_name
                                    news['search_term'] = batch_query  # è®°å½•ä½¿ç”¨çš„æœç´¢è¯
                                    news['source'] = collector.__class__.__name__.replace('Collector', '').lower()
                                    news['query_batch'] = batch_index + 1  # è®°å½•æŸ¥è¯¢æ‰¹æ¬¡
                                    news['search_time_range'] = f"{start_date} to {end_date}"  # è®°å½•æœç´¢æ—¶é—´èŒƒå›´
                                    expanded_news.append(news)
                        except Exception as batch_error:
                            Tools().log(f"âš ï¸ æŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1} æ‰§è¡Œå¤±è´¥: {batch_error}")
            except Exception as e:
                Tools().log(f"âš ï¸ ä» {collector.__class__.__name__} æœç´¢å¤±è´¥: {e}")

    return expanded_news


def get_time_ranges(default_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    è·å–æœç´¢çš„æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
    1. æ—¶é—´èŒƒå›´åªèƒ½ä»2020å¹´è‡³ä»Š
    2. é»˜è®¤æ£€ç´¢å‰30å¤©å†…çš„æ–°é—»
    3. å…¨é¢æ£€ç´¢æ—¶ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´

    Args:
        default_days: é»˜è®¤çš„æ—¶é—´çª—å£å¤©æ•°ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢

    Returns:
        æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«startå’Œendæ—¥æœŸå­—ç¬¦ä¸²
    """
    time_ranges = []
    now = datetime.now(timezone.utc)

    # å®šä¹‰2020å¹´1æœˆ1æ—¥ä½œä¸ºèµ·å§‹æ—¶é—´
    start_date_2020 = datetime(2020, 1, 1, tzinfo=timezone.utc)

    if not full_search:
        # éå…¨é¢æ£€ç´¢ï¼Œåªè¿”å›é»˜è®¤æ—¶é—´çª—å£
        end_date = now
        start_date = max(start_date_2020, now - timedelta(days=default_days))
        time_ranges.append({
            'start': start_date.strftime('%Y-%m-%d'),
            'end': end_date.strftime('%Y-%m-%d')
        })
    else:
        # å…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´
        Tools().log("ğŸ”„ æ‰§è¡Œå…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æ‰¹æ¬¡ç›´åˆ°2020å¹´...")

        end_date = now
        batch_count = 0

        while end_date > start_date_2020:
            start_date = max(start_date_2020, end_date - timedelta(days=default_days))

            # ç¡®ä¿ä¸é‡å¤æ·»åŠ ç›¸åŒçš„æ—¶é—´èŒƒå›´
            if not time_ranges or time_ranges[-1]['start'] != start_date.strftime('%Y-%m-%d'):
                time_ranges.append({
                    'start': start_date.strftime('%Y-%m-%d'),
                    'end': end_date.strftime('%Y-%m-%d')
                })

            batch_count += 1
            end_date = start_date - timedelta(days=1)  # é¿å…æ—¥æœŸé‡å 

        Tools().log(f"âœ… ç”Ÿæˆäº† {batch_count} ä¸ªæ—¶é—´èŒƒå›´æ‰¹æ¬¡")

    return time_ranges


def get_recent_entities(time_window_days: int = 30, limit: int = 50) -> List[Dict]:
    """
    è·å–æœ€è¿‘æ—¶é—´çª—å£å†…çš„å®ä½“åˆ—è¡¨ï¼ŒåŒ…å«åŸå§‹è¯ä¿¡æ¯

    Args:
        time_window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        limit: è¿”å›çš„å®ä½“æ•°é‡é™åˆ¶

    Returns:
        æœ€è¿‘çš„å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«åç§°å’ŒåŸå§‹è¯ä¿¡æ¯
    """
    entities = []
    tools = Tools()

    if not tools.ENTITIES_FILE.exists():
        tools.log("âš ï¸ å®ä½“åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return entities

    # è¯»å–å®ä½“åº“
    with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
        entity_data = json.load(f)

    # æ ¹æ® first_seen æ’åºï¼Œè·å–æœ€è¿‘çš„å®ä½“
    sorted_entities = sorted(
        entity_data.items(),
        key=lambda x: x[1].get('first_seen', ''),
        reverse=True
    )

    # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„å®ä½“
    now = datetime.now(timezone.utc)
    time_window = timedelta(days=time_window_days)

    for entity_name, entity_info in sorted_entities:
        first_seen = entity_info.get('first_seen')
        if first_seen:
            try:
                # è§£ææ—¶é—´å­—ç¬¦ä¸²
                if 'T' in first_seen:
                    # ISOæ ¼å¼æ—¶é—´
                    seen_time = datetime.fromisoformat(first_seen.replace('Z', '+00:00'))
                else:
                    # æ™®é€šæ ¼å¼æ—¶é—´
                    seen_time = datetime.strptime(first_seen, '%Y-%m-%d %H:%M:%S')
                    seen_time = seen_time.replace(tzinfo=timezone.utc)

                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
                if now - seen_time <= time_window:
                    entity_info = {
                        'name': entity_name,
                        'original_forms': entity_data[entity_name].get('original_forms', [])
                    }
                    entities.append(entity_info)
                    if len(entities) >= limit:
                        break
            except Exception as e:
                tools.log(f"âš ï¸ è§£æå®ä½“ '{entity_name}' çš„æ—¶é—´æˆ³å¤±è´¥: {e}")

    tools.log(f"âœ… è·å–äº† {len(entities)} ä¸ªæœ€è¿‘å®ä½“")
    return entities


async def process_expanded_news(expanded_news: List[Dict], rate_limit: float = 1.0, max_workers: int = 3) -> int:
    """
    å¤„ç†æ‹“å±•çš„æ–°é—»ï¼Œæå–å®ä½“å’Œäº‹ä»¶

    Args:
        expanded_news: æ‹“å±•çš„æ–°é—»åˆ—è¡¨
        rate_limit: æ¯ç§’é€Ÿç‡é™åˆ¶
        max_workers: æœ€å¤§å¹¶å‘æ•°

    Returns:
        å¤„ç†çš„æ–°é—»æ•°é‡
    """
    processed_count = 0
    tools = Tools()

    # åˆå§‹åŒ–æ–°é—»å»é‡å™¨
    deduplicator = NewsDeduplicator(threshold=tools.get_dedupe_threshold())

    # åˆ›å»ºå»é‡é›†åˆï¼ˆIDå»é‡ï¼‰
    seen_news = set()

    # ä½¿ç”¨ç»Ÿä¸€å¼‚æ­¥æ‰§è¡Œå™¨å’Œé™é€Ÿå™¨
    async_executor = AsyncExecutor()
    limiter = RateLimiter(rate_limit) if rate_limit > 0 else None

    async def handle_one(news: Dict) -> int:
        nonlocal processed_count
        try:
            news_id = news.get('id')
            source = news.get('source', 'unknown')
            if news_id:
                news_key = f"{source}:{news_id}"
                if news_key in seen_news:
                    return 0
                seen_news.add(news_key)

            title = news.get('title', '')
            content = news.get('content', '')
            if not title:
                return 0

            news_text = f"{title} {content}".strip()
            if deduplicator.is_duplicate(news_text):
                return 0

            # åº”ç”¨é™é€Ÿ
            if limiter:
                await limiter.acquire_async()

            loop = asyncio.get_running_loop()
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨LLM APIæ± ï¼Œä¸æ˜¯æ–°é—»APIæ± 
            from ..core import get_llm_pool
            api_pool = get_llm_pool()
            extracted = await loop.run_in_executor(None, llm_extract_events, title, content, api_pool)

            if extracted:
                all_entities = []
                for ev in extracted:
                    all_entities.extend(ev['entities'])

                if all_entities:
                    published_at = news.get('datetime')
                    if published_at and isinstance(published_at, datetime):
                        published_at = published_at.isoformat()

                    all_entities_original = all_entities
                    update_entities(all_entities, all_entities_original, source, published_at)
                    update_abstract_map(extracted, source, published_at)
                    return 1
        except Exception as e:
            tools.log(f"âš ï¸ å¤„ç†æ‹“å±•æ–°é—»å¤±è´¥: {e}")
        return 0

    tasks = [handle_one(news) for news in expanded_news]
    # ä½¿ç”¨AsyncExecutorè¿›è¡Œå¹¶å‘æ‰§è¡Œ
    results = await async_executor.run_concurrent_tasks(
        tasks=tasks,
        concurrency=max_workers
    )
    processed_count = sum(results)

    return processed_count


@register_tool(
    name="expand_news_by_recent_entities",
    description="[å·¥ä½œæµ] æ ¹æ®æœ€è¿‘å®ä½“æœç´¢ç›¸å…³æ–°é—»å¹¶æå–äº‹ä»¶",
    category="Workflow"
)
async def expand_news_by_recent_entities(
    entity_limit: int = 1,
    time_window_days: int = 30,
    limit_per_entity: int = 120,
    full_search: bool = False,
    rate_limit: float = 1.0,
    max_workers: int = 3
) -> Dict[str, Any]:
    """
    æ ¹æ®æœ€è¿‘å®ä½“æœç´¢ç›¸å…³æ–°é—»çš„å·¥ä½œæµ

    Args:
        entity_limit: å®ä½“æ•°é‡é™åˆ¶
        time_window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        limit_per_entity: æ¯ä¸ªå®ä½“æœç´¢æ–°é—»æ•°é‡
        full_search: æ˜¯å¦å…¨é¢æ£€ç´¢
        rate_limit: é€Ÿç‡é™åˆ¶
        max_workers: æœ€å¤§å¹¶å‘æ•°

    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    # ä»æ•°æ®åº“è·å–å·²å¤„ç†çš„ID
    from src.adapters.sqlite.store import get_store
    processed_ids = get_store().get_processed_ids()

    # è·å–æœ€è¿‘å®ä½“
    recent_entities = get_recent_entities(time_window_days=time_window_days, limit=entity_limit)
    if not recent_entities:
        tools.log("ğŸ“­ æ²¡æœ‰å¯ç”¨çš„å®ä½“è¿›è¡Œæ–°é—»æ‹“å±•")
        return {"processed_count": 0, "expanded_news_count": 0}

    # æœç´¢ç›¸å…³æ–°é—»
    tools.log(f"ğŸ” å¼€å§‹æœç´¢ {len(recent_entities)} ä¸ªå®ä½“çš„ç›¸å…³æ–°é—»...")
    expanded_news = await expand_news_by_entities(
        recent_entities,
        limit_per_entity=limit_per_entity,
        time_window_days=time_window_days,
        full_search=full_search
    )
    tools.log(f"âœ… å…±æœç´¢åˆ° {len(expanded_news)} æ¡ç›¸å…³æ–°é—»")

    # å¤„ç†æœç´¢åˆ°çš„æ–°é—»
    if expanded_news:
        deduped_path = persist_expanded_news_to_tmp(expanded_news, processed_ids)
        processed_count = 0
        if deduped_path and deduped_path.exists():
            tools.log(f"ğŸ“„ å¼€å§‹å¤„ç†æ‹“å±•çš„æ–°é—» (deduped: {deduped_path.name}) ...")
            news_list = []
            with open(deduped_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news_list.append(json.loads(line))
                    except Exception as e:
                        tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
            processed_count = await process_expanded_news(news_list, rate_limit=rate_limit, max_workers=max_workers)
            # æ¸…ç† tmp æ–‡ä»¶
            try:
                raw_file = tools.RAW_NEWS_TMP_DIR / deduped_path.name.replace("_deduped", "")
                file_paths = [raw_file, deduped_path]
                safe_unlink_multiple(file_paths, "ä¸´æ—¶")
            except Exception as e:
                tools.log(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        tools.log(f"âœ… æˆåŠŸå¤„ç† {processed_count} æ¡æ‹“å±•æ–°é—»")
        return {"processed_count": processed_count, "expanded_news_count": len(expanded_news)}

    return {"processed_count": 0, "expanded_news_count": 0}

