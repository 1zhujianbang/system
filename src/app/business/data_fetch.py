from typing import List, Dict, Any, Optional, Set
import pandas as pd
from ...infra.registry import register_tool
from ...infra.paths import tools as Tools
from ...core import ConfigManager, AsyncExecutor, RateLimiter, get_config_manager, get_llm_pool
from ...domain.data_operations import update_entities, update_abstract_map
from ...adapters.news.fetch_utils import fetch_from_multiple_sources, normalize_news_items
from .extraction import llm_extract_events, NewsDeduplicator, persist_expanded_news_to_tmp
from ...infra.file_utils import safe_unlink_multiple, safe_unlink
from ...domain.data_operations import sanitize_datetime_fields, write_jsonl_file
from pathlib import Path
import json
import asyncio
import time
from datetime import datetime, timedelta, timezone

# å¯¼å…¥æ–°é—»APIç®¡ç†å™¨
from ...core import NewsAPIManager

@register_tool(
    name="fetch_news_stream",
    description="ä»æ‰€æœ‰é…ç½®çš„æ•°æ®æºï¼ˆå½“å‰ä»… GNewsï¼‰è·å–æœ€æ–°æ–°é—»",
    category="Data Fetch"
)
async def fetch_news_stream(
    limit: int = 50,
    sources: Optional[List[str]] = None,
    # GNews å¯é€‰å‚æ•°
    category: Optional[str] = None,
    query: Optional[str] = None,
    from_: Optional[str] = None,
    to: Optional[str] = None,
    nullable: Optional[str] = None,
    truncate: Optional[str] = None,
    sortby: Optional[str] = None,
    in_fields: Optional[str] = None,
    page: Optional[int] = None,
    daily_incremental: bool = False,  # æ–°å¢å‚æ•°ï¼šæ˜¯å¦æŒ‰å¤©é€’å¢è¯·æ±‚
) -> List[Dict[str, Any]]:
    """
    è·å–å…¨æ¸ é“æ–°é—»æ•°æ®ã€‚

    Args:
        limit: æ¯ä¸ªæºè·å–çš„æœ€å¤§æ¡æ•°
        sources: æŒ‡å®šæºåˆ—è¡¨ (å¦‚ ["GNews-cn"]), é»˜è®¤ä¸ºæ‰€æœ‰å¯ç”¨æº
        daily_incremental: æ˜¯å¦æŒ‰å¤©é€’å¢è¯·æ±‚æ•°æ®ï¼ˆä»from_å¼€å§‹ï¼Œæ¯å¤©è¯·æ±‚ä¸€æ¬¡ï¼Œç›´åˆ°to_ï¼‰

    Returns:
        æ–°é—»åˆ—è¡¨ (List[Dict])
    """
    tools = Tools()
    tools.log(f"[fetch_news_stream] å¼€å§‹æ‰§è¡Œï¼Œè¯·æ±‚æº: {sources}, limit: {limit}, daily_incremental: {daily_incremental}")
    
    # é…ç½®é©±åŠ¨çš„å¹¶å‘ä¸Šé™ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ï¼‰
    config_manager = get_config_manager()
    concurrency = config_manager.get_concurrency_limit("agent1_config")
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«ä¸ºDEBUGä»¥ä¾¿è·å–æ›´å¤šè°ƒè¯•ä¿¡æ¯
    from ...infra.logging import LoggerManager
    LoggerManager.set_level("DEBUG")

    # åˆå§‹åŒ– API Pool
    api_pool = NewsAPIManager.get_instance()
    available_sources = api_pool.list_available_sources()
    tools.log(f"[fetch_news_stream] å¯ç”¨æ•°æ®æº: {available_sources}")
    
    if sources:
        target_sources = [s for s in sources if s in available_sources]
        tools.log(f"[fetch_news_stream] ç­›é€‰åç›®æ ‡æº: {target_sources}")
    else:
        target_sources = available_sources

    if not target_sources:
        tools.log("Warning: No valid sources to fetch from. APIå¯†é’¥å¯èƒ½æœªé…ç½®ã€‚")
        return []

    # å¦‚æœå¯ç”¨äº†æŒ‰å¤©é€’å¢è¯·æ±‚ï¼Œå¹¶ä¸”æä¾›äº†from_å’Œto_å‚æ•°
    if daily_incremental and from_ and to:
        tools.log(f"[fetch_news_stream] å¯ç”¨æŒ‰å¤©é€’å¢è¯·æ±‚: ä» {from_} åˆ° {to}")
        all_news = []
        
        # å®šä¹‰æ‰€æœ‰å¯ç”¨çš„ç±»åˆ«
        categories = ["general", "world", "nation", "business", "technology", "entertainment", "sports", "science", "health"]
        
        # è§£ææ—¥æœŸ
        try:
            from_date = datetime.fromisoformat(from_.replace("Z", "+00:00")).date()
            to_date = datetime.fromisoformat(to.replace("Z", "+00:00")).date()
            
            # æŒ‰å¤©é€’å¢è¯·æ±‚æ•°æ®
            current_date = from_date
            while current_date <= to_date:
                # æ„é€ å½“å¤©çš„æ—¥æœŸèŒƒå›´
                day_start = f"{current_date.isoformat()}T00:00:00.000Z"
                day_end = f"{current_date.isoformat()}T23:59:59.999Z"
                
                tools.log(f"[fetch_news_stream] è¯·æ±‚ {current_date.isoformat()} çš„æ•°æ®")
                
                # ä¸ºæ¯ä¸ªç±»åˆ«è·å–æ•°æ®
                day_news = []
                for cat in categories:
                    tools.log(f"[fetch_news_stream] è¯·æ±‚ {current_date.isoformat()} çš„ {cat} ç±»åˆ«æ•°æ®")
                    cat_news = await fetch_from_multiple_sources(
                        api_pool=api_pool,
                        source_names=target_sources,
                        concurrency_limit=concurrency,
                        query=query,
                        category=cat,
                        limit=limit,
                        from_=day_start,
                        to=day_end,
                        nullable=nullable,
                        truncate=truncate,
                        sortby=sortby,
                        in_fields=in_fields,
                        page=page,
                    )
                    day_news.extend(cat_news)
                    tools.log(f"[fetch_news_stream] {current_date.isoformat()} çš„ {cat} ç±»åˆ«è·å–åˆ° {len(cat_news)} æ¡æ•°æ®")
                
                all_news.extend(day_news)
                tools.log(f"[fetch_news_stream] {current_date.isoformat()} æ€»å…±è·å–åˆ° {len(day_news)} æ¡æ•°æ®")
                
                # ç§»åŠ¨åˆ°ä¸‹ä¸€å¤©
                current_date = current_date + timedelta(days=1)                
        except Exception as e:
            tools.log(f"[fetch_news_stream] æŒ‰å¤©é€’å¢è¯·æ±‚å‡ºé”™: {e}")
            # å¦‚æœå‡ºé”™ï¼Œå›é€€åˆ°åŸæ¥çš„å®ç°
            return await fetch_from_multiple_sources(
                api_pool=api_pool,
                source_names=target_sources,
                concurrency_limit=concurrency,
                query=query,
                category=category,
                limit=limit,
                from_=from_,
                to=to,
                nullable=nullable,
                truncate=truncate,
                sortby=sortby,
                in_fields=in_fields,
                page=page,
            )
            
        tools.log(f"[fetch_news_stream] æŒ‰å¤©é€’å¢è¯·æ±‚å®Œæˆï¼Œæ€»å…±è·å–åˆ° {len(all_news)} æ¡æ•°æ®")
        return all_news
    else:
        # å¦‚æœæŒ‡å®šäº†ç±»åˆ«ï¼Œåˆ™åªä½¿ç”¨è¯¥ç±»åˆ«ï¼›å¦åˆ™éå†æ‰€æœ‰ç±»åˆ«
        categories = ["general", "world", "nation", "business", "technology", "entertainment", "sports", "science", "health"]
        
        # ä¸ºæ¯ä¸ªç±»åˆ«è·å–æ•°æ®å¹¶åˆå¹¶
        all_news = []
        for cat in categories:
            cat_news = await fetch_from_multiple_sources(
                api_pool=api_pool,
                source_names=target_sources,
                concurrency_limit=concurrency,
                query=query,
                category=cat,
                limit=limit,
                from_=from_,
                to=to,
                nullable=nullable,
                truncate=truncate,
                sortby=sortby,
                in_fields=in_fields,
                page=page,
            )
            all_news.extend(cat_news)
        
        return all_news

def _load_entity_equivs() -> Dict[str, Set[str]]:
    """
    æ„å»ºå®ä½“åŠåŒä¹‰è¯ç´¢å¼•ï¼šå®ä½“åº“ original_forms + åˆå¹¶è§„åˆ™çš„åˆ«åã€‚
    è¿”å› dict: è¯ -> åŒä¹‰é›†åˆï¼ˆåŒ…å«è‡ªèº«ï¼‰ã€‚
    """
    tools = Tools()
    idx: Dict[str, Set[str]] = {}

    def add_forms(key: str, forms: List[str]):
        if not key:
            return
        bucket = idx.setdefault(key, set())
        for f in forms:
            if f:
                bucket.add(f)
        bucket.add(key)

    # å®ä½“åº“ original_forms
    try:
        if tools.ENTITIES_FILE.exists():
            with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
                ents = json.load(f)
            for name, data in ents.items():
                forms = data.get("original_forms", []) if isinstance(data, dict) else []
                if isinstance(forms, list):
                    flat = []
                    for x in forms:
                        if isinstance(x, str):
                            flat.append(x)
                        elif isinstance(x, list):
                            flat.extend([str(i) for i in x])
                    add_forms(name, flat)
                else:
                    add_forms(name, [])
    except Exception:
        pass

    # åˆå¹¶è§„åˆ™ï¼ˆalias -> primaryï¼‰ï¼ŒåŒå‘åŠ å…¥
    try:
        merge_rules_file = tools.CONFIG_DIR / "entity_merge_rules.json"
        if merge_rules_file.exists():
            with open(merge_rules_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            rules = data.get("merge_rules", {}) if isinstance(data, dict) else {}
            inv: Dict[str, List[str]] = {}
            for alias, primary in rules.items():
                add_forms(alias, [primary])
                inv.setdefault(primary, []).append(alias)
            for primary, aliases in inv.items():
                add_forms(primary, aliases)
    except Exception:
        pass

    # å±•å¼€ï¼šç¡®ä¿äº’ç›¸åŒ…å«
    for k, forms in list(idx.items()):
        for f in list(forms):
            if f in idx:
                forms.update(idx[f])
    return idx


def _expand_keywords(keywords: List[str]) -> List[List[str]]:
    """
    å°†æ¯ä¸ªå…³é”®è¯æ‰©å±•ä¸ºåŒä¹‰é›†åˆåˆ—è¡¨ï¼ˆæŒ‰è¾“å…¥é¡ºåºä¿ç•™ç»„ï¼‰ã€‚
    """
    idx = _load_entity_equivs()
    groups: List[List[str]] = []
    for kw in keywords:
        kw_norm = (kw or "").strip()
        if not kw_norm:
            continue
        forms = set([kw_norm])
        if kw_norm in idx:
            forms.update(idx[kw_norm])
        groups.append(list(forms))
    return groups


def _build_boolean_query(groups: List[List[str]]) -> str:
    """
    æ„å»ºgnews.io APIæŸ¥è¯¢
    è§„åˆ™ï¼š
    1. æ¯ä¸ªåˆ†ç»„å†…çš„è¯ç”¨ORè¿æ¥
    2. ä¸åŒåˆ†ç»„ä¹‹é—´ç”¨ANDè¿æ¥  
    """
    if not groups:
        return ""
    
    query_parts = []
    
    for group in groups:
        if not group:
            continue
            
        # æ¸…ç†ç»„å†…çš„è¯
        valid_terms = []
        for term in group:
            if isinstance(term, str):
                term = term.strip()
                if term:
                    valid_terms.append(term)
        
        if not valid_terms:
            continue
            
        # å¤„ç†å•ä¸ªè¯
        if len(valid_terms) == 1:
            term = valid_terms[0]
            query_parts.append(f'"{term}"')
        # å¤„ç†å¤šä¸ªè¯
        else:
            or_terms = []
            for term in valid_terms:
                or_terms.append(f'"{term}"')
            query_parts.append(f"{' OR '.join(or_terms)}")
    
    if not query_parts:
        return ""
    
    base_query = " AND ".join(query_parts)
    return base_query



@register_tool(
    name="search_news_by_keywords",
    description="æŒ‰å…³é”®è¯æœç´¢æ–°é—»ï¼ˆå½“å‰ä»… GNewsï¼‰ï¼Œæ”¯æŒå¯é€‰æ—¶é—´èŒƒå›´ä¸æ’åº",
    category="Data Fetch"
)
async def search_news_by_keywords(
    keywords: List[str],
    apis: Optional[List[str]] = None,
    limit: int = 10,
    category: Optional[str] = None,
    from_: Optional[str] = None,
    to: Optional[str] = None,
    nullable: Optional[str] = None,
    truncate: Optional[str] = None,
    sortby: Optional[str] = None,
    in_fields: Optional[str] = None,
    page: Optional[int] = None,
) -> List[Dict[str, Any]]:
    """
    æ ¹æ®å…³é”®è¯åˆ—è¡¨æœç´¢æ–°é—»ï¼ˆGNewsï¼‰ï¼Œè¿”å›åˆå¹¶åçš„ç»“æœã€‚
    """
    tools = Tools()
    if not keywords:
        return []

    # æ„é€  (A OR A2) AND (B OR B2) æŸ¥è¯¢ä¸²
    groups = _expand_keywords(keywords)
    query_str = _build_boolean_query(groups)
    if not query_str:
        return []

    # åˆå§‹åŒ– API Pool
    api_pool = NewsAPIManager.get_instance()

    # å¹¶å‘ä¸Šé™ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ï¼‰
    config_manager = get_config_manager()
    concurrency = config_manager.get_concurrency_limit("agent2_config")

    available_sources = api_pool.list_available_sources()
    if apis:
        target_sources = [s for s in apis if s in available_sources]
    else:
        target_sources = available_sources

    if not target_sources:
        tools.log("Warning: No valid sources to search from.")
        return []

    # ä½¿ç”¨å…¬å…±å‡½æ•°è·å–æ•°æ®
    return await fetch_from_multiple_sources(
        api_pool=api_pool,
        source_names=target_sources,
        concurrency_limit=concurrency,
        query=query_str,
        category=category,
        limit=limit,
        from_=from_,
        to=to,
        nullable=nullable,
        truncate=truncate,
        sortby=sortby,
        in_fields=in_fields,
        page=page,
    )


async def expand_news_by_entities(entities: List[Dict], limit_per_entity: int = 10, time_window_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    æ ¹æ®å®ä½“åˆ—è¡¨æœç´¢ç›¸å…³æ–°é—»ï¼Œæ”¯æŒä½¿ç”¨åŸå§‹è¯è¿›è¡Œæ£€ç´¢

    Args:
        entities: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«nameå’Œoriginal_formså­—æ®µ
        limit_per_entity: æ¯ä¸ªå®ä½“æœç´¢çš„æ–°é—»æ•°é‡é™åˆ¶
        time_window_days: é»˜è®¤æ£€ç´¢æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢ï¼Œå¦‚æœä¸ºTrueåˆ™ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´

    Returns:
        æœç´¢åˆ°çš„ç›¸å…³æ–°é—»åˆ—è¡¨
    """
    expanded_news = []
    news_id_set = set()  # ç”¨äºå»é‡

    # è·å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨
    news_collectors = []
    api_pool = NewsAPIManager.get_instance()
    available_sources = api_pool.list_available_sources()

    # ä¸æ›´æ–°åçš„APIæ± å…¼å®¹ï¼Œç§»é™¤å¯èƒ½çš„å¤‡ç”¨é€»è¾‘
    for source_name in available_sources:
        try:
            collector = api_pool.get_collector(source_name)
            news_collectors.append(collector)
        except Exception as e:
            Tools().log(f"âš ï¸ æ— æ³•åˆ›å»ºæ–°é—»æ”¶é›†å™¨ {source_name}: {e}")

    if not news_collectors:
        Tools().log("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨")
        return expanded_news

    # ä¸ºæ¯ä¸ªå®ä½“æœç´¢ç›¸å…³æ–°é—»
    for entity in entities:
        entity_name = entity['name']
        original_forms = entity.get('original_forms', [])

        # æ„å»ºä½¿ç”¨ORæ“ä½œç¬¦è¿æ¥çš„æœç´¢æŸ¥è¯¢ï¼šå®ä½“åç§° + æ‰€æœ‰åŸå§‹è¯
        all_terms = [entity_name] + original_forms

        # ç”ŸæˆæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶
        query_batches = []
        current_batch = []
        current_length = 0

        for term in all_terms:
            quoted_term = f'"{term}"'
            term_length = len(quoted_term)

            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯ï¼Œç›´æ¥æ·»åŠ ï¼›å¦åˆ™éœ€è¦è€ƒè™‘ORæ“ä½œç¬¦çš„é•¿åº¦
            if current_batch:
                required_length = current_length + 4 + term_length  # 4æ˜¯" OR "çš„é•¿åº¦
                if required_length > 200:
                    # å¦‚æœæ·»åŠ å½“å‰è¯ä¼šè¶…è¿‡é™åˆ¶ï¼Œä¿å­˜å½“å‰æ‰¹æ¬¡å¹¶å¼€å§‹æ–°æ‰¹æ¬¡
                    query_batches.append(" OR ".join(current_batch))
                    current_batch = [quoted_term]
                    current_length = term_length
                else:
                    current_batch.append(quoted_term)
                    current_length = required_length
            else:
                current_batch.append(quoted_term)
                current_length = term_length

        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            query_batches.append(" OR ".join(current_batch))

        Tools().log(f"ğŸ” ä¸ºå®ä½“ '{entity_name}' æœç´¢ç›¸å…³æ–°é—»ï¼Œä½¿ç”¨ORæŸ¥è¯¢è¿æ¥ {len(original_forms)} ä¸ªåŸå§‹è¯...")
        Tools().log(f"   ğŸ“ ç”Ÿæˆäº† {len(query_batches)} ä¸ªæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶")

        # è·å–æ—¶é—´èŒƒå›´
        time_ranges = get_time_ranges(time_window_days, full_search)

        for collector in news_collectors:
            try:
                # å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰¹æ¬¡å’Œæ—¶é—´èŒƒå›´è¿›è¡Œæœç´¢
                for time_range in time_ranges:
                    start_date = time_range['start']
                    end_date = time_range['end']

                    Tools().log(f"   ğŸ“… æœç´¢æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")

                    for batch_index, batch_query in enumerate(query_batches):
                        Tools().log(f"   ğŸ“ æ‰§è¡ŒæŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1}/{len(query_batches)}: '{batch_query}'")

                        try:
                            # ä½¿ç”¨æœç´¢åŠŸèƒ½è·å–ç›¸å…³æ–°é—»ï¼Œä¼ å…¥æ—¶é—´èŒƒå›´å‚æ•°
                            search_params = {
                                'keyword' if hasattr(collector, 'search_news_by_keyword') else 'query': batch_query,
                                'limit': limit_per_entity // (len(query_batches) * len(time_ranges)) + 1  # å¹³å‡åˆ†é…é™åˆ¶
                            }

                            # å¦‚æœæ”¶é›†å™¨æ”¯æŒæ—¶é—´èŒƒå›´å‚æ•°ï¼Œåˆ™æ·»åŠ 
                            if hasattr(collector, 'search_news_by_keyword'):
                                if 'start_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date
                            elif hasattr(collector, 'search'):
                                if 'start_date' in collector.search.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date

                            # ä½¿ç”¨æ›´æ–°åçš„APIè°ƒç”¨æ–¹æ³•
                            # ä¼˜å…ˆä½¿ç”¨search_news_by_keywordæ–¹æ³•ï¼Œç¡®ä¿ä¸æ›´æ–°åçš„APIæ± å…¼å®¹
                            if hasattr(collector, 'search_news_by_keyword'):
                                news_list = await collector.search_news_by_keyword(**search_params)
                            elif hasattr(collector, 'search'):
                                news_list = await collector.search(**search_params)
                            else:
                                Tools().log(f"âš ï¸ æ”¶é›†å™¨ {collector.__class__.__name__} æ²¡æœ‰æ”¯æŒçš„æœç´¢æ–¹æ³•")
                                continue

                            # ä¸ºæ¯æ¡æ–°é—»æ·»åŠ å®ä½“æ ‡ç­¾å¹¶å»é‡
                            for news in news_list:
                                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦ç”¨äºå»é‡
                                news_id = f"{news.get('url', '')}_{news.get('publishedAt', '')}"
                                if news_id not in news_id_set:
                                    news_id_set.add(news_id)
                                    news['expanded_from_entity'] = entity_name
                                    news['search_term'] = batch_query  # è®°å½•ä½¿ç”¨çš„æœç´¢è¯
                                    news['source'] = collector.__class__.__name__.replace('Collector', '').lower()
                                    news['query_batch'] = batch_index + 1  # è®°å½•æŸ¥è¯¢æ‰¹æ¬¡
                                    news['search_time_range'] = f"{start_date} to {end_date}"  # è®°å½•æœç´¢æ—¶é—´èŒƒå›´
                                    expanded_news.append(news)
                        except Exception as batch_error:
                            Tools().log(f"âš ï¸ æŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1} æ‰§è¡Œå¤±è´¥: {batch_error}")
            except Exception as e:
                Tools().log(f"âš ï¸ ä» {collector.__class__.__name__} æœç´¢å¤±è´¥: {e}")

    return expanded_news


def get_time_ranges(default_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    è·å–æœç´¢çš„æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
    1. æ—¶é—´èŒƒå›´åªèƒ½ä»2020å¹´è‡³ä»Š
    2. é»˜è®¤æ£€ç´¢å‰30å¤©å†…çš„æ–°é—»
    3. å…¨é¢æ£€ç´¢æ—¶ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´

    Args:
        default_days: é»˜è®¤çš„æ—¶é—´çª—å£å¤©æ•°ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢

    Returns:
        æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«startå’Œendæ—¥æœŸå­—ç¬¦ä¸²
    """
    time_ranges = []
    now = datetime.now(timezone.utc)

    # å®šä¹‰2020å¹´1æœˆ1æ—¥ä½œä¸ºèµ·å§‹æ—¶é—´
    start_date_2020 = datetime(2020, 1, 1, tzinfo=timezone.utc)

    if not full_search:
        # éå…¨é¢æ£€ç´¢ï¼Œåªè¿”å›é»˜è®¤æ—¶é—´çª—å£
        end_date = now
        start_date = max(start_date_2020, now - timedelta(days=default_days))
        time_ranges.append({
            'start': start_date.strftime('%Y-%m-%d'),
            'end': end_date.strftime('%Y-%m-%d')
        })
    else:
        # å…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´
        Tools().log("ğŸ”„ æ‰§è¡Œå…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æ‰¹æ¬¡ç›´åˆ°2020å¹´...")

        end_date = now
        batch_count = 0

        while end_date > start_date_2020:
            start_date = max(start_date_2020, end_date - timedelta(days=default_days))

            # ç¡®ä¿ä¸é‡å¤æ·»åŠ ç›¸åŒçš„æ—¶é—´èŒƒå›´
            if not time_ranges or time_ranges[-1]['start'] != start_date.strftime('%Y-%m-%d'):
                time_ranges.append({
                    'start': start_date.strftime('%Y-%m-%d'),
                    'end': end_date.strftime('%Y-%m-%d')
                })

            batch_count += 1
            end_date = start_date - timedelta(days=1)  # é¿å…æ—¥æœŸé‡å 

        Tools().log(f"âœ… ç”Ÿæˆäº† {batch_count} ä¸ªæ—¶é—´èŒƒå›´æ‰¹æ¬¡")

    return time_ranges


def get_recent_entities(time_window_days: int = 30, limit: int = 50) -> List[Dict]:
    """
    è·å–æœ€è¿‘æ—¶é—´çª—å£å†…çš„å®ä½“åˆ—è¡¨ï¼ŒåŒ…å«åŸå§‹è¯ä¿¡æ¯

    Args:
        time_window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        limit: è¿”å›çš„å®ä½“æ•°é‡é™åˆ¶

    Returns:
        æœ€è¿‘çš„å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«åç§°å’ŒåŸå§‹è¯ä¿¡æ¯
    """
    entities = []
    tools = Tools()

    if not tools.ENTITIES_FILE.exists():
        tools.log("âš ï¸ å®ä½“åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return entities

    # è¯»å–å®ä½“åº“
    with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
        entity_data = json.load(f)

    # æ ¹æ® first_seen æ’åºï¼Œè·å–æœ€è¿‘çš„å®ä½“
    sorted_entities = sorted(
        entity_data.items(),
        key=lambda x: x[1].get('first_seen', ''),
        reverse=True
    )

    # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„å®ä½“
    now = datetime.now(timezone.utc)
    time_window = timedelta(days=time_window_days)

    for entity_name, entity_info in sorted_entities:
        first_seen = entity_info.get('first_seen')
        if first_seen:
            try:
                # è§£ææ—¶é—´å­—ç¬¦ä¸²
                if 'T' in first_seen:
                    # ISOæ ¼å¼æ—¶é—´
                    seen_time = datetime.fromisoformat(first_seen.replace('Z', '+00:00'))
                else:
                    # æ™®é€šæ ¼å¼æ—¶é—´
                    seen_time = datetime.strptime(first_seen, '%Y-%m-%d %H:%M:%S')
                    seen_time = seen_time.replace(tzinfo=timezone.utc)

                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
                if now - seen_time <= time_window:
                    entity_info = {
                        'name': entity_name,
                        'original_forms': entity_data[entity_name].get('original_forms', [])
                    }
                    entities.append(entity_info)
                    if len(entities) >= limit:
                        break
            except Exception as e:
                tools.log(f"âš ï¸ è§£æå®ä½“ '{entity_name}' çš„æ—¶é—´æˆ³å¤±è´¥: {e}")

    tools.log(f"âœ… è·å–äº† {len(entities)} ä¸ªæœ€è¿‘å®ä½“")
    return entities


async def process_expanded_news(expanded_news: List[Dict], rate_limit: float = 1.0, max_workers: int = 3) -> int:
    """
    å¤„ç†æ‹“å±•çš„æ–°é—»ï¼Œæå–å®ä½“å’Œäº‹ä»¶

    Args:
        expanded_news: æ‹“å±•çš„æ–°é—»åˆ—è¡¨
        rate_limit: æ¯ç§’é€Ÿç‡é™åˆ¶
        max_workers: æœ€å¤§å¹¶å‘æ•°

    Returns:
        å¤„ç†çš„æ–°é—»æ•°é‡
    """
    processed_count = 0
    tools = Tools()

    # åˆå§‹åŒ–æ–°é—»å»é‡å™¨
    deduplicator = NewsDeduplicator(threshold=tools.get_dedupe_threshold())

    # åˆ›å»ºå»é‡é›†åˆï¼ˆIDå»é‡ï¼‰
    seen_news = set()

    # ä½¿ç”¨ç»Ÿä¸€å¼‚æ­¥æ‰§è¡Œå™¨å’Œé™é€Ÿå™¨
    async_executor = AsyncExecutor()
    limiter = RateLimiter(rate_limit) if rate_limit > 0 else None

    async def handle_one(news: Dict) -> int:
        nonlocal processed_count
        try:
            news_id = news.get('id')
            source = news.get('source', 'unknown')
            if news_id:
                news_key = f"{source}:{news_id}"
                if news_key in seen_news:
                    return 0
                seen_news.add(news_key)

            title = news.get('title', '')
            content = news.get('content', '')
            if not title:
                return 0

            news_text = f"{title} {content}".strip()
            if deduplicator.is_duplicate(news_text):
                return 0

            # åº”ç”¨é™é€Ÿ
            if limiter:
                await limiter.acquire_async()

            loop = asyncio.get_running_loop()
            # è¿™é‡Œåº”è¯¥ä½¿ç”¨LLM APIæ± ï¼Œä¸æ˜¯æ–°é—»APIæ± 
            from ..core import get_llm_pool
            api_pool = get_llm_pool()
            extracted = await loop.run_in_executor(None, llm_extract_events, title, content, api_pool)

            if extracted:
                all_entities = []
                for ev in extracted:
                    all_entities.extend(ev['entities'])

                if all_entities:
                    published_at = news.get('datetime')
                    if published_at and isinstance(published_at, datetime):
                        published_at = published_at.isoformat()

                    all_entities_original = all_entities
                    update_entities(all_entities, all_entities_original, source, published_at)
                    update_abstract_map(extracted, source, published_at)
                    return 1
        except Exception as e:
            tools.log(f"âš ï¸ å¤„ç†æ‹“å±•æ–°é—»å¤±è´¥: {e}")
        return 0

    tasks = [handle_one(news) for news in expanded_news]
    # ä½¿ç”¨AsyncExecutorè¿›è¡Œå¹¶å‘æ‰§è¡Œ
    results = await async_executor.run_concurrent_tasks(
        tasks=tasks,
        concurrency=max_workers
    )
    processed_count = sum(results)

    return processed_count


@register_tool(
    name="expand_news_by_recent_entities",
    description="[å·¥ä½œæµ] æ ¹æ®æœ€è¿‘å®ä½“æœç´¢ç›¸å…³æ–°é—»å¹¶æå–äº‹ä»¶",
    category="Workflow"
)
async def expand_news_by_recent_entities(
    entity_limit: int = 1,
    time_window_days: int = 30,
    limit_per_entity: int = 120,
    full_search: bool = False,
    rate_limit: float = 1.0,
    max_workers: int = 3
) -> Dict[str, Any]:
    """
    æ ¹æ®æœ€è¿‘å®ä½“æœç´¢ç›¸å…³æ–°é—»çš„å·¥ä½œæµ

    Args:
        entity_limit: å®ä½“æ•°é‡é™åˆ¶
        time_window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        limit_per_entity: æ¯ä¸ªå®ä½“æœç´¢æ–°é—»æ•°é‡
        full_search: æ˜¯å¦å…¨é¢æ£€ç´¢
        rate_limit: é€Ÿç‡é™åˆ¶
        max_workers: æœ€å¤§å¹¶å‘æ•°

    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    # ä»æ•°æ®åº“è·å–å·²å¤„ç†çš„ID
    from src.adapters.sqlite.store import get_store
    processed_ids = get_store().get_processed_ids()

    # è·å–æœ€è¿‘å®ä½“
    recent_entities = get_recent_entities(time_window_days=time_window_days, limit=entity_limit)
    if not recent_entities:
        tools.log("ğŸ“­ æ²¡æœ‰å¯ç”¨çš„å®ä½“è¿›è¡Œæ–°é—»æ‹“å±•")
        return {"processed_count": 0, "expanded_news_count": 0}

    # æœç´¢ç›¸å…³æ–°é—»
    tools.log(f"ğŸ” å¼€å§‹æœç´¢ {len(recent_entities)} ä¸ªå®ä½“çš„ç›¸å…³æ–°é—»...")
    expanded_news = await expand_news_by_entities(
        recent_entities,
        limit_per_entity=limit_per_entity,
        time_window_days=time_window_days,
        full_search=full_search
    )
    tools.log(f"âœ… å…±æœç´¢åˆ° {len(expanded_news)} æ¡ç›¸å…³æ–°é—»")

    # å¤„ç†æœç´¢åˆ°çš„æ–°é—»
    if expanded_news:
        deduped_path = persist_expanded_news_to_tmp(expanded_news, processed_ids)
        processed_count = 0
        if deduped_path and deduped_path.exists():
            tools.log(f"ğŸ“„ å¼€å§‹å¤„ç†æ‹“å±•çš„æ–°é—» (deduped: {deduped_path.name}) ...")
            news_list = []
            with open(deduped_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news_list.append(json.loads(line))
                    except Exception as e:
                        tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
            processed_count = await process_expanded_news(news_list, rate_limit=rate_limit, max_workers=max_workers)
            # æ¸…ç† tmp æ–‡ä»¶
            try:
                raw_file = tools.RAW_NEWS_TMP_DIR / deduped_path.name.replace("_deduped", "")
                file_paths = [raw_file, deduped_path]
                safe_unlink_multiple(file_paths, "ä¸´æ—¶")
            except Exception as e:
                tools.log(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        tools.log(f"âœ… æˆåŠŸå¤„ç† {processed_count} æ¡æ‹“å±•æ–°é—»")
        return {"processed_count": processed_count, "expanded_news_count": len(expanded_news)}

    return {"processed_count": 0, "expanded_news_count": 0}

