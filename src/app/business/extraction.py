from typing import List, Dict, Any, Optional, Set, Tuple
from ...infra.registry import register_tool
from ...core import ConfigManager, RateLimiter, LLMAPIPool, AsyncExecutor, tools, get_config_manager, get_llm_pool
from ...domain.data_operations import update_entities, update_abstract_map
from ...infra.serialization import extract_json_from_llm_response
from ...infra.async_utils import call_llm_with_retry, create_extraction_prompt
from ...infra.file_utils import ensure_dirs, safe_unlink, generate_timestamp
from ...domain.data_operations import write_jsonl_file, sanitize_datetime_fields, create_temp_file_path
import json
from pathlib import Path
import time
import threading
import asyncio
import datetime
from datetime import datetime as dt, timezone, timedelta
from collections import defaultdict

class NewsDeduplicator:
    """æ–°é—»å»é‡å™¨ï¼Œæ”¯æŒä¾èµ–æ³¨å…¥"""

    def __init__(self, threshold: int = 3):
        self.threshold = threshold
        self.seen_hashes: Set[int] = set()

    @staticmethod
    def _news_key(news: Dict) -> str:
        """æ„é€ ç”¨äºå»é‡çš„å”¯ä¸€é”®ï¼ŒåŒ…å« source å‰ç¼€ï¼Œå…¼å®¹å¤šæ•°æ®æºã€‚"""
        return f"{news.get('source', 'unknown')}:{news.get('id')}"

    def is_duplicate(self, text: str) -> bool:
        h = tools.simhash(text)
        for seen_h in self.seen_hashes:
            if tools.hamming_distance(h, seen_h) <= self.threshold:
                return True
        self.seen_hashes.add(h)
        return False

    def dedupe_file(self, input_path: Path, output_path: Path, processed_ids: Optional[Set[str]] = None):
        """
        å¯¹å•ä¸ªåŸå§‹æ–‡ä»¶åšå»é‡ï¼š
        - å…ˆç”¨ processed_idsï¼ˆå…¨å±€å·²å¤„ç† IDï¼Œå¦‚ blockbeats:323066ï¼‰è¿‡æ»¤å†å²å·²å¤„ç†æ–°é—»
        - å†ç»“åˆå·²æœ‰å»é‡æ–‡ä»¶ & simhash å»æ‰æœ¬æ‰¹å†…/è·¨æ‰¹çš„é‡å¤å†…å®¹
        """
        tools.log(f"ğŸ” å»é‡ä¸­: {input_path.name}")

        # å…ˆåŠ è½½"å…¨å±€å·²å¤„ç† ID"ï¼Œé¿å…è€æ–°é—»å†æ¬¡è¿›å…¥å»é‡ç»“æœ
        seen_ids: Set[str] = set(processed_ids or set())
        if processed_ids:
            tools.log(f"ğŸ” å·²æœ‰å†å² processed_ids æ•°é‡: {len(processed_ids)}")

        # å†åŠ è½½å·²æœ‰å»é‡ç»“æœæ–‡ä»¶ä¸­çš„ IDï¼Œå®ç°è·¨æ‰¹æ¬¡çš„æœ¬åœ°å»é‡
        if output_path.exists():
            with open(output_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        item = json.loads(line)
                        seen_ids.add(self._news_key(item))
                    except Exception as e:
                        tools.log(f"âš ï¸ è¯»å–å†å²å»é‡æ–‡ä»¶æ—¶è·³è¿‡æ— æ•ˆè¡Œ: {e}")

        kept, skipped_id, skipped_sim = 0, 0, 0
        with open(input_path, "r", encoding="utf-8") as fin, \
             open(output_path, "a", encoding="utf-8") as fout:
            for line in fin:
                try:
                    news = json.loads(line)
                    key = self._news_key(news)

                    # 1) æŒ‰å…¨å±€ ID å»é‡ï¼ˆåŒ…æ‹¬ processed_ids å’Œå·²æœ‰å»é‡æ–‡ä»¶ä¸­çš„ IDï¼‰
                    if key in seen_ids:
                        skipped_id += 1
                        continue

                    # 2) æ„é€ æ–‡æœ¬ï¼ŒæŒ‰å†…å®¹ç›¸ä¼¼åº¦å»é‡
                    raw_text = (news.get("title", "") + " " + news.get("content", "")).strip()
                    if not raw_text:
                        continue
                    if self.is_duplicate(raw_text):
                        skipped_sim += 1
                        continue
                    fout.write(line)
                    seen_ids.add(key)
                    kept += 1
                except Exception as e:
                    tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
        tools.log(f"âœ… å»é‡å®Œæˆ: ä¿ç•™ {kept} æ¡, æŒ‰ ID è·³è¿‡ {skipped_id} æ¡, æŒ‰ç›¸ä¼¼åº¦è·³è¿‡ {skipped_sim} æ¡")


def llm_extract_events(
    title: str,
    content: str,
    api_pool: LLMAPIPool,
    max_retries: int = 2,
    reported_at: Optional[str] = None
) -> List[Dict]:
    """ä½¿ç”¨LLMæå–äº‹ä»¶ï¼Œæ”¯æŒä¾èµ–æ³¨å…¥"""
    tools.log(f"[LLMè¯·æ±‚] å¼€å§‹å¤„ç†æ–°é—»: {title[:100]}...")
    if api_pool is None:
        tools.log("[LLMè¯·æ±‚] âŒ API æ± æœªåˆå§‹åŒ–")
        return []

    # ä½¿ç”¨å·¥å…·å‡½æ•°åˆ›å»ºæç¤º
    tools.log("[LLMè¯·æ±‚] æ„å»ºæç¤ºè¯")
    entity_definitions = """
å¿…è¦æ¡ä»¶ï¼š
- æ˜¯æ„æˆè¯¥äº‹ä»¶å¿…è¦çš„å®ä½“ï¼Œè‹¥è¯¥å®ä½“åœ¨äº‹ä»¶ä¸­ç¼ºå¤±åˆ™å¯èƒ½å¯¼è‡´äº‹ä»¶ä¸å®Œå¤‡çš„å®ä½“

âœ… å¿…é¡»æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
- è‡ªç„¶äººï¼ˆå¦‚ Elon Muskã€Cathie Woodã€Warren Buffettï¼‰
- æ³¨å†Œå…¬å¸ï¼ˆå¦‚ Apple Inc.ã€Goldman Sachsã€ä¸­å›½å·¥å•†é“¶è¡Œã€Volkswagen AGï¼‰
- æ”¿åºœæœºæ„æˆ–éƒ¨é—¨ï¼ˆå¦‚ ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šã€ä¸­å›½äººæ°‘é“¶è¡Œã€æ¬§ç›Ÿå§”å‘˜ä¼šã€æ—¥æœ¬é‡‘èå…ï¼‰
- ä¸»æƒå›½å®¶æˆ–æ˜ç¡®è¡Œæ”¿åŒºï¼ˆå¦‚ ç¾å›½ã€æ–°åŠ å¡ã€åŠ åˆ©ç¦å°¼äºšå·ã€é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºã€å¾·æ„å¿—è”é‚¦å…±å’Œå›½ï¼‰
- å›½é™…ç»„ç»‡ï¼ˆå¦‚ å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡ã€ä¸–ç•Œé“¶è¡Œã€è”åˆå›½ã€é‡‘èç¨³å®šç†äº‹ä¼šï¼‰
- é‡è¦äº§å“/å“ç‰Œ/å‹å·ï¼ˆç”±æ˜ç¡®ä¸»ä½“ç”Ÿäº§/æä¾›çš„å…·ä½“äº§å“æˆ–å“ç‰Œï¼Œå¦‚ iPhone 15 Proã€Tesla Model 3ã€ChatGPTã€Windows 11ã€Redmi 12Cï¼‰

âŒ ä»¥ä¸‹å†…å®¹**ä¸å¾—**è§†ä¸ºå®ä½“ï¼š
- åªä½œä¸ºæ–°é—»é€šè®¯æºçš„å®ä½“ä¸åº”è¯¥ä½œä¸ºå®ä½“
- æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚ "å¸‚åœºæ³¢åŠ¨"ã€"ç³»ç»Ÿæ€§é£é™©"ã€"èµ„æœ¬æµåŠ¨"ï¼‰
- æŠ€æœ¯æˆ–é‡‘èæœ¯è¯­ï¼ˆå¦‚ "æœŸæƒå®šä»·"ã€"èµ„äº§è´Ÿå€ºè¡¨"ã€"é‡åŒ–å®½æ¾"ï¼‰
- é‡‘èå·¥å…·æˆ–èµ„äº§åç§°ï¼ˆå¦‚ "æ ‡æ™®500æŒ‡æ•°"ã€"10å¹´æœŸç¾å€º"ã€"é»„é‡‘æœŸè´§"ã€"BTC"ï¼‰â€”â€”é™¤éæŒ‡ä»£å…¶å‘è¡Œæ–¹ã€ç®¡ç†æ–¹æˆ–å…³è”æ³•äººï¼ˆå¦‚ "æ ‡æ™®é“ç¼æ–¯æŒ‡æ•°å…¬å¸"ï¼‰
- æ³›ç§°ï¼ˆå¦‚ "æŠ•èµ„è€…"ã€"ç›‘ç®¡æœºæ„"ã€"æŸé“¶è¡Œ"ã€"å¤§å‹ç§‘æŠ€å…¬å¸"ã€"æ™ºèƒ½æ‰‹æœº"ï¼‰
- æƒ…ç»ª/è¡Œæƒ…æè¿°ï¼ˆå¦‚ "æš´æ¶¨"ã€"æŠ›å”®æ½®"ã€"ç»æµè¡°é€€æ‹…å¿§"ï¼‰

é¢å¤–çº¦æŸï¼š
- å®ä½“å¿…é¡»â€œåŸå­åŒ–â€ï¼šä¸è¦æŠŠå¤šä¸ªå®ä½“ç²˜è¿æˆä¸€ä¸ªå®ä½“åç§°ï¼ˆä¾‹å¦‚â€œç¾å›½æ€»ç»Ÿç‰¹æœ—æ™®â€å¿…é¡»æ‹†åˆ†ä¸ºâ€œç¾å›½æ€»ç»Ÿâ€å’Œâ€œç‰¹æœ—æ™®â€ï¼Œä¸è¦è¾“å‡ºç²˜è¿å½¢å¼ï¼‰
- åŒä¸€ä¸»ä½“å¤šç§è¡¨è¿°æ—¶ï¼šentities ç”¨æœ€è§„èŒƒä¸”ä¸æ­§ä¹‰çš„ä¸»åç§°ï¼Œentities_original ä¿ç•™å¯¹åº”åŸæ–‡è¡¨è¿°å¹¶é€ä¸€å¯¹é½"""

    prompt = create_extraction_prompt(title, content, entity_definitions, reported_at=reported_at)
    tools.log(f"[LLMè¯·æ±‚] æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")

    # ä½¿ç”¨ç»Ÿä¸€çš„LLMè°ƒç”¨å‡½æ•°
    tools.log("[LLMè¯·æ±‚] è°ƒç”¨LLM API")
    raw_content = call_llm_with_retry(
        llm_pool=api_pool,
        prompt=prompt,
        max_tokens=1500,
        timeout=55,
        retries=max_retries
    )
    tools.log(f"[LLMè¯·æ±‚] LLMè¿”å›å†…å®¹é•¿åº¦: {len(raw_content) if raw_content else 0} å­—ç¬¦")

    if not raw_content:
        tools.log("[LLMè¯·æ±‚] LLMè¿”å›ç©ºå†…å®¹")
        return []

    # ä½¿ç”¨ç»Ÿä¸€çš„JSONè§£æå‡½æ•°
    try:
        tools.log("[LLMè¯·æ±‚] è§£æLLMå“åº”")
        data = extract_json_from_llm_response(raw_content)
        events = data.get("events", [])
        tools.log(f"[LLMè¯·æ±‚] è§£æåˆ° {len(events)} ä¸ªäº‹ä»¶")
        result = []

        for item in events:
            abstract = item.get("abstract", "").strip()
            # ----------------------------
            # 1) entities / entities_originalï¼šå¯¹é½ & å®¹é”™
            # ----------------------------
            entities_raw = item.get("entities", []) or []
            entities_original_raw = item.get("entities_original", []) or []
            if isinstance(entities_raw, str):
                entities_raw = [entities_raw]
            if isinstance(entities_original_raw, str):
                entities_original_raw = [entities_original_raw]

            entities: List[str] = []
            entities_original: List[str] = []
            for i, ent in enumerate(entities_raw if isinstance(entities_raw, list) else []):
                if not isinstance(ent, str):
                    continue
                ent = ent.strip()
                if not ent or not tools.is_valid_entity(ent):
                    continue

                ent_original = ""
                if isinstance(entities_original_raw, list) and i < len(entities_original_raw):
                    ent_original = entities_original_raw[i]
                if not isinstance(ent_original, str):
                    ent_original = ""
                ent_original = ent_original.strip()

                # åŸå§‹è¡¨è¿°ç¼ºå¤±æ—¶å›é€€åˆ°å®ä½“åï¼ˆé¿å…å›  zip æˆªæ–­/ç¼ºå¤±å¯¼è‡´å®ä½“æ•´ä½“è¢«ä¸¢å¼ƒï¼‰
                if not ent_original or not tools.is_valid_entity(ent_original):
                    ent_original = ent

                entities.append(ent)
                entities_original.append(ent_original)

            # ----------------------------
            # 2) entity_rolesï¼šå®ä½“è¯­ä¹‰è§’è‰²ï¼ˆkey å¿…é¡»æ¥è‡ª entitiesï¼‰
            # ----------------------------
            roles_raw = item.get("entity_roles", {}) or {}
            entity_roles: Dict[str, List[str]] = {}
            if isinstance(roles_raw, dict):
                allowed = set(entities)
                for k, v in roles_raw.items():
                    if not isinstance(k, str):
                        continue
                    ek = k.strip()
                    if ek not in allowed:
                        continue
                    roles_list: List[str] = []
                    if isinstance(v, str):
                        roles_list = [v]
                    elif isinstance(v, list):
                        roles_list = [r for r in v if isinstance(r, str)]
                    cleaned_roles = []
                    seen = set()
                    for r in roles_list:
                        rr = r.strip()
                        if not rr:
                            continue
                        if rr not in seen:
                            seen.add(rr)
                            cleaned_roles.append(rr)
                    if cleaned_roles:
                        entity_roles[ek] = cleaned_roles

            # ----------------------------
            # 3) event_typesï¼šäº‹ä»¶ç±»å‹æ ‡ç­¾
            # ----------------------------
            types_raw = item.get("event_types", []) or []
            event_types: List[str] = []
            if isinstance(types_raw, str):
                types_raw = [types_raw]
            if isinstance(types_raw, list):
                seen_t = set()
                for t in types_raw:
                    if not isinstance(t, str):
                        continue
                    tt = t.strip()
                    if not tt:
                        continue
                    if tt not in seen_t:
                        seen_t.add(tt)
                        event_types.append(tt)

            # ----------------------------
            # 4) event_start_timeï¼šäº‹ä»¶èµ·å§‹æ—¶é—´ï¼ˆä¸ reported_at åŒºåˆ†ï¼‰
            # ----------------------------
            event_start_time = item.get("event_start_time", "")
            event_start_time_text = item.get("event_start_time_text", "")
            event_start_time_precision = item.get("event_start_time_precision", "unknown")
            if not isinstance(event_start_time, str):
                event_start_time = ""
            if not isinstance(event_start_time_text, str):
                event_start_time_text = ""
            if not isinstance(event_start_time_precision, str):
                event_start_time_precision = "unknown"
            event_start_time = event_start_time.strip()
            event_start_time_text = event_start_time_text.strip()
            event_start_time_precision = event_start_time_precision.strip() or "unknown"

            # ----------------------------
            # 5) relationsï¼š(å®ä½“, å…³ç³», å®ä½“) ä¸‰å…ƒç»„
            # ----------------------------
            relations_raw = item.get("relations", []) or []
            relations: List[Dict[str, str]] = []
            allowed_entities = set(entities)
            seen_rel = set()

            def _add_relation(s: str, p: str, o: str, ev: str = "", relation_kind: Any = ""):
                ss = s.strip() if isinstance(s, str) else ""
                pp = p.strip() if isinstance(p, str) else ""
                oo = o.strip() if isinstance(o, str) else ""
                ee = ev.strip() if isinstance(ev, str) else ""
                if not ss or not pp or not oo:
                    return
                if ss not in allowed_entities or oo not in allowed_entities:
                    return
                if ss == oo:
                    return
                rk_raw = relation_kind if isinstance(relation_kind, str) else str(relation_kind or "")
                rk = rk_raw.strip().lower()
                if rk not in {"state", "event"}:
                    rk = ""
                key = (ss, pp, oo, rk)
                if key in seen_rel:
                    return
                seen_rel.add(key)
                relations.append({
                    "subject": ss,
                    "predicate": pp,
                    "object": oo,
                    "relation_kind": rk,
                    "evidence": ee
                })

            if isinstance(relations_raw, dict):
                relations_raw = [relations_raw]
            if isinstance(relations_raw, list):
                for rel in relations_raw:
                    # æ”¯æŒ dict å½¢å¼ï¼š{"subject","predicate","object","evidence"}
                    if isinstance(rel, dict):
                        _add_relation(
                            rel.get("subject", ""),
                            rel.get("predicate", ""),
                            rel.get("object", ""),
                            rel.get("evidence", "") or rel.get("text", ""),
                            rel.get("relation_kind", "") or rel.get("kind", "") or rel.get("type", ""),
                        )
                        continue
                    # å…¼å®¹ tuple/list å½¢å¼ï¼š[s,p,o] æˆ– [s,p,o,evidence]
                    if isinstance(rel, (list, tuple)) and len(rel) >= 3:
                        s, p, o = rel[0], rel[1], rel[2]
                        ev = rel[3] if len(rel) >= 4 else ""
                        _add_relation(s, p, o, ev, "")

            summary = item.get("event_summary", "").strip()
            if abstract and entities and summary:
                result.append({
                    "abstract": abstract,
                    "entities": entities,
                    "entities_original": entities_original,
                    "entity_roles": entity_roles,
                    "event_types": event_types,
                    "event_start_time": event_start_time,
                    "event_start_time_text": event_start_time_text,
                    "event_start_time_precision": event_start_time_precision,
                    "relations": relations,
                    "event_summary": summary
                })
        tools.log(f"[LLMè¯·æ±‚] æå–å®Œæˆï¼Œå…± {len(result)} ä¸ªæœ‰æ•ˆäº‹ä»¶")
        return result
    except Exception as e:
        tools.log(f"[LLMè·å–] âŒ LLM è¿”å›å†…å®¹è§£æå¤±è´¥: {e}")
        return []


@register_tool(
    name="extract_entities_events",
    description="ä½¿ç”¨ LLM ä»æ–°é—»æ ‡é¢˜å’Œå†…å®¹ä¸­æå–å®ä½“å’Œäº‹ä»¶",
    category="Information Extraction"
)
def extract_entities_events(title: str, content: str) -> List[Dict[str, Any]]:
    """
    ä»æ–°é—»ä¸­æå–å®ä½“å’Œäº‹ä»¶

    Args:
        title: æ–°é—»æ ‡é¢˜
        content: æ–°é—»å†…å®¹

    Returns:
        äº‹ä»¶åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« entities, event_summary ç­‰
    """
    api_pool = get_llm_pool()
    return llm_extract_events(title, content, api_pool)

@register_tool(
    name="deduplicate_news_batch",
    description="å¯¹æ–°é—»åˆ—è¡¨è¿›è¡Œæ‰¹é‡å»é‡ (åŸºäº SimHash)",
    category="Data Processing"
)
def deduplicate_news_batch(news_list: List[Dict[str, Any]], threshold: int = 3) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡å»é‡
    
    Args:
        news_list: æ–°é—»å­—å…¸åˆ—è¡¨
        threshold: SimHash æ±‰æ˜è·ç¦»é˜ˆå€¼
        
    Returns:
        å»é‡åçš„æ–°é—»åˆ—è¡¨
    """
    if not news_list:
        return []
        
    deduper = NewsDeduplicator(threshold=threshold)
    unique_news = []
    
    for news in news_list:
        # æ„é€ æŒ‡çº¹æ–‡æœ¬
        text = (news.get("title", "") + " " + news.get("content", "")).strip()
        if not text: 
            continue
            
        # æ£€æŸ¥é‡å¤
        if not deduper.is_duplicate(text):
            unique_news.append(news)
            
    return unique_news

def get_unprocessed_news_files() -> List[Path]:
    """
    ä»…ä½¿ç”¨ tmp ç›®å½•çš„å»é‡ä¸å¤„ç†ã€‚
    tmp ç”¨äºæ–°æŠ“å–çš„å¾…å¤„ç†æ•°æ®ï¼Œå¤„ç†å®Œæˆåä¼šåˆ é™¤å¯¹åº” raw/dedupedã€‚
    """
    # ä»æ•°æ®åº“è·å–å·²å¤„ç†çš„ID
    from src.adapters.sqlite.store import get_store
    processed_ids = get_store().get_processed_ids()

    unprocessed: List[Path] = []
    raw_dir = tools.RAW_NEWS_TMP_DIR
    dedup_dir = tools.DEDUPED_NEWS_TMP_DIR
    raw_dir.mkdir(parents=True, exist_ok=True)
    dedup_dir.mkdir(parents=True, exist_ok=True)

    for raw_file in sorted(raw_dir.glob("*.jsonl")):
        deduped_file = dedup_dir / f"{raw_file.stem}_deduped.jsonl"
        if not deduped_file.exists():
            deduper = NewsDeduplicator(threshold=tools.get_dedupe_threshold())
            deduper.dedupe_file(raw_file, deduped_file, processed_ids)
        unprocessed.append(deduped_file)
    return unprocessed


@register_tool(
    name="process_news_pipeline",
    description="[å·¥ä½œæµ] å¤„ç†æ–°é—»ç®¡é“ï¼šä»tmpæ–‡ä»¶è¯»å–ã€æå–å®ä½“äº‹ä»¶ã€æ›´æ–°å›¾è°±",
    category="Workflow"
)
async def process_news_pipeline(max_workers: int = 3, rate_limit_per_sec: float = 1.0) -> Dict[str, Any]:
    """
    ä¸»å¤„ç†æµç¨‹ï¼šå¹¶å‘å®ä½“æå–

    Args:
        max_workers: æœ€å¤§å¹¶å‘æ•°
        rate_limit_per_sec: æ¯ç§’é€Ÿç‡é™åˆ¶

    Returns:
        å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    tools.log(f"ğŸš€ å¯åŠ¨æ–°é—»å¤„ç†ç®¡é“ | workers={max_workers}, rate={rate_limit_per_sec}/s")
    files = get_unprocessed_news_files()
    if not files:
        tools.log("ğŸ“­ æ— å¯å¤„ç†æ–°é—»æ–‡ä»¶")
        return {"processed_count": 0, "files_processed": 0}

    # ä»æ•°æ®åº“è·å–å·²å¤„ç†çš„ID
    from src.adapters.sqlite.store import get_store
    processed_ids = get_store().get_processed_ids()

    limiter = RateLimiter(rate_limit_per_sec)
    async_executor = AsyncExecutor()
    logger = tools.get_logger(__name__)
    api_pool = get_llm_pool()
    total_processed = 0

    def build_published_at(ts: Optional[str]) -> Optional[str]:
        if not ts:
            return None
        try:
            return ts if isinstance(ts, str) else str(ts)
        except Exception:
            return None

    async def extract_task_async(global_id: str, title: str, content: str, source: str, published_at: Optional[str]) -> Tuple[str, str, Optional[str], List[Dict]]:
        try:
            await limiter.acquire_async()
            loop = asyncio.get_running_loop()
            extracted = await loop.run_in_executor(
                None,
                lambda: llm_extract_events(title, content, api_pool, reported_at=published_at)
            )
            return global_id, source, published_at, extracted
        except Exception as e:
            logger.error(f"ä»»åŠ¡ {global_id} æå–å¤±è´¥: {e}")
            return global_id, source, published_at, []

    # æ”¶é›†æ‰€æœ‰éœ€è¦è®°å½•çš„å·²å¤„ç†IDï¼Œç¨åæ‰¹é‡æ’å…¥æ•°æ®åº“
    processed_ids_to_add = []
    for file_path in files:
        logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")

        # æ”¶é›†éœ€è¦å¤„ç†çš„æ–°é—»ä»»åŠ¡
        news_tasks = []
        with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news = json.loads(line)
                        raw_id = str(news.get("id", "")).strip()
                        source = news.get("source", "unknown").strip().lower()

                        if not raw_id or not source:
                            logger.warning("âš ï¸ è·³è¿‡æ—  ID æˆ–æ—  source çš„æ–°é—»")
                            continue

                        global_id = f"{source}:{raw_id}"
                        if global_id in processed_ids:
                            continue

                        title = news.get("title", "")
                        content = news.get("content", "")
                        MAX_CONTENT_CHARS = 2000
                        if isinstance(content, str) and len(content) > MAX_CONTENT_CHARS:
                            content = content[:MAX_CONTENT_CHARS] + "â€¦â€¦ã€åæ–‡å·²æˆªæ–­ã€‘"

                        published_at = build_published_at(news.get("timestamp"))

                        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
                        news_tasks.append(
                            lambda gid=global_id, t=title, c=content, s=source, p=published_at: extract_task_async(gid, t, c, s, p)
                        )
                    except Exception as e:
                        logger.error(f"âš ï¸ è§£ææ–°é—»è¡Œå¤±è´¥: {e}")

        if news_tasks:
            logger.info(f"ğŸ”„ å¼€å§‹å¹¶å‘å¤„ç† {len(news_tasks)} ä¸ªæ–°é—»æå–ä»»åŠ¡")
            # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†å¹¶å‘æ‰§è¡Œ
            results = await async_executor.run_concurrent_tasks(
                tasks=news_tasks,
                concurrency=max_workers
            )

            for result in results:
                    try:
                        global_id, source, published_at, extracted = result
                        if not extracted:
                            logger.debug(f"â³ æ–°é—» {global_id}ï¼šLLM æœªè¿”å›æœ‰æ•ˆäº‹ä»¶ï¼Œä¿ç•™é‡è¯•æœºä¼š")
                            continue

                        all_entities = []
                        all_entities_original = []
                        for ev in extracted:
                            all_entities.extend(ev["entities"])
                            all_entities_original.extend(ev["entities_original"])

                        if all_entities and len(all_entities) == len(all_entities_original):
                            update_entities(all_entities, all_entities_original, source, published_at)
                            update_abstract_map(extracted, source, published_at)
                            total_processed += 1
                            # æ”¶é›†éœ€è¦è®°å½•çš„ID
                            processed_ids_to_add.append((global_id, source, raw_id))
                            processed_ids.add(global_id)
                        else:
                            logger.debug(f"ğŸ” æ–°é—» {global_id}ï¼šLLM è¿”å›äº‹ä»¶ä½†æ— æœ‰æ•ˆå®ä½“ï¼Œæš‚ä¸æ ‡è®°")
                    except Exception as e:
                        logger.error(f"âš ï¸ å¤„ç†æå–ç»“æœå¤±è´¥: {e}")

            try:
                # å¤„ç† tmp ç›®å½•ä¸‹çš„ raw/deduped æ–‡ä»¶
                raw_dir = tools.RAW_NEWS_TMP_DIR
                raw_file_name = file_path.stem.replace("_deduped", "") + ".jsonl"
                raw_file_path = raw_dir / raw_file_name

                # ä½¿ç”¨ç»Ÿä¸€çš„åˆ é™¤å‡½æ•°
                safe_unlink(raw_file_path, "åŸå§‹æ–°é—»")
                safe_unlink(file_path, "å»é‡æ–°é—»")
            except Exception as e:
                tools.log(f"âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")

    # æ‰¹é‡æ’å…¥å·²å¤„ç†çš„IDåˆ°æ•°æ®åº“
    if processed_ids_to_add:
        try:
            from src.adapters.sqlite.store import get_store
            count = get_store().add_processed_ids(processed_ids_to_add)
            tools.log(f"âœ… æ‰¹é‡è®°å½• {count} ä¸ªå·²å¤„ç†IDåˆ°æ•°æ®åº“")
        except Exception as e:
            tools.log(f"âš ï¸ è®°å½•å·²å¤„ç†IDåˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    tools.log(f"âœ… å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡å«æœ‰æ•ˆå®ä½“çš„æ–°é—»")
    # SQLite ä¸ºä¸»å­˜å‚¨ï¼šæ‰¹é‡å¤„ç†ç»“æŸåç»Ÿä¸€å¯¼å‡ºå…¼å®¹ JSONï¼ˆé¿å…æ¯æ¡æ–°é—»éƒ½å†™ä¸€æ¬¡å¤§æ–‡ä»¶ï¼‰
    if total_processed > 0:
        try:
            from src.adapters.sqlite.store import get_store
            get_store().export_compat_json_files()
        except Exception as e:
            tools.log(f"âš ï¸ å¯¼å‡ºå…¼å®¹JSONå¤±è´¥ï¼ˆä¸å½±å“ä¸»å­˜å‚¨SQLiteï¼‰: {e}")
    return {"processed_count": total_processed, "files_processed": len(files)}


@register_tool(
    name="batch_process_news",
    description="[å·¥ä½œæµ] æ‰¹é‡å¤„ç†æ–°é—»ï¼šå»é‡å¹¶æå–äº‹ä»¶",
    category="Workflow"
)
async def batch_process_news(news_list: List[Dict[str, Any]], limit: int = -1) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡å¤„ç†æ–°é—»ï¼š
    1. å»é‡
    2. æå–å®ä½“å’Œäº‹ä»¶
    3. é™„åŠ å…ƒæ•°æ® (source, published_at)

    Args:
        news_list: æ–°é—»åˆ—è¡¨
        limit: é™åˆ¶å¤„ç†çš„æ–°é—»æ•°é‡ï¼Œ-1 è¡¨ç¤ºä¸é™åˆ¶ã€‚ç”¨äºæµ‹è¯•/èŠ‚çœ Tokenã€‚

    Returns:
        æ‰å¹³åŒ–çš„äº‹ä»¶åˆ—è¡¨
    """
    from ...infra.paths import tools as Tools
    tools = Tools()
    tools.log(f"[batch_process_news] å¼€å§‹å¤„ç†ï¼Œæ–°é—»æ•°é‡: {len(news_list)}, limit: {limit}")
    # 1. å»é‡
    unique_news = deduplicate_news_batch(news_list)
    tools.log(f"[batch_process_news] å»é‡åæ–°é—»æ•°é‡: {len(unique_news)}")
    if limit > 0:
        unique_news = unique_news[:limit]
        tools.log(f"[batch_process_news] åº”ç”¨limitåæ–°é—»æ•°é‡: {len(unique_news)}")

    # è¯»å–å¹¶å‘/é™é€Ÿé…ç½®ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ï¼‰
    config_manager = get_config_manager()
    max_workers = config_manager.get_concurrency_limit("agent1_config")
    rate_limit = config_manager.get_rate_limit("agent1_config")

    # ä½¿ç”¨ç»Ÿä¸€é™é€Ÿå™¨
    limiter = RateLimiter(rate_limit)

    # å¤ç”¨ä¸€ä¸ª API poolï¼Œé¿å…æ¯æ¡æ–°é—»éƒ½åˆå§‹åŒ– LLMAPIPool è§¦å‘â€œè¿ç§»/åŠ è½½æœåŠ¡â€åˆ·å±
    api_pool = get_llm_pool()

    def process_one(news: Dict[str, Any]) -> (List[Dict[str, Any]], Optional[str]):
        events_out = []
        processed_id = None
        try:
            title = news.get("title", "")
            content = news.get("content", "")
            source = news.get("source", "unknown")
            timestamp = news.get("datetime") or news.get("formatted_time")
            news_id = str(news.get("id", "")).strip()
            if news_id and source:
                processed_id = f"{source}:{news_id}"
            limiter.acquire()
            extracted = llm_extract_events(title, content, api_pool, reported_at=timestamp)
            for ev in extracted:
                ev["source"] = source
                ev["published_at"] = timestamp
                ev["news_id"] = news.get("id")
                events_out.append(ev)
        except Exception as e:
            print(f"Extraction failed for news {news.get('id', '')}: {e}")
        return events_out, processed_id

    all_events: List[Dict[str, Any]] = []
    processed_ids: List[str] = []
    if not unique_news:
        tools.log("[batch_process_news] æ²¡æœ‰æ–°é—»éœ€è¦å¤„ç†")
        return all_events
    else:
        tools.log(f"[batch_process_news] å‡†å¤‡å¤„ç† {len(unique_news)} æ¡å”¯ä¸€æ–°é—»")

    if max_workers <= 1:
        tools.log(f"[batch_process_news] å¼€å§‹ä¸²è¡Œå¤„ç† {len(unique_news)} æ¡æ–°é—»")
        for i, n in enumerate(unique_news):
            tools.log(f"[batch_process_news] å¤„ç†ç¬¬ {i+1} æ¡æ–°é—»: {n.get('title', '')[:50]}...")
            evs, pid = process_one(n)
            tools.log(f"[batch_process_news] ç¬¬ {i+1} æ¡æ–°é—»æå–åˆ° {len(evs)} ä¸ªäº‹ä»¶")
            all_events.extend(evs)
            if pid:
                processed_ids.append(pid)
    else:
        # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†çº¿ç¨‹å¹¶å‘
        tools.log(f"[batch_process_news] å¼€å§‹å¹¶å‘å¤„ç† {len(unique_news)} æ¡æ–°é—»ï¼Œæœ€å¤§å¹¶å‘æ•°: {max_workers}")
        async_executor = AsyncExecutor()
        task_results = async_executor.run_threaded_tasks(
            tasks=unique_news,
            func=process_one,
            max_workers=max_workers
        )
        tools.log(f"[batch_process_news] å¹¶å‘å¤„ç†å®Œæˆï¼Œè·å–åˆ° {len(task_results)} ä¸ªç»“æœ")

        for evs, pid in task_results:
            all_events.extend(evs or [])
            if pid:
                processed_ids.append(pid)

    # å†™å…¥SQLiteæ•°æ®åº“ï¼ˆå®ä½“å’Œäº‹ä»¶ï¼‰
    if all_events:
        tools.log(f"[batch_process_news] å¼€å§‹å†™å…¥ {len(all_events)} ä¸ªäº‹ä»¶åˆ°SQLite")
        try:
            # æŒ‰æ¥æºåˆ†ç»„äº‹ä»¶ï¼Œæ‰¹é‡å†™å…¥
            events_by_source = {}
            for ev in all_events:
                source = ev.get("source", "unknown")
                if source not in events_by_source:
                    events_by_source[source] = []
                events_by_source[source].append(ev)
            
            # é€ä¸ªæ¥æºå†™å…¥
            for source, events in events_by_source.items():
                # æ”¶é›†æ‰€æœ‰å®ä½“
                all_entities = []
                all_entities_original = []
                for ev in events:
                    all_entities.extend(ev.get("entities", []))
                    all_entities_original.extend(ev.get("entities_original", []))
                
                # å†™å…¥å®ä½“
                if all_entities and len(all_entities) == len(all_entities_original):
                    published_at = events[0].get("published_at") if events else None
                    update_entities(all_entities, all_entities_original, source, published_at)
                    tools.log(f"[batch_process_news] å·²å†™å…¥ {len(all_entities)} ä¸ªå®ä½“ (æ¥æº: {source})")
                
                # å†™å…¥äº‹ä»¶
                update_abstract_map(events, source, events[0].get("published_at") if events else None)
                tools.log(f"[batch_process_news] å·²å†™å…¥ {len(events)} ä¸ªäº‹ä»¶ (æ¥æº: {source})")
        except Exception as e:
            tools.log(f"[batch_process_news] å†™å…¥SQLiteå¤±è´¥: {e}")

    # è®°å½• processed_ids åˆ°æ•°æ®åº“ï¼Œé¿å…é‡å¤å¤„ç†
    if processed_ids:
        tools.log(f"[batch_process_news] è®°å½• {len(processed_ids)} ä¸ªå·²å¤„ç†çš„ID")
        try:
            # ä» processed_ids ä¸­æå– source å’Œ news_id
            ids_to_add = []
            for pid in processed_ids:
                parts = pid.split(':', 1)
                if len(parts) == 2:
                    source, news_id = parts
                    ids_to_add.append((pid, source, news_id))
            
            from src.adapters.sqlite.store import get_store
            count = get_store().add_processed_ids(ids_to_add)
            tools.log(f"[batch_process_news] æˆåŠŸè®°å½• {count} ä¸ªå·²å¤„ç†IDåˆ°æ•°æ®åº“")
        except Exception as e:
            tools.log(f"[batch_process_news] è®°å½•å·²å¤„ç†IDåˆ°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
    else:
        tools.log("[batch_process_news] æ²¡æœ‰éœ€è¦è®°å½•çš„å·²å¤„ç†ID")

    # å¯¼å‡ºSQLiteæ•°æ®åˆ°JSONæ–‡ä»¶ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
    if all_events:
        try:
            from src.adapters.sqlite.store import get_store
            get_store().export_compat_json_files()
            tools.log(f"[batch_process_news] âœ… å·²å¯¼å‡ºSQLiteæ•°æ®åˆ°JSONæ–‡ä»¶")
        except Exception as e:
            tools.log(f"[batch_process_news] âš ï¸ å¯¼å‡ºJSONæ–‡ä»¶å¤±è´¥ï¼ˆä¸å½±å“SQLiteä¸»å­˜å‚¨ï¼‰: {e}")

    tools.log(f"[batch_process_news] å¤„ç†å®Œæˆï¼Œæ€»å…±æå–åˆ° {len(all_events)} ä¸ªäº‹ä»¶")
    return all_events


@register_tool(
    name="persist_expanded_news_tmp",
    description="å°†æ‹“å±•æ–°é—»å†™å…¥ tmp/raw_news & tmp/deduped_newsï¼Œå¹¶è¿”å›è·¯å¾„",
    category="Workflow"
)
def persist_expanded_news_tmp(expanded_news: List[Dict[str, Any]]) -> Dict[str, str]:
    """
    ä¸ºå‰ç«¯è°ƒç”¨çš„åŒ…è£…ï¼šè½åœ°æ‹“å±•æ–°é—»åˆ° tmpï¼Œå¹¶è¿”å›æ–‡ä»¶è·¯å¾„ã€‚
    """
    # ä»æ•°æ®åº“è·å–å·²å¤„ç†çš„ID
    from src.adapters.sqlite.store import get_store
    processed_ids = get_store().get_processed_ids()

    deduped_path = persist_expanded_news_to_tmp(expanded_news, processed_ids)
    return {
        "deduped_path": str(deduped_path) if deduped_path else "",
        "raw_path": str(tools.RAW_NEWS_TMP_DIR) if deduped_path else "",
    }


@register_tool(
    name="save_extracted_events_tmp",
    description="å°†æå–çš„äº‹ä»¶åˆ—è¡¨å†™å…¥ data/tmp/extracted_events_*.jsonlï¼Œå¹¶è¿”å›è·¯å¾„",
    category="Data Processing"
)
def save_extracted_events_tmp(events: List[Dict[str, Any]]) -> Dict[str, str]:
    if not events:
        return {"path": ""}

    out_path = create_temp_file_path(tools.DATA_TMP_DIR, "extracted_events")
    write_jsonl_file(out_path, events, ensure_ascii=False)
    return {"path": str(out_path)}


def persist_expanded_news_to_tmp(expanded_news: List[Dict], processed_ids: Set[str]) -> Optional[Path]:
    """
    å°†æ‹“å±•æ–°é—»å†™å…¥ tmp åŸå§‹æ–‡ä»¶å¹¶åšå»é‡ï¼Œè¿”å›å»é‡åçš„æ–‡ä»¶è·¯å¾„ã€‚
    """
    if not expanded_news:
        return None

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    ensure_dirs(tools.RAW_NEWS_TMP_DIR, tools.DEDUPED_NEWS_TMP_DIR)

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
    ts = generate_timestamp()
    raw_path = tools.RAW_NEWS_TMP_DIR / f"expanded_{ts}.jsonl"
    deduped_path = tools.DEDUPED_NEWS_TMP_DIR / f"expanded_{ts}_deduped.jsonl"

    # å†™å…¥åŸå§‹æ•°æ®ï¼ˆå¤„ç†datetimeå­—æ®µï¼‰
    sanitized_news = sanitize_datetime_fields(expanded_news)
    write_jsonl_file(raw_path, sanitized_news, ensure_ascii=False)

    # å»é‡å¤„ç†
    deduper = NewsDeduplicator(threshold=tools.get_dedupe_threshold())
    deduper.dedupe_file(raw_path, deduped_path, processed_ids)
    return deduped_path
