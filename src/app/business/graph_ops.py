from typing import List, Dict, Any, Optional, Set, Tuple
from ...infra.registry import register_tool
from ...core import DataNormalizer, StandardEventPipeline, ConfigManager, LLMAPIPool, RateLimiter, AsyncExecutor, tools, get_config_manager, get_llm_pool
from ...domain.data_operations import update_entities, update_abstract_map
from ...infra.serialization import extract_json_from_llm_response
from ...infra.async_utils import call_llm_with_retry, create_deduplication_prompt, create_event_deduplication_prompt
from ...infra.file_utils import ensure_dir
from ...domain.data_operations import write_json_file, read_json_file
from pathlib import Path
import json
from datetime import datetime, timedelta, timezone
import time
import threading
from difflib import SequenceMatcher
from collections import defaultdict

@register_tool(
    name="update_graph_data",
    description="å°†æå–çš„äº‹ä»¶æ•°æ®å†™å…¥çŸ¥è¯†å›¾è°±æ–‡ä»¶ (Entities & Events)",
    category="Knowledge Graph"
)
def update_graph_data(events_list: List[Dict[str, Any]], default_source: str = "auto_pipeline") -> Dict[str, Any]:
    """
    æ›´æ–°çŸ¥è¯†å›¾è°±æ•°æ®æ–‡ä»¶ã€‚
    
    Args:
        events_list: äº‹ä»¶åˆ—è¡¨ï¼Œæ¯é¡¹åº”åŒ…å« entities, abstract ç­‰ï¼Œä»¥åŠå¯é€‰çš„ source, published_at
        default_source: é»˜è®¤æ¥æºæ ‡è¯†
        
    Returns:
        æ›´æ–°çŠ¶æ€
    """
    count = 0
    for event in events_list:
        entities = event.get("entities", [])
        entities_original = event.get("entities_original", [])
        # å¦‚æœæ²¡æœ‰åŸå§‹å½¢å¼ï¼Œå›é€€åˆ°å®ä½“å
        if not entities_original:
            entities_original = entities
            
        source = event.get("source", default_source)
        published_at = event.get("published_at")
        
        # æ›´æ–°å®ä½“åº“
        update_entities(entities, entities_original, source, published_at)
        count += 1
    
    # ç®€å•çš„æŒ‰ä¸ªè°ƒç”¨ (æ•ˆç‡ç¨ä½ä½†å®‰å…¨)
    for event in events_list:
        src = event.get("source", default_source)
        ts = event.get("published_at")
        update_abstract_map([event], src, ts)

    # SQLite ä¸ºä¸»å­˜å‚¨ï¼šæ‰¹é‡å†™å…¥åç»Ÿä¸€å¯¼å‡ºå…¼å®¹ JSON
    try:
        from ...adapters.sqlite.store import get_store
        get_store().export_compat_json_files()
    except Exception as e:
        tools.log(f"âš ï¸ å¯¼å‡ºå…¼å®¹JSONå¤±è´¥ï¼ˆä¸å½±å“ä¸»å­˜å‚¨SQLiteï¼‰: {e}")
    
    return {"status": "success", "updated_count": count}

@register_tool(
    name="refresh_knowledge_graph",
    description="é‡å»ºå¹¶å‹ç¼©çŸ¥è¯†å›¾è°± (è§¦å‘ Agent3 é€»è¾‘)",
    category="Knowledge Graph"
)
def refresh_knowledge_graph() -> Dict[str, str]:
    """
    åˆ·æ–°çŸ¥è¯†å›¾è°±ï¼šæ„å»ºã€å‹ç¼©ã€æ›´æ–°
    """
    try:
        # ä¼˜å…ˆåº”ç”¨å·²ç¡®è®¤çš„å®ä½“åˆå¹¶å†³ç­–ï¼ˆé¿å…æ¯æ¬¡ refresh éƒ½è®© LLM é‡æ–°â€œæ¼‚ç§»â€ï¼‰
        try:
            from .review_ops import apply_entity_merge_decisions
            apply_entity_merge_decisions(max_actions=200)
        except Exception:
            pass
        # ä¼˜å…ˆåº”ç”¨å·²ç¡®è®¤çš„äº‹ä»¶ merge/evolve å†³ç­–
        try:
            from .review_ops import apply_event_merge_or_evolve_decisions
            apply_event_merge_or_evolve_decisions(max_actions=200)
        except Exception:
            pass
        kg = KnowledgeGraph()
        kg.refresh_graph()
        return {"status": "refreshed"}
    except Exception as e:
        return {"status": "error", "message": str(e)}


@register_tool(
    name="generate_kg_visual_snapshots",
    description="ç”Ÿæˆè½»é‡å›¾è°±å¿«ç…§ï¼škg_visualï¼ˆå®ä½“-äº‹ä»¶è£å‰ªï¼‰ä¸ kg_visual_timelineï¼ˆäº‹ä»¶æ—¶é—´çº¿ï¼‰",
    category="Knowledge Graph"
)
def generate_kg_visual_snapshots(
    kg_path: str = "data/knowledge_graph.json",
    out_path_graph: str = "data/kg_visual.json",
    out_path_timeline: str = "data/kg_visual_timeline.json",
    top_entities: int = 500,
    top_events: int = 500,
    max_title_len: int = 80,
    days_window: Optional[int] = None
) -> Dict[str, str]:
    """
    ç”Ÿæˆè½»é‡çº§å¯è§†åŒ–å¿«ç…§ï¼Œä¾¿äºå‰ç«¯ç›´æ¥åŠ è½½ï¼š
    - kg_visual: nodes (id/label/type/color), edges (from/to/title/time)
    - kg_visual_timeline: events æŒ‰æ—¶é—´æ’åºï¼ŒåŒ…å«æ‘˜è¦ã€æ—¶é—´ã€å®ä½“åˆ—è¡¨ï¼ˆæˆªæ–­ï¼‰
    """
    # SQLite ä¸ºçœŸç›¸æºï¼šä¼˜å…ˆç›´æ¥ä» SQLite æ„å»ºå¿«ç…§ï¼ˆä¸ä¾èµ– knowledge_graph.jsonï¼‰
    try:
        from ...adapters.sqlite.store import get_store
        import sqlite3

        store = get_store()
        db_path = str(getattr(tools, "SQLITE_DB_FILE", None) or "")
        if not db_path:
            raise RuntimeError("SQLITE_DB_FILE not configured")

        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        try:
            # entity_id -> name
            ent_name = {
                str(r["entity_id"]): str(r["name"])
                for r in conn.execute("SELECT entity_id, name FROM entities").fetchall()
            }
            # event_id -> (abstract, summary, time)
            ev_rows = conn.execute(
                "SELECT event_id, abstract, event_summary, event_start_time, reported_at, first_seen FROM events"
            ).fetchall()
            evt_by_id = {}
            for r in ev_rows:
                abstract = str(r["abstract"] or "")
                if not abstract:
                    continue
                evt_by_id[str(r["event_id"])] = {
                    "abstract": abstract,
                    "event_summary": str(r["event_summary"] or "") or abstract,
                    "time": str(r["event_start_time"] or "").strip()
                    or str(r["reported_at"] or "").strip()
                    or str(r["first_seen"] or "").strip(),
                }

            # edgesï¼ˆentity-event / relation / event_edgeï¼‰ï¼Œå…¨éƒ¨å¸¦ timeï¼ˆç¼ºå¤±å…œåº•ï¼‰
            edges = []

            # participants -> entity-event
            for r in conn.execute(
                "SELECT event_id, entity_id, time FROM participants"
            ).fetchall():
                eid = str(r["event_id"])
                ent = ent_name.get(str(r["entity_id"]), "")
                evt = evt_by_id.get(eid)
                if not ent or not evt:
                    continue
                t = str(r["time"] or "").strip() or str(evt.get("time") or "").strip() or datetime.now(timezone.utc).isoformat()
                edges.append({"from": ent, "to": f"EVT:{evt['abstract']}", "type": "involved_in", "title": evt["event_summary"], "time": t})

            # relations -> entity-entity
            for r in conn.execute(
                "SELECT subject_entity_id, predicate, object_entity_id, time FROM relations"
            ).fetchall():
                s = ent_name.get(str(r["subject_entity_id"]), "")
                o = ent_name.get(str(r["object_entity_id"]), "")
                p = str(r["predicate"] or "").strip()
                if not s or not o or not p:
                    continue
                t = str(r["time"] or "").strip() or datetime.now(timezone.utc).isoformat()
                edges.append({"from": s, "to": o, "type": "relation", "title": p, "time": t, "predicate": p})

            # event_edges -> event-event
            for r in conn.execute(
                "SELECT from_event_id, to_event_id, edge_type, time FROM event_edges"
            ).fetchall():
                a = evt_by_id.get(str(r["from_event_id"]))
                b = evt_by_id.get(str(r["to_event_id"]))
                if not a or not b:
                    continue
                edge_type = str(r["edge_type"] or "related")
                t = str(r["time"] or "").strip() or str(a.get("time") or "").strip() or datetime.now(timezone.utc).isoformat()
                edges.append({"from": f"EVT:{a['abstract']}", "to": f"EVT:{b['abstract']}", "type": "event_edge", "edge_type": edge_type, "title": edge_type, "time": t})

            # nodesï¼šç”±è¾¹æ¨å¯¼
            deg = {}
            for e in edges:
                u, v = e.get("from"), e.get("to")
                if not u or not v:
                    continue
                deg[u] = deg.get(u, 0) + 1
                deg[v] = deg.get(v, 0) + 1

            # æ—¶é—´çª—è¿‡æ»¤ï¼ˆæŒ‰ edge.timeï¼‰
            cutoff = None
            if days_window and days_window > 0:
                cutoff = datetime.now(timezone.utc) - timedelta(days=days_window)

            def _parse_dt(val: str):
                if not val:
                    return None
                try:
                    dt = datetime.fromisoformat(str(val).replace("Z", "+00:00"))
                    if dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    return dt
                except Exception:
                    return None

            filtered_edges = []
            for e in edges:
                if cutoff:
                    dt = _parse_dt(str(e.get("time") or ""))
                    if not dt or dt < cutoff:
                        continue
                filtered_edges.append(e)

            # åˆ†åˆ«é€‰ Top å®ä½“/äº‹ä»¶ï¼Œç¡®ä¿ä¸¤ç±»éƒ½èƒ½è¿›å…¥å¯è§†åŒ–
            event_nodes = [n for n in deg.keys() if isinstance(n, str) and n.startswith("EVT:")]
            entity_nodes = [n for n in deg.keys() if not (isinstance(n, str) and n.startswith("EVT:"))]
            top_evt = set(sorted(event_nodes, key=deg.get, reverse=True)[: int(top_events)])
            top_ent = set(sorted(entity_nodes, key=deg.get, reverse=True)[: int(top_entities)])
            top_ids = top_evt | top_ent

            # æ„å»ºèŠ‚ç‚¹
            vis_nodes = []
            for nid in top_ids:
                if isinstance(nid, str) and nid.startswith("EVT:"):
                    abs_key = nid[4:]
                    # åå‘æŸ¥ summary
                    summary = abs_key
                    for ev in evt_by_id.values():
                        if ev.get("abstract") == abs_key:
                            summary = ev.get("event_summary") or abs_key
                            break
                    vis_nodes.append({"id": nid, "label": str(summary)[:max_title_len], "type": "event", "color": "#ff7f0e"})
                else:
                    vis_nodes.append({"id": nid, "label": str(nid), "type": "entity", "color": "#1f77b4"})

            # æ„å»ºè¾¹
            vis_edges = []
            for e in filtered_edges:
                u, v = e.get("from"), e.get("to")
                if u in top_ids and v in top_ids:
                    title = str(e.get("title") or "")[:max_title_len]
                    edge_out = {"from": u, "to": v, "title": title}
                    if e.get("time"):
                        edge_out["time"] = e.get("time")
                    if e.get("type") == "event_edge" and e.get("edge_type"):
                        edge_out["edge_type"] = e.get("edge_type")
                    vis_edges.append(edge_out)

            out_graph = Path(out_path_graph)
            ensure_dir(out_graph.parent)
            graph_data = {"nodes": vis_nodes, "edges": vis_edges}
            write_json_file(out_graph, graph_data, ensure_ascii=False, indent=None)

            # æ—¶é—´çº¿å¿«ç…§ï¼ˆåŒ…å« abstractï¼Œä¿®å¤ KG é¡µä¾èµ–ï¼‰
            parts = conn.execute(
                "SELECT event_id, entity_id, time FROM participants"
            ).fetchall()
            ents_by_evt = {}
            for p in parts:
                ents_by_evt.setdefault(str(p["event_id"]), set()).add(ent_name.get(str(p["entity_id"]), ""))

            tl_rows = []
            for eid, ev in evt_by_id.items():
                ts = str(ev.get("time") or "").strip()
                dt = _parse_dt(ts)
                if not dt:
                    continue
                if cutoff and dt < cutoff:
                    continue
                tl_rows.append(
                    {
                        "time": ts,
                        "abstract": ev.get("abstract", ""),
                        "event_summary": str(ev.get("event_summary") or ev.get("abstract") or "")[:max_title_len],
                        "entities": [x for x in list(ents_by_evt.get(eid, set())) if x][:20],
                    }
                )

            tl_rows = sorted(tl_rows, key=lambda x: x["time"])[: int(top_events)]
            out_tl = Path(out_path_timeline)
            ensure_dir(out_tl.parent)
            write_json_file(out_tl, tl_rows, ensure_ascii=False, indent=None)

            return {"status": "success", "graph": str(out_graph), "timeline": str(out_tl)}
        finally:
            conn.close()
    except Exception:
        # è‹¥ SQLite ä¸å¯ç”¨ï¼Œå›é€€æ—§é€»è¾‘ï¼šä» knowledge_graph.json è¯»å–
        kg_file = Path(kg_path)
        if not kg_file.exists():
            return {"status": "error", "message": f"{kg_file} not found"}

        try:
            kg = json.loads(kg_file.read_text(encoding="utf-8"))
        except Exception as e:
            return {"status": "error", "message": f"load kg failed: {e}"}

        entities = kg.get("entities") or {}
        events = kg.get("events") or {}
        edges = kg.get("edges") or []

    # æ—¶é—´çª—å£è¿‡æ»¤
    cutoff = None
    if days_window and days_window > 0:
        cutoff = datetime.now(timezone.utc) - timedelta(days=days_window)

    # è®¡ç®—åº¦
    deg = {}
    filtered_edges = []
    for e in edges:
        u, v = e.get("from"), e.get("to")
        if not u or not v:
            continue
        # äº‹ä»¶æ—¶é—´è¿‡æ»¤ï¼ˆåŸºäºäº‹ä»¶èŠ‚ç‚¹ï¼‰
        if cutoff and isinstance(v, str) and v.startswith("EVT:"):
            evt_key = v[4:]
            evt = events.get(evt_key, {})
            ts = evt.get("event_start_time") or evt.get("first_seen") or evt.get("published_at")
            try:
                dt = datetime.fromisoformat(ts.replace("Z", "+00:00")) if ts else None
                if dt and dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
            except Exception:
                dt = None
            if dt and cutoff and dt < cutoff:
                continue

        deg[u] = deg.get(u, 0) + 1
        deg[v] = deg.get(v, 0) + 1
        filtered_edges.append(e)

    # å– Top èŠ‚ç‚¹
    top_ids = set()
    if deg:
        top_all = sorted(deg, key=deg.get, reverse=True)
        top_ids = set(top_all[: top_entities + top_events])

    # èŠ‚ç‚¹è¡¨
    vis_nodes = []
    node_types = {}
    for nid in top_ids:
        if isinstance(nid, str) and nid.startswith("EVT:"):
            evt = events.get(nid[4:], {})
            label = (evt.get("event_summary") or evt.get("abstract") or nid[4:])[:max_title_len]
            vis_nodes.append({"id": nid, "label": label, "type": "event", "color": "#ff7f0e"})
            node_types[nid] = "event"
        else:
            vis_nodes.append({"id": nid, "label": str(nid), "type": "entity", "color": "#1f77b4"})
            node_types[nid] = "entity"

    # è¾¹è¡¨
    vis_edges = []
    for e in filtered_edges:
        u, v = e.get("from"), e.get("to")
        if u in top_ids and v in top_ids:
            title = ""
            if e.get("type") == "event_edge":
                title = (str(e.get("edge_type") or "related"))[:max_title_len]
            elif isinstance(v, str) and v.startswith("EVT:"):
                evt_key = v[4:]
                title = (events.get(evt_key, {}) or {}).get("event_summary", "")[:max_title_len]
            else:
                # å¯¹å®ä½“-å®ä½“å…³ç³»è¾¹æ˜¾ç¤ºè°“è¯
                title = (e.get("predicate") or e.get("title") or "")[:max_title_len]
            edge_out = {"from": u, "to": v, "title": title}
            # å…³é”®ï¼šæŠŠâ€œæ¯æ¡å…ƒç»„çš„æ—¶é—´â€ä¼ é€’ç»™å‰ç«¯å¿«ç…§ï¼ˆå¦‚æœæœ‰ï¼‰
            if e.get("time"):
                edge_out["time"] = e.get("time")
            vis_edges.append(edge_out)

    out_graph = Path(out_path_graph)
    ensure_dir(out_graph.parent)
    graph_data = {"nodes": vis_nodes, "edges": vis_edges}
    write_json_file(out_graph, graph_data, ensure_ascii=False, indent=None)

    # æ—¶é—´çº¿å¿«ç…§ï¼ˆå– TopN äº‹ä»¶æŒ‰æ—¶é—´æ’åºï¼‰
    tl_rows = []
    for evt_key, evt in events.items():
        ts = evt.get("event_start_time") or evt.get("first_seen") or evt.get("published_at")
        try:
            dt = datetime.fromisoformat(ts.replace("Z", "+00:00")) if ts else None
            if dt and dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
        except Exception:
            dt = None
        if not dt:
            continue
        if cutoff and dt < cutoff:
            continue
        ents = evt.get("entities", [])
        tl_rows.append({
            "time": ts,
            "event_summary": (evt.get("event_summary") or evt_key)[:max_title_len],
            "entities": ents[:20],
        })

    tl_rows = sorted(tl_rows, key=lambda x: x["time"])[: top_events]
    out_tl = Path(out_path_timeline)
    ensure_dir(out_tl.parent)
    write_json_file(out_tl, tl_rows, ensure_ascii=False, indent=None)

    return {
        "status": "ok",
        "graph_snapshot": str(out_graph),
        "timeline_snapshot": str(out_tl),
        "nodes": len(vis_nodes),
        "edges": len(vis_edges),
        "timeline_events": len(tl_rows),
    }


@register_tool(
    name="append_only_update_graph",
    description="ä»…è¿½åŠ æ–°äº‹ä»¶/å®ä½“åˆ°ç°æœ‰å›¾è°±ï¼Œä¸ä¿®æ”¹æ—§è®°å½•ï¼ˆå¯é€‰æ˜¯å¦è¿½åŠ åŸå§‹è¡¨è¿°ï¼‰",
    category="Knowledge Graph"
)
async def append_only_update_graph_tool(
    events_list: Any,
    default_source: str = "auto_pipeline",
    allow_append_original_forms: bool = True
) -> Dict[str, int]:
    """
    åŒ…è£… agent3.append_only_update_graphï¼š
    - ä»…å½“å®ä½“/äº‹ä»¶ä¸å­˜åœ¨æ—¶æ–°å¢
    - ä¸æ”¹æ—§ first_seen / sources / æ‘˜è¦ï¼›å¯é€‰æ˜¯å¦ä¸ºæ—§å®ä½“è¿½åŠ  original_forms
    - ä½¿ç”¨æ ‡å‡†äº‹ä»¶æ•°æ®ç®¡é“è¿›è¡Œå¤„ç†
    """
    # ä½¿ç”¨æ ‡å‡†äº‹ä»¶æ•°æ®ç®¡é“
    pipeline = StandardEventPipeline()

    # æ‰§è¡Œæ•°æ®ç®¡é“å¤„ç†
    pipeline_result = await pipeline.execute(events_list)

    # è·å–åºåˆ—åŒ–åçš„æ•°æ®
    processed_data = pipeline_result.get("serialization", [])
    if isinstance(processed_data, str):
        # å¦‚æœæ˜¯åºåˆ—åŒ–çš„JSONå­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æå›æ¥
        import json
        processed_data = json.loads(processed_data)

    kg = KnowledgeGraph()
    return kg.append_only_update(
        events_list=processed_data,
        default_source=default_source,
        allow_append_original_forms=allow_append_original_forms
    )


@register_tool(
    name="append_tmp_extracted_events",
    description="ä» data/tmp/extracted_events_*.jsonl è¯»å–äº‹ä»¶å¹¶è¿½åŠ åˆ°å›¾è°±ï¼ˆä¸æ”¹æ—§è®°å½•ï¼‰",
    category="Knowledge Graph"
)
def append_tmp_extracted_events(
    base_dir: str = "data/tmp",
    pattern: str = "extracted_events_*.jsonl",
    max_files: int = 0,
    default_source: str = "auto_pipeline",
    allow_append_original_forms: bool = True
) -> Dict[str, Any]:
    """
    è¯»å– tmp ä¸­çš„æå–ç»“æœ jsonl æ–‡ä»¶å¹¶è°ƒç”¨ append_only_update_graph è¿½åŠ åˆ°å®ä½“/äº‹ä»¶åº“.
    max_files=0 è¡¨ç¤ºå…¨éƒ¨; >0 æ—¶æŒ‰ä¿®æ”¹æ—¶é—´å€’åºå–å‰ N ä¸ª.
    """
    base = Path(base_dir)
    if not base.exists():
        return {"status": "error", "message": f"{base} not found"}
    files = sorted(base.glob(pattern), key=lambda x: x.stat().st_mtime, reverse=True)
    if max_files and max_files > 0:
        files = files[:max_files]
    events: List[Dict[str, Any]] = []
    for fp in files:
        try:
            with fp.open("r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        if isinstance(obj, dict):
                            events.append(obj)
                    except Exception:
                        continue
        except Exception:
            continue
    kg = KnowledgeGraph()
    res = kg.append_only_update(
        events_list=events,
        default_source=default_source,
        allow_append_original_forms=allow_append_original_forms
    )
    return {
        "status": "ok",
        "files_used": [str(f) for f in files],
        "added_entities": res.get("added_entities", 0),
        "added_events": res.get("added_events", 0)
    }


class KnowledgeGraph:
    """
    å‹ç¼©çŸ¥è¯†å›¾è°±ç³»ç»Ÿï¼Œç”¨äºç®¡ç†å®ä½“å’Œäº‹ä»¶ï¼Œæ”¯æŒé‡å¤æ£€æµ‹å’Œæ›´æ–°ã€‚
    """

    def __init__(self):
        self.entities_file = tools.ENTITIES_FILE
        self.events_file = tools.EVENTS_FILE
        self.abstract_map_file = tools.ABSTRACT_MAP_FILE
        self.entities_tmp_file = tools.ENTITIES_TMP_FILE
        self.abstract_tmp_file = tools.ABSTRACT_TMP_FILE
        self.kg_file = tools.KNOWLEDGE_GRAPH_FILE
        self.merge_rules_file = tools.CONFIG_DIR / "entity_merge_rules.json" # è§„åˆ™æ–‡ä»¶è·¯å¾„
        self.merge_rules = {} # å†…å­˜ä¸­çš„è§„åˆ™ç¼“å­˜
        self.settings = {}  # å»¶è¿ŸåŠ è½½é…ç½®
        self.graph = {
            "entities": {},  # å®ä½“ID -> å®ä½“ä¿¡æ¯
            "events": {},   # äº‹ä»¶æ‘˜è¦ -> äº‹ä»¶ä¿¡æ¯
            "edges": []     # è¾¹åˆ—è¡¨ï¼Œè¿æ¥å®ä½“å’Œäº‹ä»¶
        }
        self.llm_pool = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self._tmp_loaded = []  # è®°å½•å·²åŠ è½½çš„tmpæ–‡ä»¶ï¼Œåˆ·æ–°å®Œæˆåæ¸…ç†
        self._load_merge_rules() # åˆå§‹åŒ–æ—¶åŠ è½½è§„åˆ™
    def _init_llm_pool(self):
        """åˆå§‹åŒ–LLM APIæ± """
        if self.llm_pool is None:
            try:
                self.llm_pool = get_llm_pool()
                tools.log("[çŸ¥è¯†å›¾è°±] LLM APIæ± åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ åˆå§‹åŒ–LLM APIæ± å¤±è´¥: {e}")
                self.llm_pool = None

    def load_data(self) -> bool:
        """åŠ è½½å®ä½“ä¸äº‹ä»¶æ•°æ®ï¼ˆä»…ä½¿ç”¨SQLiteä¸»å­˜å‚¨ï¼‰"""
        try:
            # ä» SQLite ä¸»å­˜å‚¨åŠ è½½æ•°æ®
            from src.adapters.sqlite.store import get_store
            store = get_store()
            self.graph["entities"] = store.export_entities_json() or {}
            abstract_map = store.export_abstract_map_json() or {}

            # è½¬æ¢ abstract_map ä¸ºäº‹ä»¶æ ¼å¼
            self.graph["events"] = {
                abstract: {
                    "event_id": data.get("event_id", ""),
                    "abstract": abstract,
                    "entities": data.get("entities", []) or [],
                    "event_summary": data.get("event_summary", ""),
                    "event_types": data.get("event_types", []),
                    "entity_roles": data.get("entity_roles", {}),
                    "relations": data.get("relations", []),
                    "event_start_time": data.get("event_start_time", ""),
                    "event_start_time_text": data.get("event_start_time_text", ""),
                    "event_start_time_precision": data.get("event_start_time_precision", "unknown"),
                    "reported_at": data.get("reported_at", ""),
                    "sources": data.get("sources", []),
                    "first_seen": data.get("first_seen", ""),
                }
                for abstract, data in (abstract_map or {}).items()
                if isinstance(abstract, str) and isinstance(data, dict)
            }

            # é¢å¤–åŠ è½½ tmp æ–°å¢æ•°æ®ï¼ˆæœªåˆå¹¶çš„æ–°å¢å®ä½“/äº‹ä»¶ï¼‰
            self._load_tmp_entities()
            self._load_tmp_events()

            self._build_edges()
            tools.log(f"[çŸ¥è¯†å›¾è°±] æ•°æ®åŠ è½½æˆåŠŸ: {len(self.graph['entities'])} å®ä½“, {len(self.graph['events'])} äº‹ä»¶")
            return True
        except Exception as e:
            tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False

    def _build_edges(self):
        """æ„å»ºå®ä½“å’Œäº‹ä»¶ä¹‹é—´çš„è¾¹ï¼ˆæ‰€æœ‰å…ƒç»„/è¾¹å¼ºåˆ¶å¸¦æ—¶é—´ timeï¼‰"""
        self.graph['edges'] = []
        for abstract, event in self.graph['events'].items():
            # ç»Ÿä¸€äº‹ä»¶èŠ‚ç‚¹IDï¼šEVT:<abstract>ï¼ˆå…¼å®¹ç°æœ‰å¯è§†åŒ–ä¸è¿‡æ»¤é€»è¾‘ï¼‰
            evt_node = f"EVT:{abstract}"

            # è¾¹æ—¶é—´ï¼šä¼˜å…ˆ event_start_timeï¼Œå…¶æ¬¡ reported_at/first_seen
            event_time = ""
            if isinstance(event.get("event_start_time"), str) and event.get("event_start_time").strip():
                event_time = event.get("event_start_time").strip()
            elif isinstance(event.get("reported_at"), str) and event.get("reported_at").strip():
                event_time = event.get("reported_at").strip()
            elif isinstance(event.get("first_seen"), str) and event.get("first_seen").strip():
                event_time = event.get("first_seen").strip()
            if not event_time:
                event_time = datetime.now(timezone.utc).isoformat()

            for entity in event.get('entities', []):
                if entity in self.graph['entities']:
                    roles = []
                    roles_map = event.get("entity_roles", {})
                    if isinstance(roles_map, dict):
                        r = roles_map.get(entity, [])
                        if isinstance(r, str):
                            roles = [r]
                        elif isinstance(r, list):
                            roles = [x for x in r if isinstance(x, str) and x.strip()]
                    self.graph['edges'].append({
                        "from": entity,
                        "to": evt_node,
                        "type": "involved_in",
                        "roles": roles,
                        "time": event_time
                    })

            # é¢å¤–æ„å»ºå®ä½“-å…³ç³»-å®ä½“è¾¹ï¼ˆå¸¦äº‹ä»¶ä¸Šä¸‹æ–‡ï¼‰
            rels = event.get("relations", [])
            if isinstance(rels, list):
                for rel in rels:
                    if not isinstance(rel, dict):
                        continue
                    s = rel.get("subject")
                    p = rel.get("predicate")
                    o = rel.get("object")
                    if not isinstance(s, str) or not isinstance(p, str) or not isinstance(o, str):
                        continue
                    s, p, o = s.strip(), p.strip(), o.strip()
                    if not s or not p or not o:
                        continue
                    if s not in self.graph["entities"] or o not in self.graph["entities"]:
                        continue
                    rel_time = ""
                    if isinstance(rel.get("time"), str) and rel.get("time").strip():
                        rel_time = rel.get("time").strip()
                    if not rel_time:
                        rel_time = event_time
                    self.graph["edges"].append({
                        "from": s,
                        "to": o,
                        "type": "relation",
                        "predicate": p,
                        "event": abstract,
                        "time": rel_time
                    })

        # =========================
        # äº‹ä»¶-äº‹ä»¶æ¼”åŒ–è¾¹ï¼šä» SQLite event_edges æŠ•å½±åˆ° knowledge_graph.json
        # è¯´æ˜ï¼ševent_edges ä»¥ event_id å­˜å‚¨ï¼›è¿™é‡Œæ˜ å°„åˆ° abstract å†è¾“å‡º EVT:<abstract> ä¾›å‰ç«¯å±•ç¤ºã€‚
        # =========================
        try:
            from src.adapters.sqlite.store import canonical_event_id
            import sqlite3
            import json as _json

            # event_id -> abstractï¼ˆä¼˜å…ˆé€‰ canonicalï¼šsha1(evt:<abstract>) == event_idï¼‰
            evtid_to_abs = {}
            for abs_key, ev in (self.graph.get("events") or {}).items():
                if not isinstance(ev, dict):
                    continue
                eid = str(ev.get("event_id") or "").strip()
                if not eid:
                    continue
                # canonical ä¼˜å…ˆ
                if canonical_event_id(abs_key) == eid:
                    evtid_to_abs[eid] = abs_key
                else:
                    evtid_to_abs.setdefault(eid, abs_key)

            db_path = str(getattr(tools, "SQLITE_DB_FILE", None) or "")
            if db_path:
                conn = sqlite3.connect(db_path)
                try:
                    rows = conn.execute(
                        """
                        SELECT from_event_id, to_event_id, edge_type, time, reported_at, confidence, evidence_json
                        FROM event_edges
                        """
                    ).fetchall()
                finally:
                    conn.close()

                for from_id, to_id, edge_type, t, rep, conf, ev_json in rows:
                    from_id = str(from_id or "")
                    to_id = str(to_id or "")
                    if from_id not in evtid_to_abs or to_id not in evtid_to_abs:
                        continue
                    from_abs = evtid_to_abs[from_id]
                    to_abs = evtid_to_abs[to_id]
                    try:
                        evidence = _json.loads(ev_json or "[]")
                        if not isinstance(evidence, list):
                            evidence = []
                    except Exception:
                        evidence = []
                    time_val = str(t or "").strip() or datetime.now(timezone.utc).isoformat()
                    self.graph["edges"].append(
                        {
                            "from": f"EVT:{from_abs}",
                            "to": f"EVT:{to_abs}",
                            "type": "event_edge",
                            "edge_type": str(edge_type or "related"),
                            "time": time_val,
                            "reported_at": str(rep or ""),
                            "confidence": float(conf or 0.0),
                            "evidence": [x for x in evidence if isinstance(x, str) and x.strip()],
                        }
                    )
        except Exception:
            # SQLite ä¸å­˜åœ¨/è¡¨ä¸å­˜åœ¨/è§£æå¤±è´¥æ—¶ï¼Œå¿½ç•¥äº‹ä»¶æ¼”åŒ–è¾¹
            pass

    def build_graph(self) -> bool:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        return self.load_data()

    def _load_merge_rules(self):
        """åŠ è½½å®ä½“åˆå¹¶è§„åˆ™"""
        if self.merge_rules_file.exists():
            try:
                with open(self.merge_rules_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.merge_rules = data.get("merge_rules", {})
                tools.log(f"[çŸ¥è¯†å›¾è°±] å·²åŠ è½½ {len(self.merge_rules)} æ¡å®ä½“åˆå¹¶è§„åˆ™")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½åˆå¹¶è§„åˆ™å¤±è´¥: {e}")
        else:
            self.merge_rules = {}

    def _load_tmp_entities(self):
        """åŠ è½½å¹¶åˆå¹¶ tmp å®ä½“æ•°æ®"""
        if self.entities_tmp_file.exists():
            try:
                with open(self.entities_tmp_file, "r", encoding="utf-8") as f:
                    tmp_entities = json.load(f)
                    for name, data in tmp_entities.items():
                        if name in self.graph['entities']:
                            self._merge_entity_record(self.graph['entities'][name], data)
                        else:
                            self.graph['entities'][name] = data
                self._tmp_loaded.append(self.entities_tmp_file)
                tools.log(f"[çŸ¥è¯†å›¾è°±] å·²åŠ è½½ tmp å®ä½“ {len(tmp_entities)} æ¡")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½ tmp å®ä½“å¤±è´¥: {e}")

    def _load_tmp_events(self):
        """åŠ è½½å¹¶åˆå¹¶ tmp äº‹ä»¶æ•°æ®"""
        if self.abstract_tmp_file.exists():
            try:
                with open(self.abstract_tmp_file, "r", encoding="utf-8") as f:
                    tmp_events = json.load(f)
                    for abstract, data in tmp_events.items():
                        if abstract in self.graph['events']:
                            self._merge_event_record(self.graph['events'][abstract], data)
                        else:
                            self.graph['events'][abstract] = {
                                "abstract": abstract,
                                "entities": data.get("entities", []),
                                "event_summary": data.get("event_summary", ""),
                                "event_types": data.get("event_types", []),
                                "entity_roles": data.get("entity_roles", {}),
                                "relations": data.get("relations", []),
                                "event_start_time": data.get("event_start_time", ""),
                                "event_start_time_text": data.get("event_start_time_text", ""),
                                "event_start_time_precision": data.get("event_start_time_precision", "unknown"),
                                "sources": data.get("sources", []),
                                "first_seen": data.get("first_seen", "")
                            }
                self._tmp_loaded.append(self.abstract_tmp_file)
                tools.log(f"[çŸ¥è¯†å›¾è°±] å·²åŠ è½½ tmp äº‹ä»¶ {len(tmp_events)} æ¡")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½ tmp äº‹ä»¶å¤±è´¥: {e}")

    def _cleanup_tmp_files(self):
        """åˆ·æ–°å®Œæˆåæ¸…ç†å·²åŠ è½½çš„ tmp æ–‡ä»¶"""
        for path in self._tmp_loaded:
            try:
                path.unlink(missing_ok=True)
                tools.log(f"[çŸ¥è¯†å›¾è°±] ğŸ—‘ï¸ å·²æ¸…ç† tmp æ–‡ä»¶: {path}")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ æ— æ³•åˆ é™¤ tmp æ–‡ä»¶ {path}: {e}")
        self._tmp_loaded = []

    def _save_merge_rules(self):
        """ä¿å­˜å®ä½“åˆå¹¶è§„åˆ™"""
        try:
            data = {
                "merge_rules": self.merge_rules,
                "last_updated": time.strftime("%Y-%m-%dT%H:%M:%S")
            }
            with open(self.merge_rules_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            tools.log(f"[çŸ¥è¯†å›¾è°±] å·²ä¿å­˜åˆå¹¶è§„åˆ™åº“ (å…± {len(self.merge_rules)} æ¡)")
        except Exception as e:
            tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ ä¿å­˜åˆå¹¶è§„åˆ™å¤±è´¥: {e}")

    def _merge_entity_record(self, target: Dict[str, Any], source: Dict[str, Any]):
        """å°†sourceå®ä½“ä¿¡æ¯åˆå¹¶åˆ°targetï¼Œä¸åˆ é™¤èŠ‚ç‚¹"""
        if not source:
            return
        # sources
        primary_sources = set()
        for s in target.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        for s in source.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        target['sources'] = list(primary_sources)

        # original_forms
        primary_forms = set()
        for f in target.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)
        for f in source.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)
        target['original_forms'] = list(primary_forms)

        # first_seen å–æœ€æ—©
        primary_first = target.get('first_seen', '')
        source_first = source.get('first_seen', '')
        if source_first and (not primary_first or source_first < primary_first):
            target['first_seen'] = source_first

    def _merge_event_record(self, target: Dict[str, Any], source: Dict[str, Any]):
        """å°†sourceäº‹ä»¶ä¿¡æ¯åˆå¹¶åˆ°targetï¼Œä¸åˆ é™¤èŠ‚ç‚¹"""
        if not source:
            return
        # sources
        primary_sources = set()
        for s in target.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        for s in source.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        target['sources'] = list(primary_sources)

        # entities union
        ents = set(target.get('entities', []))
        ents.update(source.get('entities', []))
        target['entities'] = list(ents)

        # event_types unionï¼ˆä¿åºï¼‰
        def _merge_unique_list(a: Any, b: Any) -> List[str]:
            out: List[str] = []
            seen = set()
            for it in (a if isinstance(a, list) else []):
                if isinstance(it, str) and it and it not in seen:
                    seen.add(it)
                    out.append(it)
            for it in (b if isinstance(b, list) else []):
                if isinstance(it, str) and it and it not in seen:
                    seen.add(it)
                    out.append(it)
            return out

        src_types = source.get("event_types", [])
        if src_types:
            merged_types = _merge_unique_list(target.get("event_types", []), src_types)
            target["event_types"] = merged_types

        # entity_roles merge
        def _merge_roles(a: Any, b: Any) -> Dict[str, List[str]]:
            out: Dict[str, List[str]] = {}
            if isinstance(a, dict):
                for k, v in a.items():
                    if isinstance(k, str) and isinstance(v, list):
                        out[k] = [x for x in v if isinstance(x, str) and x]
            if isinstance(b, dict):
                for k, v in b.items():
                    if not isinstance(k, str):
                        continue
                    roles_list: List[str] = []
                    if isinstance(v, str):
                        roles_list = [v]
                    elif isinstance(v, list):
                        roles_list = [x for x in v if isinstance(x, str)]
                    roles_list = [x.strip() for x in roles_list if isinstance(x, str) and x.strip()]
                    if not roles_list:
                        continue
                    out[k] = _merge_unique_list(out.get(k, []), roles_list)
            return out

        src_roles = source.get("entity_roles", {})
        if src_roles:
            target["entity_roles"] = _merge_roles(target.get("entity_roles", {}), src_roles)

        # relations merge
        def _merge_relations(a: Any, b: Any) -> List[Dict[str, Any]]:
            out: Dict[Tuple[str, str, str], Dict[str, Any]] = {}
            def ns(x: Any) -> str:
                return x.strip() if isinstance(x, str) else ""
            def add_one(rel: Any):
                if not isinstance(rel, dict):
                    return
                s = ns(rel.get("subject", ""))
                p = ns(rel.get("predicate", ""))
                o = ns(rel.get("object", ""))
                if not s or not p or not o:
                    return
                key = (s, p, o)
                ev = rel.get("evidence", [])
                if key not in out:
                    out[key] = {"subject": s, "predicate": p, "object": o, "evidence": []}
                # evidence å…è®¸ str æˆ– list[str]
                ev_list: List[str] = []
                if isinstance(ev, str):
                    ev_list = [ev]
                elif isinstance(ev, list):
                    ev_list = [x for x in ev if isinstance(x, str)]
                ev_list = [x.strip() for x in ev_list if isinstance(x, str) and x.strip()]
                if ev_list:
                    cur = out[key].get("evidence", [])
                    if not isinstance(cur, list):
                        cur = []
                    for x in ev_list:
                        if x not in cur:
                            cur.append(x)
                    out[key]["evidence"] = cur
            for rel in (a if isinstance(a, list) else []):
                add_one(rel)
            for rel in (b if isinstance(b, list) else []):
                add_one(rel)
            return list(out.values())

        src_rels = source.get("relations", [])
        if src_rels:
            target["relations"] = _merge_relations(target.get("relations", []), src_rels)

        # event_start_time choose better
        def _choose_better_time(cur_t: Any, cur_txt: Any, cur_p: Any, inc_t: Any, inc_txt: Any, inc_p: Any) -> Tuple[str, str, str]:
            def ns(x: Any) -> str:
                return x.strip() if isinstance(x, str) else ""
            ct, ctxt, cp = ns(cur_t), ns(cur_txt), (ns(cur_p) or "unknown")
            it, itxt, ip = ns(inc_t), ns(inc_txt), (ns(inc_p) or "unknown")
            if not ct and it:
                return it, itxt, ip
            if ct and not it:
                return ct, ctxt, cp
            if not ct and not it:
                return "", "", cp
            rank = {"exact": 3, "date": 2, "relative": 1, "unknown": 0}
            rc, ri = rank.get(cp, 0), rank.get(ip, 0)
            if ri > rc:
                return it, itxt, ip
            if ri < rc:
                return ct, ctxt, cp
            if len(ct) >= 10 and ct[4:5] == "-" and ct[7:8] == "-" and len(it) >= 10 and it[4:5] == "-" and it[7:8] == "-":
                if it < ct:
                    return it, itxt, ip
            return ct, ctxt, cp

        nt, ntxt, np = _choose_better_time(
            target.get("event_start_time", ""),
            target.get("event_start_time_text", ""),
            target.get("event_start_time_precision", "unknown"),
            source.get("event_start_time", ""),
            source.get("event_start_time_text", ""),
            source.get("event_start_time_precision", "unknown"),
        )
        target["event_start_time"] = nt
        target["event_start_time_text"] = ntxt
        target["event_start_time_precision"] = np

        # first_seen å–æœ€æ—©
        primary_first = target.get('first_seen', '')
        source_first = source.get('first_seen', '')
        if source_first and (not primary_first or source_first < primary_first):
            target['first_seen'] = source_first

        # event_summary è‹¥ç¼ºå¤±åˆ™è¡¥
        if not target.get('event_summary') and source.get('event_summary'):
            target['event_summary'] = source['event_summary']

    def _ensure_settings_loaded(self):
        """ç¡®ä¿é…ç½®å·²åŠ è½½"""
        if not self.settings:
            self.settings = self._load_agent3_settings()

    def _load_agent3_settings(self) -> Dict[str, Any]:
        """
        åŠ è½½agent3ç›¸å…³é…ç½®ï¼Œä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ã€‚
        """
        config_manager = get_config_manager()

        # ä½¿ç”¨ConfigManagerè·å–é…ç½®ï¼Œé»˜è®¤å€¼åœ¨ConfigManagerä¸­å¤„ç†
        try:
            settings = {
                "entity_batch_size": config_manager.get_config_value("entity_batch_size", 80, "agent3_config"),
                "event_batch_size": config_manager.get_config_value("event_batch_size", 15, "agent3_config"),
                "event_bucket_days": config_manager.get_config_value("event_bucket_days", 7, "agent3_config"),
                "event_bucket_entity_overlap": config_manager.get_config_value("event_bucket_entity_overlap", 1, "agent3_config"),
                "event_bucket_max_size": config_manager.get_config_value("event_bucket_max_size", 40, "agent3_config"),
                "event_precluster_similarity": config_manager.get_config_value("event_precluster_similarity", 0.82, "agent3_config"),
                "event_precluster_limit": config_manager.get_config_value("event_precluster_limit", 300, "agent3_config"),
                "entity_precluster_similarity": config_manager.get_config_value("entity_precluster_similarity", 0.93, "agent3_config"),
                "entity_precluster_limit": config_manager.get_config_value("entity_precluster_limit", 500, "agent3_config"),
                "max_summary_chars": config_manager.get_config_value("max_summary_chars", 360, "agent3_config"),
                "entity_max_workers": config_manager.get_config_value("entity_max_workers", 3, "agent3_config"),
                "event_max_workers": config_manager.get_config_value("event_max_workers", 3, "agent3_config"),
                "rate_limit_per_sec": config_manager.get_config_value("rate_limit_per_sec", 1.0, "agent3_config"),
                "entity_evidence_per_entity": config_manager.get_config_value("entity_evidence_per_entity", 2, "agent3_config"),
                "entity_evidence_max_chars": config_manager.get_config_value("entity_evidence_max_chars", 400, "agent3_config"),
            }
            return settings
        except Exception as e:
            # è¿”å›é»˜è®¤å€¼
            return {
                "entity_batch_size": 80,
                "event_batch_size": 15,
                "event_bucket_days": 7,
                "event_bucket_entity_overlap": 1,
                "event_bucket_max_size": 40,
                "event_precluster_similarity": 0.82,
                "event_precluster_limit": 300,
                "entity_precluster_similarity": 0.93,
                "entity_precluster_limit": 500,
                "max_summary_chars": 360,
                "entity_max_workers": 3,
                "event_max_workers": 3,
                "rate_limit_per_sec": 1.0,
                "entity_evidence_per_entity": 2,
                "entity_evidence_max_chars": 400,
            }
    def _string_similarity(self, a: str, b: str) -> float:
        """
        å­—ç¬¦ä¸²ç›¸ä¼¼åº¦(0-1)
        
        ä½¿ç”¨æ··åˆç­–ç•¥æé«˜å‡†ç¡®åº¦ï¼š
        1. å®Œå…¨åŒ¹é… â†’ 1.0
        2. å½’ä¸€åŒ–åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™/ç©ºæ ¼/æ ‡ç‚¹ï¼‰â†’ 0.98
        3. Jaro-Winklerç®—æ³•ï¼ˆé€‚åˆçŸ­å­—ç¬¦ä¸²å’Œå…¬å¸åï¼‰
        4. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰â€”â€”æ”¯æŒè·¨è¯­è¨€åŒ¹é…
        5. ä¸­è‹±æ–‡æ··åˆå®ä½“ç‰¹æ®Šå¤„ç†
        """
        import re
        
        # å¿«é€Ÿè·¯å¾„ï¼šå®Œå…¨ç›¸åŒ
        if a == b:
            return 1.0
        
        # å½’ä¸€åŒ–ï¼šå»é™¤ç©ºæ ¼ã€æ ‡ç‚¹ã€è½¬å°å†™
        def normalize(s: str) -> str:
            # å»é™¤æ‰€æœ‰éå­—æ¯æ•°å­—å­—ç¬¦
            s = re.sub(r'[^\w]', '', s, flags=re.UNICODE)
            return s.lower()
        
        a_norm = normalize(a)
        b_norm = normalize(b)
        
        # å½’ä¸€åŒ–åç›¸åŒ
        if a_norm and b_norm and a_norm == b_norm:
            return 0.98
        
        # å°è¯•å¯¼å…¥Jaro-Winklerï¼ˆæ›´é€‚åˆå®ä½“åç§°ï¼‰
        try:
            from jellyfish import jaro_winkler_similarity
            jw_score = jaro_winkler_similarity(a.lower(), b.lower())
        except ImportError:
            # é™çº§åˆ°SequenceMatcher
            jw_score = SequenceMatcher(None, a.lower(), b.lower()).ratio()
        
        # ä¸­è‹±æ–‡æ··åˆå®ä½“ï¼šå°è¯•è¯­ä¹‰åŒ¹é…ï¼Œå¦åˆ™å¤§å¹…é™æƒ
        a_is_chinese = self._is_chinese(a)
        b_is_chinese = self._is_chinese(b)
        
        if a_is_chinese != b_is_chinese:
            # ä¸€ä¸ªä¸­æ–‡ä¸€ä¸ªè‹±æ–‡ï¼šå°è¯•è¯­ä¹‰åŒ¹é…
            try:
                from ...infra.semantic_matcher import get_semantic_matcher
                semantic_matcher = get_semantic_matcher()
                if semantic_matcher.is_available():
                    semantic_score = semantic_matcher.similarity(a, b)
                    if semantic_score is not None:
                        # è¯­ä¹‰ç›¸ä¼¼åº¦ä¼˜å…ˆï¼Œä½†è¦ä¸å­—ç¬¦ä¸²ç›¸ä¼¼åº¦åŠ æƒå¹³å‡
                        # è¯­ä¹‰ç›¸ä¼¼åº¦70%æƒé‡ + å­—ç¬¦ä¸²ç›¸ä¼¼åº¦30%æƒé‡
                        return semantic_score * 0.7 + jw_score * 0.3
            except Exception as e:
                # è¯­ä¹‰åŒ¹é…å¤±è´¥ï¼Œé™çº§åˆ°å­—ç¬¦ä¸²åŒ¹é…
                pass
            
            # å¦‚æœè¯­ä¹‰åŒ¹é…ä¸å¯ç”¨ï¼Œé™æƒ70%
            return jw_score * 0.3
        
        return jw_score

    def _is_chinese(self, text: str) -> bool:
        return any('\u4e00' <= ch <= '\u9fff' for ch in text)

    def _entity_type(self, name: str) -> str:
        """
        è½»é‡ç±»å‹åˆ¤å®šï¼šgeo/org/person/product/unknown
        ä»…ç”¨äºé˜»æ–­è·¨ç±»å‹åˆå¹¶ï¼Œå®å¯ unknownã€‚
        """
        n = name or ""
        geo_kw = ["å¸‚", "çœ", "å·", "å¿", "åŒº", "é•‡", "ä¹¡", "éƒ¡", "å²›", "åºœ", "é“", "è‡ªæ²»", "å…±å’Œå›½", "ç‹å›½", "ç‰¹åŒº", "è‡ªæ²»åŒº"]
        org_kw = ["å…¬å¸", "é›†å›¢", "é“¶è¡Œ", "æ”¿åºœ", "éƒ¨", "å±€", "ç½²", "é™¢", "å…", "è¡Œ", "å…š", "æœºæ„", "æ³•é™¢", "æ£€å¯Ÿé™¢", "å§”å‘˜ä¼š", "ç»„ç»‡", "è”ç›Ÿ", "ç†äº‹ä¼š", "åä¼š", "åŸºé‡‘ä¼š", "å¤§å­¦", "å­¦é™¢", "å­¦æ ¡", "å·¥å‚", "å‚", "æŠ¥", "ç”µè§†", "æ–°é—»", "æ—¥æŠ¥", "æ™šæŠ¥", "å‘¨æŠ¥", "ç¤¾"]
        product_kw = ["ç³»åˆ—", "ç‰ˆ", "å‹", "å‹å·", "Pro", "Ultra"]
        if any(k in n for k in geo_kw):
            return "geo"
        if any(k in n for k in org_kw):
            return "org"
        if " " in n or "Â·" in n:
            return "person"
        if any(k in n for k in product_kw):
            return "product"
        return "unknown"

    def _valid_entity_group(self, group: List[str]) -> bool:
        """ä½¿ç”¨LLMå†³ç­–å™¨åˆ¤æ–­å®ä½“ç»„æ˜¯å¦å¯ä»¥åˆå¹¶"""
        # æ”¶é›†è¯æ®
        evidence_map = self._collect_entity_evidence(group)
        
        # ä½¿ç”¨LLMå†³ç­–å™¨è¿›è¡Œåˆ¤æ–­
        if self.llm_pool:
            try:
                from ...adapters.extraction import LLMEntityMergeDecider
                decider = LLMEntityMergeDecider(self.llm_pool)
                decision = decider.decide_entity_merges(group, evidence_map)
                
                # å¦‚æœæœ‰åˆå¹¶ç»„ï¼Œè¯´æ˜å¯ä»¥åˆå¹¶
                # å¦‚æœæ²¡æœ‰åˆå¹¶ç»„ï¼Œè¯´æ˜ä¸åº”è¯¥åˆå¹¶
                can_merge = len(decision.merge_groups) > 0
                
                if not can_merge:
                    tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ LLMå†³ç­–å™¨é˜»æ­¢åˆå¹¶: {group}")
                    # è®°å½•è¯¦ç»†åˆ†æç»“æœ
                    for analysis in decision.entity_analyses:
                        tools.log(f"[çŸ¥è¯†å›¾è°±] å®ä½“åˆ†æ - {analysis.entity}: ç±»å‹={analysis.entity_type.value}, ç½®ä¿¡åº¦={analysis.confidence}, ç†ç”±={analysis.reasoning}")
                
                return can_merge
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ LLMå†³ç­–å™¨è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°ç¡¬ç¼–ç é€»è¾‘: {e}")
                # å›é€€åˆ°åŸæœ‰çš„ç¡¬ç¼–ç é€»è¾‘
                types = set()
                for name in group:
                    t = self._entity_type(name)
                    if t != "unknown":
                        types.add(t)
                # å¦‚æœæ£€æµ‹åˆ°å¤šç§å·²çŸ¥ç±»å‹åˆ™è§†ä¸ºé«˜é£é™©
                if len(types) > 1:
                    tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ è·¨ç±»å‹åˆå¹¶è¢«é˜»æ­¢: {group} | types={types}")
                    return False
                return True
        else:
            # å¦‚æœæ²¡æœ‰LLMæ± ï¼Œå›é€€åˆ°åŸæœ‰çš„ç¡¬ç¼–ç é€»è¾‘
            types = set()
            for name in group:
                t = self._entity_type(name)
                if t != "unknown":
                    types.add(t)
            # å¦‚æœæ£€æµ‹åˆ°å¤šç§å·²çŸ¥ç±»å‹åˆ™è§†ä¸ºé«˜é£é™©
            if len(types) > 1:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ è·¨ç±»å‹åˆå¹¶è¢«é˜»æ­¢: {group} | types={types}")
                return False
            return True

    def _collect_entity_evidence(self, entities_batch: List[str]) -> Dict[str, List[str]]:
        """
        ä¸ºå®ä½“æ‰¹æ¬¡æ”¶é›†ç›¸å…³äº‹ä»¶æ‘˜è¦+å®ä½“ï¼Œå‡å°‘å¹»è§‰ã€‚
        """
        self._ensure_settings_loaded()
        per_entity = int(self.settings.get("entity_evidence_per_entity", 2))
        max_chars = int(self.settings.get("entity_evidence_max_chars", 400))
        evidence: Dict[str, List[str]] = {e: [] for e in entities_batch}
        for abstract, event in self.graph['events'].items():
            ents = event.get('entities', [])
            summary = self._trim_text(event.get('event_summary', "") or abstract, max_chars)
            for e in ents:
                if e in evidence and len(evidence[e]) < per_entity:
                    evidence[e].append(f"{abstract} | {', '.join(ents)} | {summary}")
        return evidence

    def _trim_text(self, text: str, max_chars: int) -> str:
        """æ§åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿"""
        if not text or max_chars <= 0:
            return text
        if len(text) <= max_chars:
            return text
        return text[:max_chars] + "..."

    def _parse_time(self, ts: str) -> float:
        """å°½é‡è§£ææ—¶é—´æˆ³ï¼Œå¤±è´¥è¿”å›0"""
        if not ts:
            return 0
        try:
            # æ”¯æŒISOå­—ç¬¦ä¸²
            return datetime.fromisoformat(ts.replace("Z", "+00:00")).timestamp()
        except Exception:
            try:
                return time.mktime(time.strptime(ts, "%Y-%m-%d %H:%M:%S"))
            except Exception:
                return 0

    def _bucket_events_by_time_and_entity(
        self,
        window_days: int,
        min_entity_overlap: int,
        max_bucket_size: int
    ) -> List[Dict[str, Any]]:
        """
        æŒ‰æ—¶é—´çª—å£ä¸å®ä½“äº¤é›†åˆ†æ¡¶äº‹ä»¶ï¼Œå‡å°‘è·¨æœŸ/è·¨ä¸»ä½“æ··æ‚ã€‚
        """
        events_items = list(self.graph["events"].items())
        window_sec = window_days * 86400

        # é¢„æ’åºï¼Œæ—¶é—´ç¼ºå¤±æ”¾æœ«å°¾
        def _sort_key(item):
            ts = self._parse_time(item[1].get("first_seen", ""))
            return ts if ts > 0 else float("inf")

        events_items.sort(key=_sort_key)

        buckets: List[Dict[str, Any]] = []
        for abstract, event in events_items:
            entities = set(event.get("entities", []))
            ts = self._parse_time(event.get("first_seen", ""))
            placed = False
            for bucket in buckets:
                if len(bucket["keys"]) >= max_bucket_size:
                    continue
                # æ—¶é—´çª—å£åˆ¤å®š
                if bucket["min_time"] and ts and ts < bucket["min_time"] - window_sec:
                    continue
                if bucket["max_time"] and ts and ts > bucket["max_time"] + window_sec:
                    continue
                # å®ä½“äº¤é›†åˆ¤å®š
                if min_entity_overlap > 0:
                    if not entities or len(bucket["entities"].intersection(entities)) < min_entity_overlap:
                        continue
                # å‘½ä¸­ï¼ŒåŠ å…¥æ¡¶
                bucket["keys"].append(abstract)
                bucket["entities"].update(entities)
                if ts:
                    bucket["min_time"] = min(bucket["min_time"] or ts, ts)
                    bucket["max_time"] = max(bucket["max_time"] or ts, ts)
                placed = True
                break
            if not placed:
                buckets.append({
                    "keys": [abstract],
                    "entities": set(entities),
                    "min_time": ts if ts else None,
                    "max_time": ts if ts else None
                })
        tools.log(f"[çŸ¥è¯†å›¾è°±] äº‹ä»¶åˆ†æ¡¶å®Œæˆï¼Œå…± {len(buckets)} ä¸ªæ¡¶")
        return buckets

    def _precluster_entities_by_string(self, entities: List[str], threshold: float, limit: int) -> List[List[str]]:
        """
        åŸºäºå­—ç¬¦ä¸²ç›¸ä¼¼åº¦çš„è½»é‡é¢„èšç±»ï¼Œé¿å…LLMè¿‡é‡è¾“å…¥ã€‚
        
        âš ï¸ é‡è¦ï¼šé¢„èšç±»ä»…åšç²—ç­›ï¼Œé«˜ç›¸ä¼¼åº¦ï¼ˆ>=thresholdï¼‰çš„æ‰è¿›å…¥å€™é€‰ç»„
        æ‰€æœ‰å€™é€‰ç»„å¿…é¡»ç»è¿‡LLMäºŒæ¬¡éªŒè¯æ‰èƒ½çœŸæ­£åˆå¹¶
        """
        if len(entities) == 0 or len(entities) > limit:
            return []
        res = []
        used = set()
        for i, ent in enumerate(entities):
            if ent in used:
                continue
            group = [ent]
            used.add(ent)
            for other in entities[i+1:]:
                if other in used:
                    continue
                # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦æ£€æŸ¥
                if self._string_similarity(ent, other) >= threshold:
                    group.append(other)
                    used.add(other)
            if len(group) > 1:
                res.append(group)
        if res:
            tools.log(f"[çŸ¥è¯†å›¾è°±] æœ¬åœ°å®ä½“é¢„èšç±»å‘ç° {len(res)} ç»„å¯èƒ½é‡å¤ï¼ˆéœ€LLMäºŒæ¬¡éªŒè¯ï¼‰")
        return res

    def _precluster_events_by_string(
        self,
        events_map: Dict[str, Dict[str, Any]],
        keys: List[str],
        threshold: float,
        limit: int,
        max_summary_chars: int
    ) -> List[List[str]]:
        """
        åŒæ¡¶äº‹ä»¶çš„å­—ç¬¦ä¸²è¿‘ä¼¼èšç±»ï¼Œå‡å°‘LLMè´Ÿæ‹…ã€‚
        """
        if len(keys) == 0 or len(keys) > limit:
            return []

        def norm_text(k: str) -> str:
            evt = events_map.get(k, {})
            summary = self._trim_text(evt.get("event_summary", "") or "", max_summary_chars)
            return (k + " " + summary).lower()

        texts = {k: norm_text(k) for k in keys}
        res = []
        used = set()
        for i, key in enumerate(keys):
            if key in used:
                continue
            base = texts.get(key, "")
            group = [key]
            used.add(key)
            for other in keys[i+1:]:
                if other in used:
                    continue
                if self._string_similarity(base, texts.get(other, "")) >= threshold:
                    group.append(other)
                    used.add(other)
            if len(group) > 1:
                res.append(group)
        if res:
            tools.log(f"[çŸ¥è¯†å›¾è°±] æœ¬åœ°äº‹ä»¶é¢„èšç±»å‘ç° {len(res)} ç»„å¯èƒ½é‡å¤")
        return res

    def _apply_merge_rules(self) -> bool:
        """
        åº”ç”¨æœ¬åœ°åˆå¹¶è§„åˆ™
        è¿”å›: æ˜¯å¦æœ‰æ›´æ–°
        """
        updated = False
        if not self.merge_rules:
            return False

        # éå†ç°æœ‰å®ä½“ï¼Œçœ‹æ˜¯å¦åŒ¹é…è§„åˆ™
        # æ³¨æ„ï¼šéœ€è¦åœ¨éå†æ—¶å¤„ç†ï¼Œé¿å…å­—å…¸å¤§å°å˜åŒ–é—®é¢˜ï¼Œé€šå¸¸æ”¶é›†åå†å¤„ç†
        to_merge = []
        for entity in list(self.graph['entities'].keys()):
            if entity in self.merge_rules:
                target = self.merge_rules[entity]
                # åªæœ‰å½“ç›®æ ‡å®ä½“ä¹Ÿå­˜åœ¨ï¼Œæˆ–è€…ç›®æ ‡å°±æ˜¯æˆ‘ä»¬æƒ³è¦ç»Ÿä¸€åˆ°çš„åç§°æ—¶ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºå¦‚æœç›®æ ‡åœ¨åº“é‡Œæˆ–æˆ‘ä»¬å†³å®šæ”¹åï¼‰
                # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾è§„åˆ™æ˜¯ A -> Bï¼Œå¦‚æœ A å­˜åœ¨ï¼Œå°±å°è¯•åˆå¹¶åˆ° Bã€‚
                # å¦‚æœ B ä¸åœ¨åº“é‡Œï¼Œå°±æŠŠ A é‡å‘½åä¸º Bã€‚
                if target != entity:
                    to_merge.append((target, entity))

        for primary, duplicate in to_merge:
            # å¦‚æœç›®æ ‡å®ä½“ä¸å­˜åœ¨ï¼Œå…ˆé‡å‘½å
            if primary not in self.graph['entities'] and duplicate in self.graph['entities']:
                self.graph['entities'][primary] = self.graph['entities'][duplicate]
                del self.graph['entities'][duplicate]
                # æ›´æ–°äº‹ä»¶å¼•ç”¨
                for abstract, event in self.graph['events'].items():
                    entities = event.get('entities', [])
                    if duplicate in entities:
                        event['entities'] = [primary if e == duplicate else e for e in entities]
                    # åŒæ­¥æ›´æ–° entity_roles keyï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    roles_map = event.get("entity_roles", {})
                    if isinstance(roles_map, dict) and duplicate in roles_map:
                        dup_roles = roles_map.get(duplicate, [])
                        prim_roles = roles_map.get(primary, [])
                        merged = []
                        seen = set()
                        for r in (prim_roles if isinstance(prim_roles, list) else []):
                            if isinstance(r, str) and r and r not in seen:
                                seen.add(r)
                                merged.append(r)
                        for r in (dup_roles if isinstance(dup_roles, list) else []):
                            if isinstance(r, str) and r and r not in seen:
                                seen.add(r)
                                merged.append(r)
                        if merged:
                            roles_map[primary] = merged
                        roles_map.pop(duplicate, None)
                        event["entity_roles"] = roles_map

                    # åŒæ­¥æ›´æ–° relations ä¸»å®¢ä½“
                    rels = event.get("relations", [])
                    if isinstance(rels, list):
                        changed = False
                        new_rels = []
                        for rel in rels:
                            if not isinstance(rel, dict):
                                continue
                            s = rel.get("subject", "")
                            o = rel.get("object", "")
                            if isinstance(s, str) and s.strip() == duplicate:
                                rel["subject"] = primary
                                changed = True
                            if isinstance(o, str) and o.strip() == duplicate:
                                rel["object"] = primary
                                changed = True
                            new_rels.append(rel)
                        if changed:
                            event["relations"] = new_rels
                tools.log(f"[çŸ¥è¯†å›¾è°±][è§„åˆ™] é‡å‘½åå®ä½“: {duplicate} -> {primary}")
                updated = True
            elif primary in self.graph['entities'] and duplicate in self.graph['entities']:
                # å¦‚æœéƒ½å­˜åœ¨ï¼Œåˆ™åˆå¹¶
                self._merge_entities(primary, duplicate)
                updated = True

        return updated

    def compress_with_llm(self) -> Dict[str, List[str]]:
        """
        ä½¿ç”¨LLMåˆ†æå‹ç¼©çŸ¥è¯†å›¾è°±ï¼Œè¾“å‡ºé‡å¤çš„å®ä½“å’Œäº‹ä»¶æŠ½è±¡ã€‚
        åˆ†æ‰¹å¤„ç†ä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é•¿ã€‚
        é›†æˆè§„åˆ™ä¼˜å…ˆç­–ç•¥ã€‚
        """
        # é¦–å…ˆåº”ç”¨æœ¬åœ°è§„åˆ™
        rule_applied = self._apply_merge_rules()
        if rule_applied:
            self._save_data()

        self._init_llm_pool()
        if self.llm_pool is None:
            tools.log("[çŸ¥è¯†å›¾è°±] âŒ LLMä¸å¯ç”¨ï¼Œè·³è¿‡å‹ç¼©")
            return {"duplicate_entities": [], "duplicate_events": []}

        self._ensure_settings_loaded()
        # å¹¶å‘ä¸é€Ÿç‡æ§åˆ¶
        rate_limit = float(self.settings.get("rate_limit_per_sec", 1.0))
        limiter = RateLimiter(rate_limit) if rate_limit > 0 else None        # å¤„ç†å®ä½“å’Œäº‹ä»¶
        all_duplicate_entities = self._compress_entities_with_llm(limiter)
        all_duplicate_events = self._compress_events_with_llm(limiter)

        return {
            "duplicate_entities": all_duplicate_entities,
            "duplicate_events": all_duplicate_events
        }

    def _compress_entities_with_llm(self, limiter: Optional[RateLimiter]) -> List[List[str]]:
        """ä½¿ç”¨LLMå‹ç¼©å®ä½“
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. é¢„èšç±»ï¼šæ‰¾å‡ºé«˜åº¦ç›¸ä¼¼çš„å®ä½“ç»„ï¼ˆthreshold=0.93ï¼‰
        2. æ‰¹å¤„ç†ï¼šåªå°†é¢„èšç±»ç»„é€ç»™LLMäºŒæ¬¡éªŒè¯ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å®ä½“
        è¿™æ ·å¯ä»¥é¿å…ä¸ç›¸å…³å®ä½“è¢«é”™è¯¯åˆ†ç»„
        """
        self._ensure_settings_loaded()
        all_duplicate_entities = []
        # æ’åºä»¥å¢åŠ ç›¸ä¼¼å®ä½“ç›¸é‚»çš„æ¦‚ç‡
        entities_list = sorted(list(self.graph['entities'].keys()))
        if not entities_list:
            return all_duplicate_entities

        # é¢„èšç±»ï¼šæ‰¾å‡ºé«˜åº¦ç›¸ä¼¼çš„å®ä½“ç»„
        precluster_entities = self._precluster_entities_by_string(
            entities_list,
            threshold=float(self.settings.get("entity_precluster_similarity", 0.93)),
            limit=int(self.settings.get("entity_precluster_limit", 500))
        )
        
        # âš ï¸ é‡è¦ä¿®æ”¹ï¼šåªå¤„ç†é¢„èšç±»çš„ç»„ï¼Œä¸å†å¯¹æ‰€æœ‰å®ä½“åšæ‰¹å¤„ç†
        # è¿™é¿å…äº†â€œæ°‘ä¼—å…šâ€ä¸â€œå¸ƒæœ—å¤§å­¦â€è¿™ç§ä¸ç›¸å…³å®ä½“è¢«é€ç»™LLMåˆ¤æ–­
        if precluster_entities:
            tools.log(f"[çŸ¥è¯†å›¾è°±] é¢„èšç±»å‘ç° {len(precluster_entities)} ç»„é«˜åº¦ç›¸ä¼¼å®ä½“ï¼Œå¼€å§‹LLMäºŒæ¬¡éªŒè¯")
            
            # å°†é¢„èšç±»ç»„è½¬æ¢ä¸ºæ‰¹æ¬¡æ ¼å¼
            entity_batches = [(idx, group) for idx, group in enumerate(precluster_entities)]
            
            if entity_batches:
                entity_results = self._process_entity_batches(entity_batches, limiter)
                all_duplicate_entities.extend(entity_results)
        else:
            tools.log(f"[çŸ¥è¯†å›¾è°±] é¢„èšç±»æœªå‘ç°é«˜åº¦ç›¸ä¼¼çš„å®ä½“ç»„ï¼Œè·³è¿‡LLMå¤„ç†")

        return all_duplicate_entities

    def _process_entity_batches(self, entity_batches: List[Tuple[int, List[str]]],
                               limiter: Optional[RateLimiter]) -> List[List[str]]:
        """å¤„ç†å®ä½“æ‰¹æ¬¡"""
        self._ensure_settings_loaded()
        entity_workers = int(self.settings.get("entity_max_workers", 3))
        all_results = []
        new_rules_count = 0

        def _run_entity_batch(idx: int, batch: List[str]) -> List[List[str]]:
            tools.log(f"[çŸ¥è¯†å›¾è°±] å¤„ç†å®ä½“æ‰¹æ¬¡ {idx+1}/{len(entity_batches)} (å¤§å°: {len(batch)})")
            prompt = self._prepare_entity_compression_prompt_strict(batch)
            response = self._call_llm_limited(prompt, timeout=90, limiter=limiter)
            return self._parse_entity_response(response) if response else []

        # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†çº¿ç¨‹å¹¶å‘
        async_executor = AsyncExecutor()
        entity_task_results = async_executor.run_threaded_tasks(
            # ç›´æ¥ä¼ å…¥ (idx, batch) é¿å…é€šè¿‡ list.index åæŸ¥å¯¼è‡´ ValueError
            tasks=entity_batches,
            func=lambda it: _run_entity_batch(it[0], it[1]),
            max_workers=entity_workers
        )

        for batch_dupes in entity_task_results:
            if batch_dupes:
                for group in batch_dupes:
                    if len(group) >= 2 and self._valid_entity_group(group):
                        primary, dupes = self._choose_primary_entity(group)
                        for duplicate in dupes:
                            if duplicate not in self.merge_rules:
                                self.merge_rules[duplicate] = primary
                                new_rules_count += 1
                        all_results.append([primary] + dupes)

        if new_rules_count > 0:
            self._save_merge_rules()

        return all_results

    def _compress_events_with_llm(self, limiter: Optional[RateLimiter]) -> List[List[str]]:
        """ä½¿ç”¨LLMå‹ç¼©äº‹ä»¶"""
        all_duplicate_events = []

        self._ensure_settings_loaded()

        events_list = sorted(list(self.graph['events'].keys()))
        if not events_list:
            return all_duplicate_events

        # äº‹ä»¶åˆ†æ¡¶
        bucket_days = int(self.settings.get("event_bucket_days", 7))
        bucket_overlap = int(self.settings.get("event_bucket_entity_overlap", 1))
        bucket_max_size = int(self.settings.get("event_bucket_max_size", 40))
        buckets = self._bucket_events_by_time_and_entity(bucket_days, bucket_overlap, bucket_max_size)

        if buckets:
            event_results = self._process_event_buckets(buckets, limiter)
            all_duplicate_events.extend(event_results)

        return all_duplicate_events

    def _process_event_buckets(self, buckets: List[Dict[str, Any]],
                              limiter: Optional[RateLimiter]) -> List[List[str]]:
        """å¤„ç†äº‹ä»¶æ¡¶"""
        self._ensure_settings_loaded()
        event_workers = int(self.settings.get("event_max_workers", 3))
        BATCH_SIZE_EVT = int(self.settings.get("event_batch_size", 15))
        evt_similarity = float(self.settings.get("event_precluster_similarity", 0.82))
        evt_limit = int(self.settings.get("event_precluster_limit", 300))
        max_summary_chars = int(self.settings.get("max_summary_chars", 360))

        def _run_event_bucket(idx: int, bucket: Dict[str, Any]) -> List[List[str]]:
            bucket_keys = bucket.get("keys", [])
            bucket_events = {k: self.graph['events'][k] for k in bucket_keys if k in self.graph['events']}
            local_dupes: List[List[str]] = []

            # é¢„èšç±»
            pre_clusters = self._precluster_events_by_string(
                bucket_events, bucket_keys, evt_similarity, evt_limit, max_summary_chars
            )
            if pre_clusters:
                local_dupes.extend(pre_clusters)

            if len(bucket_keys) <= 1:
                # tools.log(f"[çŸ¥è¯†å›¾è°±] è·³è¿‡äº‹ä»¶æ¡¶ {idx+1}/{len(buckets)}ï¼ˆä»…1æ¡ï¼Œæ— éœ€å»é‡ï¼‰")
                return local_dupes

            # LLMæ‰¹å¤„ç†
            total_batches = (len(bucket_keys) - 1) // BATCH_SIZE_EVT + 1
            for i in range(0, len(bucket_keys), BATCH_SIZE_EVT):
                batch_keys = bucket_keys[i:i+BATCH_SIZE_EVT]
                batch_events = {
                    k: {
                        **bucket_events.get(k, {}),
                        "event_summary": self._trim_text(
                            bucket_events.get(k, {}).get("event_summary", "") or "",
                            max_summary_chars
                        )
                    }
                    for k in batch_keys
                }
                tools.log(
                    f"[çŸ¥è¯†å›¾è°±] å¤„ç†äº‹ä»¶æ¡¶ {idx+1}/{len(buckets)} çš„æ‰¹æ¬¡ {i//BATCH_SIZE_EVT + 1}/{total_batches} (å¤§å°: {len(batch_keys)})"
                )
                prompt = self._prepare_event_compression_prompt(batch_events)
                response = self._call_llm_limited(prompt, timeout=120, limiter=limiter)
                if response:
                    batch_dupes = self._parse_event_response(response)
                    local_dupes.extend(batch_dupes)
            return local_dupes

        # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†çº¿ç¨‹å¹¶å‘
        async_executor = AsyncExecutor()
        event_task_results = async_executor.run_threaded_tasks(
            tasks=[bucket for bucket in buckets],
            func=lambda bucket: _run_event_bucket(buckets.index(bucket), bucket),
            max_workers=event_workers
        )

        all_results = []
        for batch_dupes in event_task_results:
            if batch_dupes:
                all_results.extend(batch_dupes)

        return all_results

    def _call_llm(self, prompt: str, timeout: int) -> Optional[str]:
        """ç»Ÿä¸€LLMè°ƒç”¨ - ä½¿ç”¨å·¥å…·å‡½æ•°"""
        return call_llm_with_retry(
            llm_pool=self.llm_pool,
            prompt=prompt,
            max_tokens=4000,
            timeout=timeout,
            retries=2
        )

    def _call_llm_limited(self, prompt: str, timeout: int, limiter: Optional[RateLimiter]) -> Optional[str]:
        """å¸¦å…¨å±€QPSé™åˆ¶çš„LLMè°ƒç”¨"""
        return call_llm_with_retry(
            llm_pool=self.llm_pool,
            prompt=prompt,
            max_tokens=4000,
            timeout=timeout,
            retries=2,
            limiter=limiter
        )

    def _choose_primary_entity(self, group: List[str]) -> (str, List[str]):
        """
        é€‰æ‹©ä¸»å®ä½“ï¼šä¼˜å…ˆä¸­æ–‡ï¼Œå…¶æ¬¡ first_seen æœ€æ—©ï¼Œå†å…¶æ¬¡åç§°é•¿åº¦ã€‚
        è¿”å› (primary, duplicates)
        """
        if not group:
            return "", []
        best = None
        best_key = None
        for name in group:
            info = self.graph['entities'].get(name, {})
            is_cn = self._is_chinese(name)
            ts = self._parse_time(info.get('first_seen', ''))
            ts_key = ts if ts > 0 else float('inf')
            key = (0 if is_cn else 1, ts_key, len(name))
            if best_key is None or key < best_key:
                best = name
                best_key = key
        duplicates = [n for n in group if n != best]
        return best, duplicates

    def _prepare_entity_compression_prompt_strict(self, entities_batch: List[str]) -> str:
        evidence_map = self._collect_entity_evidence(entities_batch)
        evidence_lines = []
        for ent, evs in evidence_map.items():
            if evs:
                for ev in evs:
                    evidence_lines.append(f"{ent} <= {ev}")

        prompt = f"""ä½ æ˜¯ä¸€åçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ä»»åŠ¡ï¼šä»…åœ¨æœ‰å……åˆ†è¯æ®æ—¶è®¤å®šå®ä½“ä¸ºåŒä¸€ä¸»ä½“ï¼ˆåˆ«å/ç¼©å†™/ä¸­è‹±æ–‡/æ³•å®šå…¨ç§°å·®å¼‚ï¼‰ã€‚ä¸è¦å› ä¸ºè¡Œä¸šç›¸ä¼¼ã€ä¸Šä¸‹çº§å…³ç³»æˆ–åœ°åŸŸç›¸ä¼¼è€Œåˆå¹¶ã€‚

ã€é«˜é£é™©è¯¯åˆ¤ç¤ºä¾‹ã€‘
- å®ä½“å…·æœ‰ç‰¹åŒ–èŒèƒ½æˆ–åŠŸæ•ˆæˆ–ç”¨é€”ï¼Œä¸å¯åˆå¹¶
- è¡Œä½¿èŒèƒ½çš„ç»„ç»‡ã€æœºæ„ä¸å…¶ä¸‹è¾–çš„æ›´å…·ä½“èŒèƒ½çš„ç»„ç»‡ã€æœºæ„ä¸å¯åˆå¹¶
- "å¤§å­¦" ä¸ "è”ç›Ÿ/åä¼š/éƒ¨é—¨/å¤®è¡Œ" ä¸æ˜¯åŒä¸€ä¸»ä½“
- ä¸åŒå›½å®¶/åœ°åŒºçš„åŒåæœºæ„ï¼Œä¸å¯åˆå¹¶
- ä¸Šå¸‚å…¬å¸ vs å­å…¬å¸/æ§è‚¡è‚¡ä¸œï¼Œä¸å¯åˆå¹¶
- æ”¿åºœéƒ¨é—¨ vs ä¸Šçº§æ”¿åºœï¼Œä¸å¯åˆå¹¶
- å›½å®¶/çœå·/åŸå¸‚/åŒºå¿ ä¹‹é—´ä¸å¾—äº’å¹¶ï¼Œä¹Ÿä¸å¾—è·¨å›½åˆå¹¶
- å…¬å¸/æœºæ„ â‰  äº§å“/å“ç‰Œ/å‹å·ï¼Œä¸å¾—äº’å¹¶
- äººå â‰  å…¬å¸/æœºæ„/åœ°ç†/äº§å“ï¼Œä¸å¾—äº’å¹¶
- åª’ä½“åç§° â‰  åœ°ç‚¹/æ”¿åºœ/ä¼ä¸š/äººå
- ä½“è‚²ä¿±ä¹éƒ¨/èµ›äº‹ â‰  åŸå¸‚/å›½å®¶/æ”¿åºœ/ä¸ªäºº

ã€å®ä½“åˆ—è¡¨ã€‘
{json.dumps(entities_batch, ensure_ascii=False, indent=2)}

ã€è¯æ®ï¼ˆéƒ¨åˆ†ç›¸å…³äº‹ä»¶æ‘˜è¦ï¼Œä¾›å‚è€ƒï¼Œé¿å…å¹»è§‰ï¼‰ã€‘
æ ¼å¼: å®ä½“ <= æ‘˜è¦ | å‚ä¸å®ä½“ | æè¿°
{chr(10).join(evidence_lines) if evidence_lines else "ï¼ˆæ— å¯ç”¨äº‹ä»¶ï¼Œè°¨æ…åˆå¹¶ï¼‰"}

ã€è¦æ±‚ã€‘
- ä¸»å®ä½“ä¼˜å…ˆæ›´é€šç”¨ï¼ˆæˆ–æ›´å€¾å‘ä¸­å›½äººè¡¨è¾¾ï¼‰ã€æ›´è¯¦ç»†ã€æ›´ç²¾ç¡®ã€æ›´å­¦æœ¯ï¼ˆ"æ›´XX"æŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰
- åªè¾“å‡ºç¡®å®šä¸ºåŒä¸€ä¸»ä½“çš„ç»„åˆï¼›ä¸ç¡®å®šå°±è¿”å›ç©ºã€‚
- ä¼˜å…ˆä¸¥æ ¼åŒ¹é…ï¼šåŒåã€æ˜æ˜¾è¯‘åã€ç¼©å†™å±•å¼€ã€‚
- ä¸è¦æ”¹å†™åç§°æ ¼å¼ï¼ˆä¸è¦æ·»åŠ ä¹¦åå·/å¼•å·/æ‹¬å·ç­‰æ ‡ç‚¹ï¼‰ã€‚
- åœ°ç†/æœºæ„ç±»åˆå¹¶éœ€åŒä¸€å›½å®¶/è¡Œæ”¿å±‚çº§ï¼›äººåä¸å¾—ä¸éäººååˆå¹¶ï¼›å…¬å¸ä¸å¾—ä¸äº§å“/å“ç‰Œåˆå¹¶ã€‚
- å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼š
{{
  "duplicate_entities": [
    ["ä¸»å®ä½“", "åˆ«åæˆ–é‡å¤"],
    ["ä¸»å®ä½“2", "åˆ«å2", "åˆ«å3"]
  ]
}}
å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å› {{ "duplicate_entities": [] }}ã€‚åªè¾“å‡ºJSONã€‚
"""
        return prompt

    def _prepare_event_compression_prompt(self, events_batch: Dict) -> str:
        prompt = f"""ä½ æ˜¯ä¸€åçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ä»»åŠ¡ï¼šä»…åœ¨æè¿°"åŒä¸€å…·ä½“äº‹å®"æ—¶æ‰è§†ä¸ºé‡å¤äº‹ä»¶ã€‚ä¸è¦åˆå¹¶ä¸åŒä¸»ä½“ã€ä¸Šä¸‹æ¸¸å…³è”æˆ–æ—¶é—´ä¸åŒçš„ç›¸ä¼¼äº‹ä»¶ã€‚

ã€æ‹’ç»åˆå¹¶çš„æƒ…å†µç¤ºä¾‹ã€‘
- è¡Œä¸šç›¸ä¼¼ä½†ä¸»ä½“ä¸åŒçš„äº‹ä»¶
- ä¸Šæ¸¸/ä¸‹æ¸¸/ç›‘ç®¡/è”ç›Ÿå…³ç³» â‰  åŒä¸€äº‹ä»¶
- æ—¶é—´é—´éš”æ˜æ˜¾ä¸åŒçš„å¤šæ¬¡äº‹ä»¶
- äº‹ä»¶å…·æœ‰è¿ç»­å‘ç”Ÿæˆ–ä¸€å‰ä¸€åçš„å…³ç³»
- å¯¹äºç›¸åŒå®ä½“ï¼Œä¸åŒæ—¶é—´ç‚¹å‘ç”Ÿçš„äº‹ä»¶

ã€äº‹ä»¶åˆ—è¡¨ã€‘
æ ¼å¼: æ‘˜è¦ | å‚ä¸å®ä½“ | äº‹ä»¶æè¿°
"""
        for abstract, event in events_batch.items():
            entities = event.get('entities', [])
            summary = event.get('event_summary', '')
            prompt += f"{abstract} | {', '.join(entities)} | {summary}\n"

        prompt += """
ã€ä»»åŠ¡ã€‘
æ‰¾å‡ºè¯­ä¹‰ä¸Šé«˜åº¦é‡å ã€æè¿°åŒä¸€äº‹å®çš„äº‹ä»¶ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼š
{
  "duplicate_events": [
    ["äº‹ä»¶æ‘˜è¦1", "äº‹ä»¶æ‘˜è¦2"],
    ["äº‹ä»¶æ‘˜è¦3", "äº‹ä»¶æ‘˜è¦4", "äº‹ä»¶æ‘˜è¦5"]
  ]
}
å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å› { "duplicate_events": [] }ã€‚åªè¾“å‡ºJSONã€‚
"""
        return prompt

    def _parse_entity_response(self, raw_content: str) -> List[List[str]]:
        try:
            data = self._extract_json(raw_content)
            res = data.get("duplicate_entities", [])
            return res if isinstance(res, list) else []
        except Exception:
            return []

    def _parse_event_response(self, raw_content: str) -> List[List[str]]:
        try:
            data = self._extract_json(raw_content)
            res = data.get("duplicate_events", [])
            return res if isinstance(res, list) else []
        except Exception:
            return []

    def _extract_json(self, text: str) -> Dict:
        """ä½¿ç”¨ç»Ÿä¸€çš„JSONæå–å‡½æ•°"""
        return extract_json_from_llm_response(text)

    def update_entities_and_events(self, duplicates: Dict[str, List[List[str]]]):
        """æ ¹æ®é‡å¤æ£€æµ‹ç»“æœæ›´æ–°å®ä½“åº“å’Œäº‹ä»¶åº“"""
        updated = False

        # åˆå¹¶é‡å¤å®ä½“
        for group in duplicates.get("duplicate_entities", []):
            if len(group) < 2:
                continue
            if not self._valid_entity_group(group):
                continue
            primary, dupes = self._choose_primary_entity(group)
            # ç¡®ä¿ä¸»å®ä½“å­˜åœ¨ï¼›è‹¥ä¸å­˜åœ¨ä½†æœ‰é‡å¤å®ä½“å­˜åœ¨ï¼Œå¯äº¤æ¢
            if primary not in self.graph['entities'] and dupes:
                for d in dupes:
                    if d in self.graph['entities']:
                        primary, dupes = d, [x for x in group if x != d]
                        break
            for duplicate in dupes:
                if duplicate in self.graph['entities'] and primary in self.graph['entities']:
                    self._merge_entities(primary, duplicate)
                    updated = True

        # åˆå¹¶é‡å¤äº‹ä»¶
        for group in duplicates.get("duplicate_events", []):
            if len(group) < 2:
                continue
            primary = group[0]
            for duplicate in group[1:]:
                if duplicate in self.graph['events']:
                    self._merge_events(primary, duplicate)
                    updated = True

        if updated:
            self._save_data()
            tools.log("[çŸ¥è¯†å›¾è°±] å®ä½“å’Œäº‹ä»¶æ›´æ–°å®Œæˆ")
        else:
            tools.log("[çŸ¥è¯†å›¾è°±] æ— é‡å¤é¡¹éœ€è¦æ›´æ–°")

    def _merge_entities(self, primary: str, duplicate: str):
        """åˆå¹¶é‡å¤å®ä½“"""
        if primary not in self.graph['entities'] or duplicate not in self.graph['entities']:
            return

        primary_data = self.graph['entities'][primary]
        duplicate_data = self.graph['entities'][duplicate]

        # åˆå¹¶sources (ç¡®ä¿è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„tupleæˆ–ç›´æ¥åˆ—è¡¨å¤„ç†)
        primary_sources = set()
        for s in primary_data.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue # æš‚æ—¶å¿½ç•¥å¤æ‚ç»“æ„
            else: primary_sources.add(s)

        for s in duplicate_data.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)

        # è½¬å›list
        primary_data['sources'] = list(primary_sources)

        # åˆå¹¶original_forms
        primary_forms = set()
        for f in primary_data.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)

        for f in duplicate_data.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)

        # å°†é‡å¤å®ä½“åä¹Ÿä½œä¸ºä¸»å®ä½“çš„å…¶ä»–è¡¨è¿°è®°å½•ï¼Œé˜²æ­¢ä¸¢å¤±åˆ«å
        primary_forms.add(duplicate)
        primary_forms.add(primary)

        primary_data['original_forms'] = list(primary_forms)

        # æ›´æ–°first_seenä¸ºæ›´æ—©çš„æ—¶é—´
        primary_first = primary_data.get('first_seen', '')
        duplicate_first = duplicate_data.get('first_seen', '')
        if duplicate_first and (not primary_first or duplicate_first < primary_first):
            primary_data['first_seen'] = duplicate_first

        # åˆ é™¤é‡å¤å®ä½“
        del self.graph['entities'][duplicate]

        # æ›´æ–°äº‹ä»¶ä¸­çš„å®ä½“å¼•ç”¨
        for abstract, event in self.graph['events'].items():
            entities = event.get('entities', [])
            if duplicate in entities:
                # æ›¿æ¢ä¸ºprimaryï¼Œå¹¶å»é‡
                new_entities = [primary if ent == duplicate else ent for ent in entities]
                # å»é‡
                unique_entities = []
                seen = set()
                for ent in new_entities:
                    if ent not in seen:
                        seen.add(ent)
                        unique_entities.append(ent)
                event['entities'] = unique_entities

            # åŒæ­¥åˆå¹¶ entity_rolesï¼ˆä¸è¦æ±‚ duplicate åœ¨ entities ä¸­ï¼Œé˜²æ­¢å†å²è„æ•°æ®ï¼‰
            roles_map = event.get("entity_roles", {})
            if isinstance(roles_map, dict) and duplicate in roles_map:
                dup_roles = roles_map.get(duplicate, [])
                prim_roles = roles_map.get(primary, [])
                merged = []
                seen_r = set()
                for r in (prim_roles if isinstance(prim_roles, list) else []):
                    if isinstance(r, str) and r and r not in seen_r:
                        seen_r.add(r)
                        merged.append(r)
                for r in (dup_roles if isinstance(dup_roles, list) else []):
                    if isinstance(r, str) and r and r not in seen_r:
                        seen_r.add(r)
                        merged.append(r)
                if merged:
                    roles_map[primary] = merged
                roles_map.pop(duplicate, None)
                event["entity_roles"] = roles_map

            # åŒæ­¥æ›´æ–° relations ä¸»å®¢ä½“
            rels = event.get("relations", [])
            if isinstance(rels, list):
                changed = False
                new_rels = []
                for rel in rels:
                    if not isinstance(rel, dict):
                        continue
                    s = rel.get("subject", "")
                    o = rel.get("object", "")
                    if isinstance(s, str) and s.strip() == duplicate:
                        rel["subject"] = primary
                        changed = True
                    if isinstance(o, str) and o.strip() == duplicate:
                        rel["object"] = primary
                        changed = True
                    new_rels.append(rel)
                if changed:
                    event["relations"] = new_rels

        tools.log(f"[çŸ¥è¯†å›¾è°±] åˆå¹¶å®ä½“: {duplicate} -> {primary}")

    def _merge_events(self, primary: str, duplicate: str):
        """åˆå¹¶é‡å¤äº‹ä»¶"""
        if primary not in self.graph['events'] or duplicate not in self.graph['events']:
            return

        primary_event = self.graph['events'][primary]
        duplicate_event = self.graph['events'][duplicate]

        # åˆå¹¶sources
        primary_sources = set()
        for s in primary_event.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)

        for s in duplicate_event.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)

        primary_event['sources'] = list(primary_sources)

        # åˆå¹¶entities
        primary_entities = set(primary_event.get('entities', []))
        duplicate_entities = set(duplicate_event.get('entities', []))
        primary_event['entities'] = list(primary_entities.union(duplicate_entities))

        # åˆå¹¶ event_typesï¼ˆå»é‡ï¼‰
        def _merge_unique_list(a: Any, b: Any) -> List[str]:
            out: List[str] = []
            seen = set()
            for it in (a if isinstance(a, list) else []):
                if isinstance(it, str) and it and it not in seen:
                    seen.add(it)
                    out.append(it)
            for it in (b if isinstance(b, list) else []):
                if isinstance(it, str) and it and it not in seen:
                    seen.add(it)
                    out.append(it)
            return out

        primary_event["event_types"] = _merge_unique_list(primary_event.get("event_types", []), duplicate_event.get("event_types", []))

        # åˆå¹¶ entity_rolesï¼ˆåŒå®ä½“è§’è‰²å»é‡ï¼‰
        def _merge_roles(a: Any, b: Any) -> Dict[str, List[str]]:
            out: Dict[str, List[str]] = {}
            if isinstance(a, dict):
                for k, v in a.items():
                    if isinstance(k, str) and isinstance(v, list):
                        out[k] = [x for x in v if isinstance(x, str) and x]
            if isinstance(b, dict):
                for k, v in b.items():
                    if not isinstance(k, str):
                        continue
                    roles_list: List[str] = []
                    if isinstance(v, str):
                        roles_list = [v]
                    elif isinstance(v, list):
                        roles_list = [x for x in v if isinstance(x, str)]
                    roles_list = [x.strip() for x in roles_list if isinstance(x, str) and x.strip()]
                    if not roles_list:
                        continue
                    out[k] = _merge_unique_list(out.get(k, []), roles_list)
            return out

        primary_event["entity_roles"] = _merge_roles(primary_event.get("entity_roles", {}), duplicate_event.get("entity_roles", {}))

        # åˆå¹¶ relationsï¼ˆä¸‰å…ƒç»„å»é‡ã€evidence å»é‡ï¼‰
        def _merge_relations(a: Any, b: Any) -> List[Dict[str, Any]]:
            out: Dict[Tuple[str, str, str], Dict[str, Any]] = {}
            def ns(x: Any) -> str:
                return x.strip() if isinstance(x, str) else ""
            def add_one(rel: Any):
                if not isinstance(rel, dict):
                    return
                s = ns(rel.get("subject", ""))
                p = ns(rel.get("predicate", ""))
                o = ns(rel.get("object", ""))
                if not s or not p or not o:
                    return
                key = (s, p, o)
                if key not in out:
                    out[key] = {"subject": s, "predicate": p, "object": o, "evidence": []}
                ev = rel.get("evidence", [])
                ev_list: List[str] = []
                if isinstance(ev, str):
                    ev_list = [ev]
                elif isinstance(ev, list):
                    ev_list = [x for x in ev if isinstance(x, str)]
                ev_list = [x.strip() for x in ev_list if isinstance(x, str) and x.strip()]
                if ev_list:
                    cur = out[key].get("evidence", [])
                    if not isinstance(cur, list):
                        cur = []
                    for x in ev_list:
                        if x not in cur:
                            cur.append(x)
                    out[key]["evidence"] = cur
            for rel in (a if isinstance(a, list) else []):
                add_one(rel)
            for rel in (b if isinstance(b, list) else []):
                add_one(rel)
            return list(out.values())

        primary_event["relations"] = _merge_relations(primary_event.get("relations", []), duplicate_event.get("relations", []))

        # åˆå¹¶ event_start_timeï¼šä¼˜å…ˆæ›´é«˜ç²¾åº¦ï¼Œå…¶æ¬¡æ›´æ—©
        def _choose_better_time(cur_t: Any, cur_txt: Any, cur_p: Any, inc_t: Any, inc_txt: Any, inc_p: Any) -> Tuple[str, str, str]:
            def ns(x: Any) -> str:
                return x.strip() if isinstance(x, str) else ""
            ct, ctxt, cp = ns(cur_t), ns(cur_txt), (ns(cur_p) or "unknown")
            it, itxt, ip = ns(inc_t), ns(inc_txt), (ns(inc_p) or "unknown")
            if not ct and it:
                return it, itxt, ip
            if ct and not it:
                return ct, ctxt, cp
            if not ct and not it:
                return "", "", cp
            rank = {"exact": 3, "date": 2, "relative": 1, "unknown": 0}
            rc, ri = rank.get(cp, 0), rank.get(ip, 0)
            if ri > rc:
                return it, itxt, ip
            if ri < rc:
                return ct, ctxt, cp
            if len(ct) >= 10 and ct[4:5] == "-" and ct[7:8] == "-" and len(it) >= 10 and it[4:5] == "-" and it[7:8] == "-":
                if it < ct:
                    return it, itxt, ip
            return ct, ctxt, cp

        nt, ntxt, np = _choose_better_time(
            primary_event.get("event_start_time", ""),
            primary_event.get("event_start_time_text", ""),
            primary_event.get("event_start_time_precision", "unknown"),
            duplicate_event.get("event_start_time", ""),
            duplicate_event.get("event_start_time_text", ""),
            duplicate_event.get("event_start_time_precision", "unknown"),
        )
        primary_event["event_start_time"] = nt
        primary_event["event_start_time_text"] = ntxt
        primary_event["event_start_time_precision"] = np

        # æ›´æ–°first_seen
        primary_first = primary_event.get('first_seen', '')
        duplicate_first = duplicate_event.get('first_seen', '')
        if duplicate_first and (not primary_first or duplicate_first < primary_first):
            primary_event['first_seen'] = duplicate_first

        # äº‹ä»¶æè¿°åˆå¹¶ï¼šä¿ç•™æ›´è¯¦ç»†çš„
        if not primary_event.get('event_summary') and duplicate_event.get('event_summary'):
            primary_event['event_summary'] = duplicate_event['event_summary']

        # åˆ é™¤é‡å¤äº‹ä»¶
        del self.graph['events'][duplicate]

        tools.log(f"[çŸ¥è¯†å›¾è°±] åˆå¹¶äº‹ä»¶: {duplicate} -> {primary}")

    def _save_data(self):
        """ä¿å­˜æ›´æ–°åçš„æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # ä¿å­˜å®ä½“
            write_json_file(self.entities_file, self.graph['entities'], ensure_ascii=False, indent=2)

            # ä¿å­˜äº‹ä»¶ï¼ˆabstract_mapæ ¼å¼ï¼‰
            abstract_map = {}
            for abstract, event in self.graph['events'].items():
                abstract_map[abstract] = {
                    "event_id": event.get("event_id", ""),
                    "entities": event.get('entities', []),
                    "event_summary": event.get('event_summary', ''),
                    "event_types": event.get("event_types", []),
                    "entity_roles": event.get("entity_roles", {}),
                    "relations": event.get("relations", []),
                    "event_start_time": event.get("event_start_time", ""),
                    "event_start_time_text": event.get("event_start_time_text", ""),
                    "event_start_time_precision": event.get("event_start_time_precision", "unknown"),
                    "reported_at": event.get("reported_at", ""),
                    "sources": event.get('sources', []),
                    "first_seen": event.get('first_seen', '')
                }

            write_json_file(self.abstract_map_file, abstract_map, ensure_ascii=False, indent=2)

            # ä¿å­˜çŸ¥è¯†å›¾è°±çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            write_json_file(self.kg_file, self.graph, ensure_ascii=False, indent=2)

            tools.log("[çŸ¥è¯†å›¾è°±] æ•°æ®ä¿å­˜å®Œæˆ")
        except Exception as e:
            tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")

    def append_only_update(self, events_list: List[Dict[str, Any]], default_source: str = "auto_pipeline", allow_append_original_forms: bool = True) -> Dict[str, int]:
        """
        åªè¿½åŠ æ–°æ•°æ®ï¼Œä¸æ”¹åŠ¨å·²æœ‰å®ä½“/äº‹ä»¶çš„æ—§å­—æ®µã€‚
        - å®ä½“å·²å­˜åœ¨ï¼šä¸æ”¹ first_seen/sourcesï¼Œé»˜è®¤ä»…å¯é€‰åœ°è¿½åŠ åŸå§‹è¡¨è¿°
        - äº‹ä»¶å·²å­˜åœ¨ï¼ˆåŒ abstractï¼‰ï¼šè·³è¿‡ï¼Œä¸æ”¹æ—§äº‹ä»¶
        """
        if not events_list:
            return {"added_entities": 0, "added_events": 0}

        if not self.build_graph():
            return {"added_entities": 0, "added_events": 0}

        # å‡†å¤‡ LLM å»é‡æ˜ å°„ï¼ˆä»…æ˜ å°„"æ–°å®ä½“"åˆ°"å·²æœ‰å®ä½“"ï¼Œä¸æ”¹æ—§å®ä½“å­—æ®µï¼‰
        self._init_llm_pool()
        merge_rules = self.merge_rules or {}
        existing_entities = set(self.graph["entities"].keys())

        # æ”¶é›†æ–°å®ä½“é›†åˆ
        new_entities_set = set()
        for ev in events_list:
            ents = ev.get("entities", []) or []
            ents = [merge_rules.get(e, e) for e in ents if e]
            for e in ents:
                if e not in existing_entities:
                    new_entities_set.add(e)

        # æ„å»ºæ˜ å°„ï¼šæ–°å®ä½“ -> å·²æœ‰å®ä½“ï¼ˆLLM åˆ¤æ–­å¯èƒ½åŒåï¼‰
        llm_merge_map: Dict[str, str] = {}
        if self.llm_pool and new_entities_set:
            # åˆ†æ¡¶é™ä½ä¸Šä¸‹æ–‡é•¿åº¦ï¼šæŒ‰é¦–å­—æ¯/å­—ç¬¦åˆ†æ¡¶
            from collections import defaultdict
            # from concurrent.futures import ThreadPoolExecutor, as_completed  # å·²è¿ç§»åˆ°AsyncExecutor
            buckets = defaultdict(list)
            for ent in new_entities_set:
                prefix = ent[0] if ent else "#"
                buckets[prefix].append(ent)

            llm_lock = threading.Lock()
            max_workers = int(self.settings.get("entity_max_workers", 3)) or 1
            async_executor = AsyncExecutor()

            def handle_bucket(prefix: str, bucket_new: List[str]):
                local_map = {}
                # å–åŒå‰ç¼€çš„éƒ¨åˆ†å·²æœ‰å®ä½“åšå¯¹æ¯”ï¼Œé¿å…è¿‡é•¿
                existing_subset = [e for e in existing_entities if e.startswith(prefix)]
                existing_subset = existing_subset[: max(10, min(80, len(existing_subset)))]
                candidates = list(existing_subset) + list(bucket_new)
                if len(candidates) < 2:
                    return local_map
                try:
                    prompt = self._prepare_entity_compression_prompt_strict(candidates)
                    resp = self._call_llm_limited(prompt, timeout=60, limiter=None)
                    groups = self._parse_entity_response(resp) if resp else []
                    for g in groups:
                        if len(g) < 2:
                            continue
                        primary, dupes = self._choose_primary_entity(g)
                        if primary in existing_entities:
                            for d in dupes:
                                if d in new_entities_set:
                                    local_map[d] = primary
                except Exception as e:
                    tools.log(f"[çŸ¥è¯†å›¾è°±] è¿½åŠ æ¨¡å¼ LLM å»é‡å¤±è´¥: {e}")
                return local_map

            # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†çº¿ç¨‹å¹¶å‘
            bucket_results = async_executor.run_threaded_tasks(
                tasks=[(p, b) for p, b in buckets.items()],
                func=lambda pb: handle_bucket(pb[0], pb[1]),
                max_workers=max_workers
            )

            for res_map in bucket_results:
                if res_map:
                    with llm_lock:
                        llm_merge_map.update(res_map)

        added_entities = 0
        added_events = 0

        def normalize_entity(name: str) -> str:
            if not name:
                return name
            name = merge_rules.get(name, name)
            return llm_merge_map.get(name, name)

        for ev in events_list:
            ents = ev.get("entities", []) or []
            ents = [normalize_entity(e) for e in ents]
            ents_original = ev.get("entities_original") or ents
            src = ev.get("source", default_source)
            published_at = ev.get("published_at") or ev.get("datetime") or ""

            # è¿½åŠ å®ä½“ï¼ˆä»…å½“ä¸å­˜åœ¨ï¼‰
            for ent, ent_orig in zip(ents, ents_original):
                if not ent:
                    continue
                if ent not in self.graph["entities"]:
                    self.graph["entities"][ent] = {
                        "first_seen": published_at or datetime.utcnow().isoformat(),
                        "sources": [src] if src else [],
                        "original_forms": [ent_orig] if ent_orig else []
                    }
                    added_entities += 1
                else:
                    # ä¸æ”¹æ—§å­—æ®µï¼Œä»…å¯é€‰è¿½åŠ åŸå§‹è¡¨è¿°
                    if allow_append_original_forms and ent_orig:
                        forms = self.graph["entities"][ent].get("original_forms", [])
                        if ent_orig not in forms:
                            forms.append(ent_orig)
                            self.graph["entities"][ent]["original_forms"] = forms

            # è¿½åŠ äº‹ä»¶ï¼ˆä»…å½“æ‘˜è¦ä¸å­˜åœ¨ï¼‰
            abstract = ev.get("abstract")
            if abstract and abstract not in self.graph["events"]:
                self.graph["events"][abstract] = {
                    "abstract": abstract,
                    "entities": ents,
                    "event_summary": ev.get("event_summary", ""),
                    "event_types": ev.get("event_types", []),
                    "entity_roles": ev.get("entity_roles", {}),
                    "relations": ev.get("relations", []),
                    "event_start_time": ev.get("event_start_time", ""),
                    "event_start_time_text": ev.get("event_start_time_text", ""),
                    "event_start_time_precision": ev.get("event_start_time_precision", "unknown"),
                    "sources": [src] if src else [],
                    "first_seen": published_at
                }
                added_events += 1

        # é‡å»ºè¾¹å¹¶ä¿å­˜ï¼ˆåªåœ¨æœ‰æ–°å¢æ—¶ï¼‰
        if added_entities or added_events:
            self._build_edges()
            self._save_data()
            tools.log(f"[çŸ¥è¯†å›¾è°±] è¿½åŠ æ¨¡å¼å®Œæˆï¼šæ–°å¢å®ä½“ {added_entities}ï¼Œæ–°å¢äº‹ä»¶ {added_events}")
        else:
            tools.log("[çŸ¥è¯†å›¾è°±] è¿½åŠ æ¨¡å¼ï¼šæ²¡æœ‰æ–°å¢å®ä½“/äº‹ä»¶")

        return {"added_entities": added_entities, "added_events": added_events}

    def refresh_graph(self):
        """åˆ·æ–°çŸ¥è¯†å›¾è°±ï¼šæ„å»ºã€å‹ç¼©ã€æ›´æ–°"""
        tools.log("[çŸ¥è¯†å›¾è°±] å¼€å§‹åˆ·æ–°çŸ¥è¯†å›¾è°±")

        # æ„å»ºå›¾
        if not self.build_graph():
            tools.log("[çŸ¥è¯†å›¾è°±] âŒ æ„å»ºå›¾å¤±è´¥")
            return

        # å‹ç¼©ï¼šä½¿ç”¨LLMæ£€æµ‹é‡å¤
        duplicates = self.compress_with_llm()

        # æ›´æ–°å®ä½“å’Œäº‹ä»¶
        self.update_entities_and_events(duplicates)

        tools.log("[çŸ¥è¯†å›¾è°±] çŸ¥è¯†å›¾è°±åˆ·æ–°å®Œæˆ")
        # æ¸…ç†å·²åŠ è½½çš„ä¸´æ—¶æ–‡ä»¶
        self._cleanup_tmp_files()

