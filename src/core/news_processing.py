"""
å…±äº«çš„æ–°é—»å¤„ç†å·¥å…·å‡½æ•°
æå–agent1å’Œagent2ä¸­çš„é‡å¤é€»è¾‘
"""

import asyncio
from typing import List, Dict, Set, Optional, Tuple, Callable
from pathlib import Path
from datetime import datetime, timezone

from ..utils.llm_utils import AsyncExecutor, RateLimiter
from .logging import LoggerManager


async def process_news_batch_async(
    news_list: List[Dict],
    extract_func: Callable,
    api_pool,
    processed_ids: Set[str],
    limiter: Optional[RateLimiter] = None,
    max_workers: int = 6,
    update_entities_func: Optional[Callable] = None,
    update_abstract_func: Optional[Callable] = None
) -> Tuple[int, Set[str]]:
    """
    å¼‚æ­¥æ‰¹é‡å¤„ç†æ–°é—»
    
    Args:
        news_list: æ–°é—»åˆ—è¡¨
        extract_func: æå–å‡½æ•°ï¼ˆå¦‚llm_extract_eventsï¼‰
        api_pool: LLM APIæ± 
        processed_ids: å·²å¤„ç†çš„IDé›†åˆ
        limiter: é€Ÿç‡é™åˆ¶å™¨
        max_workers: æœ€å¤§å¹¶å‘æ•°
        update_entities_func: å®ä½“æ›´æ–°å‡½æ•°
        update_abstract_func: äº‹ä»¶æ˜ å°„æ›´æ–°å‡½æ•°
        
    Returns:
        (å¤„ç†æˆåŠŸçš„æ•°é‡, æ›´æ–°åçš„å·²å¤„ç†IDé›†åˆ)
    """
    logger = LoggerManager.get_logger(__name__)
    async_executor = AsyncExecutor()
    total_processed = 0
    new_processed_ids = processed_ids.copy()

    async def extract_task_async(
        global_id: str, 
        title: str, 
        content: str, 
        source: str, 
        published_at: Optional[str]
    ) -> Tuple[str, str, Optional[str], List[Dict]]:
        """å•ä¸ªæ–°é—»æå–ä»»åŠ¡"""
        try:
            if limiter:
                await limiter.acquire_async()
            
            loop = asyncio.get_running_loop()
            extracted = await loop.run_in_executor(
                None, 
                extract_func, 
                title, 
                content, 
                api_pool
            )
            return global_id, source, published_at, extracted
        except Exception as e:
            logger.error(f"ä»»åŠ¡ {global_id} æå–å¤±è´¥: {e}")
            return global_id, source, published_at, []

    # æ„å»ºä»»åŠ¡åˆ—è¡¨
    tasks = []
    for news in news_list:
        raw_id = str(news.get("id", "")).strip()
        source = news.get("source", "unknown").strip().lower()

        if not raw_id or not source:
            logger.warning("âš ï¸ è·³è¿‡æ—  ID æˆ–æ—  source çš„æ–°é—»")
            continue

        global_id = f"{source}:{raw_id}"
        if global_id in new_processed_ids:
            continue

        title = news.get("title", "")
        content = news.get("content", "")
        MAX_CONTENT_CHARS = 2000
        if isinstance(content, str) and len(content) > MAX_CONTENT_CHARS:
            content = content[:MAX_CONTENT_CHARS] + "â€¦â€¦ã€åæ–‡å·²æˆªæ–­ã€‘"

        published_at = build_published_at(news.get("timestamp") or news.get("datetime"))

        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks.append(
            lambda gid=global_id, t=title, c=content, s=source, p=published_at: 
            extract_task_async(gid, t, c, s, p)
        )

    if not tasks:
        return 0, new_processed_ids

    logger.info(f"ğŸ”„ å¼€å§‹å¹¶å‘å¤„ç† {len(tasks)} ä¸ªæ–°é—»æå–ä»»åŠ¡")
    
    # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†å¹¶å‘æ‰§è¡Œ
    results = await async_executor.run_concurrent_tasks(
        tasks=tasks,
        concurrency=max_workers
    )

    for result in results:
        try:
            global_id, source, published_at, extracted = result
            if not extracted:
                logger.debug(f"â³ æ–°é—» {global_id}ï¼šLLM æœªè¿”å›æœ‰æ•ˆäº‹ä»¶")
                continue

            all_entities = []
            all_entities_original = []
            for ev in extracted:
                all_entities.extend(ev["entities"])
                all_entities_original.extend(ev.get("entities_original", ev["entities"]))

            if all_entities and len(all_entities) == len(all_entities_original):
                if update_entities_func:
                    update_entities_func(all_entities, all_entities_original, source, published_at)
                if update_abstract_func:
                    update_abstract_func(extracted, source, published_at)
                total_processed += 1
                new_processed_ids.add(global_id)
            else:
                logger.debug(f"ğŸ” æ–°é—» {global_id}ï¼šLLM è¿”å›äº‹ä»¶ä½†æ— æœ‰æ•ˆå®ä½“")
        except Exception as e:
            logger.error(f"âš ï¸ å¤„ç†æå–ç»“æœå¤±è´¥: {e}")

    logger.info(f"âœ… å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡å«æœ‰æ•ˆå®ä½“çš„æ–°é—»")
    return total_processed, new_processed_ids


def build_published_at(ts: Optional[str]) -> Optional[str]:
    """æ„å»ºæ ‡å‡†åŒ–çš„å‘å¸ƒæ—¶é—´å­—ç¬¦ä¸²"""
    if not ts:
        return None
    try:
        if isinstance(ts, datetime):
            return ts.isoformat()
        return ts if isinstance(ts, str) else str(ts)
    except Exception:
        return None


def load_processed_ids(file_path: Path) -> Set[str]:
    """åŠ è½½å·²å¤„ç†çš„IDé›†åˆ"""
    processed_ids = set()
    if file_path.exists():
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            processed_ids = set(line.strip() for line in f if line.strip())
    return processed_ids


def save_processed_id(file_path: Path, global_id: str):
    """ä¿å­˜å·²å¤„ç†çš„ID"""
    with open(file_path, "a", encoding="utf-8") as f:
        f.write(global_id + "\n")

