# src/data/news_collector.py
import aiohttp
import asyncio
import pandas as pd
from datetime import datetime, timedelta, timezone
import time
from typing import Dict, List, Optional, Any
import json
from enum import Enum
import os
import hashlib

os.environ["AIODNS_NO_winloop"] = "1"

import sys

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())


from ..utils.tool_function import tools

tools = tools()

API_POOL = None

def init_api_pool():
    """æƒ°æ€§åˆå§‹åŒ– DataAPIPoolï¼Œé¿å…ä¸ api_client å½¢æˆå¾ªç¯å¯¼å…¥ã€‚"""
    from .api_client import DataAPIPool  # æœ¬åœ°å¯¼å…¥ï¼Œå»¶ååˆ°è¿è¡Œæ—¶

    global API_POOL
    if API_POOL is None:
        API_POOL = DataAPIPool()

def _json_serializer(obj):
    """æ”¯æŒ datetime çš„ JSON åºåˆ—åŒ–è¾…åŠ©å‡½æ•°"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Object of type {type(obj).__name__} is not JSON serializable")

class NewsType(Enum):
    """æ–°é—»ç±»å‹æšä¸¾"""
    FLASH = "flash"  # å¿«è®¯
    ARTICLE = "article"  # æ–‡ç« 
    IMPORTANT = "push"  # é‡è¦æ–°é—»

class NewsCollector:
    def __init__(self):
        pass

    async def data_extract(
        self,
        limit: int = 10,
        category: Optional[str] = None,
        query: Optional[str] = None,
        from_: Optional[str] = None,
        to: Optional[str] = None,
        nullable: Optional[str] = None,
        truncate: Optional[str] = None,
        sortby: Optional[str] = None,
        in_fields: Optional[str] = None,
        page: Optional[int] = None,
    ):
        """
        æŠ“å–é…ç½®ä¸­çš„æ‰€æœ‰æ–°é—»æºï¼ˆå¦‚ GNewsï¼‰ï¼Œç»Ÿä¸€å†™å…¥ raw_news ç›®å½•ã€‚
        é¢å¤–æ”¯æŒ GNews å¯é€‰å‚æ•°ï¼ˆcategory/query/from/to/nullable/truncate/sortby/in/pageï¼‰ã€‚
        
        Args:
            limit: æ¯ä¸ªæ•°æ®æºæŠ“å–çš„æœ€å¤§æ¡æ•°
            category: GNews åˆ†ç±»
            query: å…³é”®è¯ï¼ˆå¦‚æä¾›åˆ™ä¼˜å…ˆä½¿ç”¨ search ç«¯ç‚¹ï¼‰
            from_: ISO8601 èµ·å§‹æ—¶é—´
            to: ISO8601 ç»“æŸæ—¶é—´
            nullable: å…è®¸ä¸º null çš„å­—æ®µï¼Œå¦‚ "description,content"
            truncate: æˆªæ–­å­—æ®µè®¾ç½®ï¼Œå¦‚ "content"
            sortby: æ’åºæ–¹å¼ï¼ˆpublishedAt|relevanceï¼‰
            in_fields: æœç´¢å­—æ®µåˆ—è¡¨ï¼Œå¦‚ "title,description"
            page: é¡µç 
        """
        tools.log(f"[æ•°æ®è·å–] ğŸš€ å¼€å§‹æ‰§è¡Œ NewsCollector.data_extract (limit={limit})")
        init_api_pool()  # åˆå§‹åŒ– DataAPIPool
        if API_POOL is None:
            tools.log("[æ•°æ®è·å–] âŒ API æ± æœªåˆå§‹åŒ–")
            return []

        try:
            sources = API_POOL.list_available_sources()
            tools.log(f"[æ•°æ®è·å–] â„¹ï¸ å¯ç”¨æ•°æ®æº: {sources}")
            if not sources:
                tools.log("[æ•°æ®è·å–] âš ï¸ æœªåœ¨ç¯å¢ƒå˜é‡ DATA_APIS ä¸­é…ç½®ä»»ä½•æ–°é—»æ•°æ®æº")
                return []

            all_dfs: List[pd.DataFrame] = []

            for source_name in sources:
                try:
                    tools.log(f"[æ•°æ®è·å–] ğŸ” å‡†å¤‡è·å–æ¥æº: {source_name}")
                    collector = API_POOL.get_collector(source_name)

                    async def fetch_one(col):
                        async with col:
                            # å¦‚æœæä¾› queryï¼Œä¼˜å…ˆä½¿ç”¨ search ç«¯ç‚¹ï¼›å¦åˆ™ä½¿ç”¨ top-headlines
                            if query:
                                news_list = await col.search(
                                    query=query,
                                    from_=from_,
                                    to=to,
                                    limit=limit,
                                    in_fields=in_fields,
                                    nullable=nullable,
                                    sortby=sortby,
                                    page=page,
                                    truncate=truncate,
                                )
                            else:
                                news_list = await col.get_top_headlines(
                                    category=category,
                                    limit=limit,
                                    nullable=nullable,
                                    from_=from_,
                                    to=to,
                                    query=query,
                                    page=page,
                                    truncate=truncate,
                                )
                            df = col.news_to_dataframe(news_list)
                            return df

                    tools.log(f"[æ•°æ®è·å–] â± å¼‚æ­¥æŠ“å– {source_name} æ–°é—»ä¸­...")
                    df = await fetch_one(collector)
                    if not df.empty:
                        all_dfs.append(df)
                        tools.log(f"[æ•°æ®è·å–] âœ… {source_name} è·å–åˆ° {len(df)} æ¡æ–°é—»")
                    else:
                        tools.log(f"[æ•°æ®è·å–] âš ï¸ {source_name} æœªè·å–åˆ°ä»»ä½•æ–°é—»")
                except Exception as e:
                    tools.log(f"[æ•°æ®è·å–] âŒ æ¥æº {source_name} æŠ“å–å¤±è´¥: {e}")

            if not all_dfs:
                tools.log("[æ•°æ®è·å–] âš ï¸ æ‰€æœ‰æ¥æºå‡æœªè·å–åˆ°æ–°é—»")
                return []

            merged_df = pd.concat(all_dfs, ignore_index=True)

            timestamp = int(time.time())
            # å†™å…¥ tmp/raw_newsï¼Œä¾¿äº Agent1 è¯»å–å¤„ç†
            output_file = tools.RAW_NEWS_TMP_DIR / f"raw_{timestamp}.jsonl"
            with open(output_file, "w", encoding="utf-8") as f:
                for _, row in merged_df.iterrows():
                    f.write(
                        json.dumps(
                            row.to_dict(),
                            default=_json_serializer,
                            ensure_ascii=False,
                        )
                        + "\n"
                    )
            tools.log(
                f"[æ•°æ®è·å–] âœ… å…±ä¿å­˜ {len(merged_df)} æ¡æ–°é—»åˆ° {output_file.name}"
            )

        except Exception as e:
            tools.log(f"[æ•°æ®è·å–] âŒ æŠ“å–å¤±è´¥: {e}")
        
        



class GNewsCollector:
    """
    GNews æ–°é—»æ•°æ®æ”¶é›†å™¨

    æ–‡æ¡£: https://gnews.io/api/v4/{endpoint}?{parameters}&apikey=YOUR_API_KEY
    """

    BASE_URL = "https://gnews.io/api/v4/"

    def __init__(
        self,
        api_key: str,
        language: str = "zh",
        country: Optional[str] = None,
        timeout: int = 30,
    ):
        """
        åˆå§‹åŒ– GNews æ”¶é›†å™¨

        Args:
            api_key: GNews API Key
            language: è¯­è¨€ä»£ç , å¦‚ 'zh', 'en'
            country: å›½å®¶ä»£ç , å¦‚ 'cn', 'us'ï¼›å¯é€‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.api_key = api_key
        self.language = language
        self.country = country
        self.timeout = timeout
        self.session: Optional[aiohttp.ClientSession] = None
        self._connector: Optional[aiohttp.TCPConnector] = None

    async def __aenter__(self):
        if not self.session:
            self._connector = aiohttp.TCPConnector(limit=10)
            self.session = aiohttp.ClientSession(
                connector=self._connector,
                timeout=aiohttp.ClientTimeout(total=self.timeout),
            )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            self.session = None
        if self._connector:
            await self._connector.close()
            self._connector = None

    async def _ensure_session(self):
        if not self.session:
            self._connector = aiohttp.TCPConnector(limit=10)
            self.session = aiohttp.ClientSession(
                connector=self._connector,
                timeout=aiohttp.ClientTimeout(total=self.timeout),
            )

    async def _make_request(self, endpoint: str, params: Dict) -> Dict[str, Any]:
        await self._ensure_session()

        url = f"{self.BASE_URL}{endpoint}"
        # åˆ›å»ºå‚æ•°å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹å‚æ•°
        request_params = dict(params or {})
        
        # ä½¿ç”¨å½“å‰æ”¶é›†å™¨çš„API key
        request_params["apikey"] = self.api_key

        try:
            # è°ƒè¯•ï¼šæ‰“å°æœ¬æ¬¡è¯·æ±‚çš„å…³é”®ä¿¡æ¯ï¼ˆä¸æ‰“å°å®Œæ•´ keyï¼‰
            safe_params = {k: (v if k != "apikey" else "***") for k, v in request_params.items()}
            print(f"[æ•°æ®è·å–][GNews] è¯·æ±‚ {url} å‚æ•°: {safe_params}")
            async with self.session.get(url, params=request_params) as response:
                if response.status != 200:
                    text = await response.text()
                    raise Exception(f"GNews API è¯·æ±‚å¤±è´¥: {response.status} - {text}")
                    
                data = await response.json()
                print(f"[æ•°æ®è·å–][GNews] å“åº”çŠ¶æ€: {response.status}, æ–‡ç« æ•°: {len(data.get('articles', []) if isinstance(data, dict) else [])}")
                return data
        except aiohttp.ClientError as e:
            raise Exception(f"GNews ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        except json.JSONDecodeError as e:
            raise Exception(f"GNews JSON è§£æé”™è¯¯: {e}")

    async def get_top_headlines(
        self,
        category: Optional[str] = None,
        limit: int = 10,
        nullable: Optional[str] = None,
        from_: Optional[str] = None,
        to: Optional[str] = None,
        query: Optional[str] = None,
        page: Optional[int] = None,
        truncate: Optional[str] = None,
    ) -> List[Dict]:
        """
        è·å–å¤´æ¡æ–°é—»ï¼ˆTop Headlines Endpointï¼‰

        å¯¹åº” GNews å‚æ•°:
        - category: åˆ†ç±»ï¼Œå¦‚ general, world, business, technology ç­‰
        - lang:     è¯­è¨€ï¼ˆå·²ç”±å®ä¾‹å±æ€§ language å†³å®šï¼‰
        - country:  å›½å®¶ï¼ˆå·²ç”±å®ä¾‹å±æ€§ country å†³å®šï¼Œå¯é€‰ï¼‰
        - max:      è¿”å›æ¡æ•°ï¼ˆlimitï¼‰
        - nullable: å…è®¸ä¸º null çš„å­—æ®µï¼Œå¦‚ "description,content"
        - from/to:  ISO8601 æ—¶é—´èŒƒå›´
        - q:        å…³é”®å­—ï¼ˆå¯é€‰ï¼‰
        - page:     é¡µç 
        - truncate: å†…å®¹æˆªæ–­è®¾ç½®ï¼Œå¦‚ "content"
        """
        params: Dict[str, Any] = {
            "lang": self.language,
            "max": min(limit, 100),
        }
        if self.country:
            params["country"] = self.country
        if category:
            params["category"] = category
        if nullable:
            params["nullable"] = nullable
        if from_:
            params["from"] = from_
        if to:
            params["to"] = to
        if query:
            params["q"] = query
        if page is not None:
            params["page"] = page
        if truncate:
            params["truncate"] = truncate

        data = await self._make_request("top-headlines", params)
        articles = data.get("articles", []) or []

        for art in articles:
            self._process_timestamp(art)

        return articles[:limit]

    async def search(
        self,
        query: str,
        from_: Optional[str] = None,
        to: Optional[str] = None,
        limit: int = 10,
        in_fields: Optional[str] = None,
        nullable: Optional[str] = None,
        sortby: Optional[str] = None,
        page: Optional[int] = None,
        truncate: Optional[str] = None,
    ) -> List[Dict]:
        """
        ä½¿ç”¨ Search Endpoint æŒ‰å…³é”®å­—æœç´¢æ–°é—»

        å¯¹åº” GNews å‚æ•°:
        - q:       å…³é”®å­—ï¼ˆå¿…å¡«ï¼‰
        - lang:    è¯­è¨€ï¼ˆå·²ç”±å®ä¾‹å±æ€§ language å†³å®šï¼‰
        - country: å›½å®¶ï¼ˆå·²ç”±å®ä¾‹å±æ€§ country å†³å®šï¼Œå¯é€‰ï¼‰
        - max:     è¿”å›æ¡æ•°ï¼ˆlimitï¼‰
        - in:      æœç´¢å­—æ®µï¼Œå¦‚ "title,description"
        - nullable: å…è®¸ä¸º null çš„å­—æ®µï¼Œå¦‚ "description,content"
        - from / to: ISO8601 æ—¶é—´èŒƒå›´
        - sortby:  "publishedAt" | "relevance"
        - page:    é¡µç 
        - truncate: å†…å®¹æˆªæ–­è®¾ç½®ï¼Œå¦‚ "content"
        """
        
        params: Dict[str, Any] = {
            "q": query,
            "lang": self.language,
            "max": min(limit, 100),
        }

        if self.country:
            params["country"] = self.country
        if from_:
            params["from"] = from_
        if to:
            params["to"] = to
        if in_fields:
            params["in"] = in_fields
        if nullable:
            params["nullable"] = nullable
        if sortby:
            params["sortby"] = sortby
        if page is not None:
            params["page"] = page
        if truncate:
            params["truncate"] = truncate

        data = await self._make_request("search", params)
        articles = data.get("articles", []) or []

        for art in articles:
            self._process_timestamp(art)

        return articles[:limit]

    def _process_timestamp(self, article: Dict) -> None:
        """
        å¤„ç† GNews çš„ publishedAt å­—æ®µï¼Œè½¬æ¢ä¸º datetime å’Œæœ¬åœ°æ ¼å¼åŒ–æ—¶é—´
        """
        ts = article.get("publishedAt")
        if not ts:
            article["datetime"] = None
            article["formatted_time"] = "æœªçŸ¥æ—¶é—´"
            return
        try:
            # ä¾‹å¦‚: 2025-12-04T09:30:00Z
            dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
            article["datetime"] = dt
            article["formatted_time"] = dt.astimezone(timezone.utc).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
        except Exception:
            article["datetime"] = None
            article["formatted_time"] = "æœªçŸ¥æ—¶é—´"

    async def get_latest_important_news(self, limit: int = 10) -> List[Dict]:
        """
        è·å–æœ€è¿‘çš„é‡è¦æ–°é—»
        """
        return await self.get_top_headlines(limit=limit)

    def news_to_dataframe(self, news_list: List[Dict]) -> pd.DataFrame:
        """
        å°† GNews æ–‡ç« åˆ—è¡¨è½¬æ¢ä¸ºä¸ Agent1 å…¼å®¹çš„ DataFrame ç»“æ„
        """
        if not news_list:
            return pd.DataFrame()

        processed: List[Dict[str, Any]] = []
        source_name = "gnews"

        for article in news_list:
            url = article.get("url", "")
            title = article.get("title", "") or ""
            content = article.get("content") or article.get("description", "") or ""
            img = article.get("image", "")
            src = article.get("source", {}) or {}
            src_name = src.get("name") or source_name

            processed.append(
                {
                    # ä½¿ç”¨ URL ä½œä¸ºå…¨å±€å”¯ä¸€ IDï¼Œåç»­ Agent1 ä¼šç»„åˆä¸º "gnews:<url>"
                    "id": url or hashlib.md5(title.encode("utf-8")).hexdigest(),
                    "source": src_name,
                    "title": title,
                    "content": content,
                    "type": "article",
                    "link": url,
                    "image_url": img,
                    "create_time": article.get("formatted_time", ""),
                    "timestamp": article.get("datetime"),
                    "is_original": False,
                    "column": src_name,
                    "entities": [],
                    "event_type": None,
                    "raw_json": json.dumps(
                        article, default=_json_serializer, ensure_ascii=False
                    ),
                }
            )

        df = pd.DataFrame(processed)
        if not df.empty and "timestamp" in df.columns:
            df = df.sort_values("timestamp", ascending=False).reset_index(drop=True)
        return df
    
    async def get_news_summary(self, hours: int = 24) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–°é—»æ‘˜è¦
        
        Args:
            hours: æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            æ–°é—»æ‘˜è¦ç»Ÿè®¡
        """
        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(hours=hours)
        
        # è·å–æœ€è¿‘çš„é‡è¦æ–°é—»
        all_news = await self.get_latest_important_news(limit=100)
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„æ–°é—»
        recent_news = []
        for news in all_news:
            news_time = news.get("datetime")
            if news_time and start_time <= news_time <= end_time:
                recent_news.append(news)
        
        # ç»Ÿè®¡ä¿¡æ¯
        flash_count = sum(1 for news in recent_news if "content" in news)
        article_count = len(recent_news) - flash_count
        
        # æå–çƒ­é—¨å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰
        all_titles = " ".join([news.get("title", "") for news in recent_news])
        words = all_titles.split()
        from collections import Counter
        word_freq = Counter(words)
        top_keywords = [word for word, count in word_freq.most_common(10) if len(word) > 1]
        
        return {
            "total_news": len(recent_news),
            "flash_count": flash_count,
            "article_count": article_count,
            "time_range": f"æœ€è¿‘{hours}å°æ—¶",
            "top_keywords": top_keywords[:5],
            "latest_news": recent_news[:10]  # æœ€æ–°10æ¡æ–°é—»
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        pass 

# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•

if __name__ == "__main__":
    pass
    
