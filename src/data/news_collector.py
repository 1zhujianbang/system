# src/data/news_collector.py
import aiohttp
import asyncio
import pandas as pd
from datetime import datetime, timedelta, timezone
import time
from typing import Dict, List, Optional, Any
import json
from enum import Enum
import os
import hashlib

os.environ["AIODNS_NO_winloop"] = "1"

import sys

if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

import aiodns

loop = asyncio.get_event_loop()
resolver = aiodns.DNSResolver(loop=loop)

from ..utils.tool_function import tools

tools = tools()

API_POOL = None

def init_api_pool():
    """æƒ°æ€§åˆå§‹åŒ– DataAPIPoolï¼Œé¿å…ä¸ api_client å½¢æˆå¾ªç¯å¯¼å…¥ã€‚"""
    from .api_client import DataAPIPool  # æœ¬åœ°å¯¼å…¥ï¼Œå»¶ååˆ°è¿è¡Œæ—¶

    global API_POOL
    if API_POOL is None:
        API_POOL = DataAPIPool()

def _json_serializer(obj):
    """æ”¯æŒ datetime çš„ JSON åºåˆ—åŒ–è¾…åŠ©å‡½æ•°"""
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Object of type {type(obj).__name__} is not JSON serializable")

class NewsType(Enum):
    """æ–°é—»ç±»å‹æšä¸¾"""
    FLASH = "flash"  # å¿«è®¯
    ARTICLE = "article"  # æ–‡ç« 
    IMPORTANT = "push"  # é‡è¦æ–°é—»

class Language(Enum):
    """è¯­è¨€ç±»å‹æšä¸¾"""
    CN = "cn"  # ä¸­æ–‡
    EN = "en"  # è‹±æ–‡
    CHT = "cht"  # ç¹ä½“ä¸­æ–‡

class NewsCollector:
    def __init__(self):
        pass

    async def data_extract(self):
        """
        æŠ“å–é…ç½®ä¸­çš„æ‰€æœ‰æ–°é—»æºï¼ˆå¦‚ Blockbeatsã€GNewsï¼‰ï¼Œç»Ÿä¸€å†™å…¥ raw_news ç›®å½•ã€‚
        """
        tools.log("[æ•°æ®è·å–] ğŸš€ å¼€å§‹æ‰§è¡Œ NewsCollector.data_extract")
        init_api_pool()  # åˆå§‹åŒ– DataAPIPool
        if API_POOL is None:
            tools.log("[æ•°æ®è·å–] âŒ API æ± æœªåˆå§‹åŒ–")
            return []

        try:
            sources = API_POOL.list_available_sources()
            tools.log(f"[æ•°æ®è·å–] â„¹ï¸ å¯ç”¨æ•°æ®æº: {sources}")
            if not sources:
                tools.log("[æ•°æ®è·å–] âš ï¸ æœªåœ¨ç¯å¢ƒå˜é‡ DATA_APIS ä¸­é…ç½®ä»»ä½•æ–°é—»æ•°æ®æº")
                return []

            all_dfs: List[pd.DataFrame] = []

            for source_name in sources:
                try:
                    tools.log(f"[æ•°æ®è·å–] ğŸ” å‡†å¤‡è·å–æ¥æº: {source_name}")
                    collector = API_POOL.get_collector(source_name)

                    async def fetch_one(col):
                        async with col:
                            # çº¦å®šï¼šæ‰€æœ‰ collector éƒ½å®ç° get_latest_important_news + news_to_dataframe
                            news_list = await col.get_latest_important_news(limit=50)
                            df = col.news_to_dataframe(news_list)
                            return df

                    tools.log(f"[æ•°æ®è·å–] â± å¼‚æ­¥æŠ“å– {source_name} æ–°é—»ä¸­...")
                    df = await fetch_one(collector)
                    if not df.empty:
                        all_dfs.append(df)
                        tools.log(f"[æ•°æ®è·å–] âœ… {source_name} è·å–åˆ° {len(df)} æ¡æ–°é—»")
                    else:
                        tools.log(f"[æ•°æ®è·å–] âš ï¸ {source_name} æœªè·å–åˆ°ä»»ä½•æ–°é—»")
                except Exception as e:
                    tools.log(f"[æ•°æ®è·å–] âŒ æ¥æº {source_name} æŠ“å–å¤±è´¥: {e}")

            if not all_dfs:
                tools.log("[æ•°æ®è·å–] âš ï¸ æ‰€æœ‰æ¥æºå‡æœªè·å–åˆ°æ–°é—»")
                return []

            merged_df = pd.concat(all_dfs, ignore_index=True)

            timestamp = int(time.time())
            output_file = tools.RAW_NEWS_DIR / f"raw_{timestamp}.jsonl"
            with open(output_file, "w", encoding="utf-8") as f:
                for _, row in merged_df.iterrows():
                    f.write(
                        json.dumps(
                            row.to_dict(),
                            default=_json_serializer,
                            ensure_ascii=False,
                        )
                        + "\n"
                    )
            tools.log(
                f"[æ•°æ®è·å–] âœ… å…±ä¿å­˜ {len(merged_df)} æ¡æ–°é—»åˆ° {output_file.name}"
            )

        except Exception as e:
            tools.log(f"[æ•°æ®è·å–] âŒ æŠ“å–å¤±è´¥: {e}")
        
        


class BlockbeatsNewsCollector:
    """Blockbeatsæ–°é—»æ•°æ®æ”¶é›†å™¨"""
    
    BASE_URL = "https://api.theblockbeats.news/v1/"
    
    def __init__(self, language: Language = Language.CN, timeout: int = 30):
        """
        åˆå§‹åŒ–æ–°é—»æ”¶é›†å™¨
        
            language: è¯­è¨€ç±»å‹
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.language = language
        self.timeout = timeout
        self.session = None
        self.cache = {}  # ç®€å•çš„å†…å­˜ç¼“å­˜
        self.cache_ttl = 300  # ç¼“å­˜æœ‰æ•ˆæœŸ5åˆ†é’Ÿ
        self._connector = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        if not self.session:
            self._connector = aiohttp.TCPConnector(limit=10)
            self.session = aiohttp.ClientSession(
                connector=self._connector,
                timeout=aiohttp.ClientTimeout(total=self.timeout)
            )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        await self.session.close()
    
    async def close(self):
        """æ˜¾å¼å…³é—­è¿æ¥"""
        if self.session:
            await self.session.close()
            self.session = None
        if self._connector:
            await self._connector.close()
            self._connector = None

    async def ensure_session(self):
        """ç¡®ä¿ä¼šè¯å­˜åœ¨"""
        if not self.session:
            self._connector = aiohttp.TCPConnector(limit=10)
            self.session = aiohttp.ClientSession(
                connector=self._connector,
                timeout=aiohttp.ClientTimeout(total=self.timeout)
            )

    def _get_cache_key(self, endpoint: str, params: Dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{endpoint}:{json.dumps(params, sort_keys=True)}"
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_key in self.cache:
            cached_time, _ = self.cache[cache_key]
            return (time.time() - cached_time) < self.cache_ttl
        return False
    
    async def _make_request(self, endpoint: str, params: Dict) -> Dict[str, Any]:
        """å‘é€APIè¯·æ±‚"""
        await self.ensure_session()  # ç¡®ä¿ä¼šè¯å­˜åœ¨

        cache_key = self._get_cache_key(endpoint, params)
        
        # æ£€æŸ¥ç¼“å­˜
        if self._is_cache_valid(cache_key):
            _, cached_data = self.cache[cache_key]
            print(f"ğŸ“° ä½¿ç”¨ç¼“å­˜æ•°æ®: {endpoint}")
            return cached_data
        
        if not self.session:
            self.session = aiohttp.ClientSession()
        
        url = f"{self.BASE_URL}{endpoint}"
        
        try:
            async with self.session.get(url, params=params) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.status} - {error_text}")
                
                result = await response.json()
                
                # ç¼“å­˜ç»“æœ
                self.cache[cache_key] = (time.time(), result)
                
                return result
                
        except aiohttp.ClientError as e:
            raise Exception(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}")
        except json.JSONDecodeError as e:
            raise Exception(f"JSONè§£æé”™è¯¯: {str(e)}")
    
    async def get_flash_news(self, 
                           page: int = 1, 
                           size: int = 10,
                           news_type: NewsType = NewsType.IMPORTANT) -> List[Dict]:
        """
        è·å–å¿«è®¯æ–°é—»
        
        Args:
            page: é¡µç 
            size: æ¯é¡µæ•°é‡
            news_type: æ–°é—»ç±»å‹
            
        Returns:
            å¿«è®¯æ–°é—»åˆ—è¡¨
        """
        endpoint = "open-api/open-flash"
        params = {
            "page": page,
            "size": size,
            "type": news_type.value,
            "lang": self.language.value
        }
        
        try:
            result = await self._make_request(endpoint, params)
            
            if result.get("status") == 0:
                data = result.get("data", {})
                news_list = data.get("data", [])
                
                # å¤„ç†æ—¶é—´æˆ³
                for news in news_list:
                    news = self._process_news_timestamp(news)
                
                print(f"âœ… è·å–åˆ° {len(news_list)} æ¡å¿«è®¯æ–°é—»")
                return news_list
            else:
                error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                raise Exception(f"APIè¿”å›é”™è¯¯: {error_msg}")
                
        except Exception as e:
            print(f"âŒ è·å–å¿«è®¯æ–°é—»å¤±è´¥: {str(e)}")
            return []
    
    async def get_articles(self, 
                         page: int = 1, 
                         size: int = 10,
                         news_type: NewsType = NewsType.IMPORTANT) -> List[Dict]:
        """
        è·å–æ–‡ç« 
        
        Args:
            page: é¡µç 
            size: æ¯é¡µæ•°é‡
            news_type: æ–°é—»ç±»å‹
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        endpoint = "open-api/open-information"
        params = {
            "page": page,
            "size": size,
            "type": news_type.value,
            "lang": self.language.value
        }
        
        try:
            result = await self._make_request(endpoint, params)
            
            if result.get("status") == 0:
                data = result.get("data", {})
                articles = data.get("data", [])
                
                # å¤„ç†æ—¶é—´æˆ³
                for article in articles:
                    article = self._process_news_timestamp(article)
                
                print(f"âœ… è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
                return articles
            else:
                error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                raise Exception(f"APIè¿”å›é”™è¯¯: {error_msg}")
                
        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« å¤±è´¥: {str(e)}")
            return []
    
    def _process_news_timestamp(self, news_item: Dict) -> Dict:
        """å¤„ç†æ–°é—»æ—¶é—´æˆ³"""
        create_time = news_item.get("create_time")
        if create_time:
            try:
                # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºdatetimeå¯¹è±¡
                timestamp = int(create_time)
                dt = datetime.fromtimestamp(timestamp, tz=timezone.utc)
                news_item["datetime"] = dt
                news_item["formatted_time"] = dt.strftime("%Y-%m-%d %H:%M:%S")
            except (ValueError, TypeError):
                news_item["datetime"] = None
                news_item["formatted_time"] = "æœªçŸ¥æ—¶é—´"
        
        return news_item
    
    async def get_latest_important_news(self, limit: int = 20) -> List[Dict]:
        """
        è·å–æœ€æ–°çš„é‡è¦æ–°é—»ï¼ˆå¿«è®¯+æ–‡ç« ï¼‰
        
        Args:
            limit: æ€»æ•°é‡é™åˆ¶
            
        Returns:
            åˆå¹¶çš„é‡è¦æ–°é—»åˆ—è¡¨
        """
        # è·å–å¿«è®¯å’Œæ–‡ç« 
        flash_news = await self.get_flash_news(page=1, size=limit//2, news_type=NewsType.IMPORTANT)
        articles = await self.get_articles(page=1, size=limit//2, news_type=NewsType.IMPORTANT)
        
        # åˆå¹¶å¹¶æ’åº
        all_news = flash_news + articles
        all_news.sort(key=lambda x: x.get("datetime") or datetime.min.replace(tzinfo=timezone.utc), reverse=True)
        
        # é™åˆ¶æ•°é‡
        return all_news[:limit]
    
    async def search_news_by_keyword(self, 
                                   keyword: str, 
                                   news_type: NewsType = None,
                                   limit: int = 50) -> List[Dict]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢æ–°é—»ï¼ˆé€šè¿‡è·å–å¤šé¡µæ•°æ®å®ç°ç®€å•æœç´¢ï¼‰
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            news_type: æ–°é—»ç±»å‹ï¼ŒNoneè¡¨ç¤ºæœç´¢æ‰€æœ‰ç±»å‹
            limit: æœ€å¤§ç»“æœæ•°é‡
            
        Returns:
            åŒ…å«å…³é”®è¯çš„æ–°é—»åˆ—è¡¨
        """
        all_results = []
        page = 1
        page_size = 20
        
        while len(all_results) < limit:
            try:
                # è·å–å¿«è®¯
                if news_type is None or news_type == NewsType.FLASH:
                    flash_news = await self.get_flash_news(page=page, size=page_size)
                    all_results.extend(flash_news)
                
                # è·å–æ–‡ç« 
                if news_type is None or news_type == NewsType.ARTICLE:
                    articles = await self.get_articles(page=page, size=page_size)
                    all_results.extend(articles)
                
                # å¦‚æœæ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œåœæ­¢æœç´¢
                if not flash_news and not articles:
                    break
                
                page += 1
                # é¿å…è¯·æ±‚è¿‡å¿«
                await asyncio.sleep(0.5)
                
            except Exception as e:
                print(f"âŒ æœç´¢æ–°é—»æ—¶å‡ºé”™: {str(e)}")
                break
        
        # è¿‡æ»¤åŒ…å«å…³é”®è¯çš„æ–°é—»
        filtered_results = []
        for news in all_results:
            title = news.get("title", "").lower()
            content = news.get("content", "").lower()
            description = news.get("description", "").lower()
            
            if (keyword.lower() in title or 
                keyword.lower() in content or 
                keyword.lower() in description):
                filtered_results.append(news)
        
        # æŒ‰æ—¶é—´æ’åº
        filtered_results.sort(key=lambda x: x.get("datetime") or datetime.min.replace(tzinfo=timezone.utc), reverse=True)
        
        return filtered_results[:limit]
    
    def news_to_dataframe(self, news_list: List[Dict]) -> pd.DataFrame:
        """
        å°†æ–°é—»åˆ—è¡¨è½¬æ¢ä¸ºDataFrame
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            
        Returns:
            DataFrameæ ¼å¼çš„æ–°é—»æ•°æ®
        """
        if not news_list:
            return pd.DataFrame()
        
        # æå–å…³é”®å­—æ®µ
        processed_news = []
        source_name = "blockbeats"
        for news in news_list:
            processed_news.append({
                "id": news.get("id"),
                "source": source_name,
                "title": news.get("title", ""),
                "content": news.get("content", news.get("description", "")),
                "type": "flash" if "content" in news else "article",
                "link": news.get("link", ""),
                "image_url": news.get("pic", ""),
                "create_time": news.get("formatted_time", ""),
                "timestamp": news.get("datetime"),
                "is_original": news.get("is_original", False),
                "column": news.get("column", ""),
                # === æ–°å¢å­—æ®µï¼šç”¨äºçŸ¥è¯†å›¾è°±æ„å»º ===
                "entities": [],          # é¢„ç•™ï¼šç”±æ™ºèƒ½ä½“1å¡«å……å®ä½“åˆ—è¡¨ï¼Œå¦‚ ["BTC", "ä»¥å¤ªåŠ"]
                "event_type": None,      # é¢„ç•™ï¼šäº‹ä»¶ç±»å‹ï¼Œå¦‚ "regulation", "hack"
                "raw_json": json.dumps(news, default=_json_serializer, ensure_ascii=False)  # é¢„ç•™ï¼šåŸå§‹æ•°æ®å›æº¯
            })
        
        df = pd.DataFrame(processed_news)
        if not df.empty and "timestamp" in df.columns:
            df = df.sort_values("timestamp", ascending=False)
            df = df.reset_index(drop=True)
        
        return df


class GNewsCollector:
    """
    GNews æ–°é—»æ•°æ®æ”¶é›†å™¨

    æ–‡æ¡£: https://gnews.io/api/v4/{endpoint}?{parameters}&apikey=YOUR_API_KEY
    """

    BASE_URL = "https://gnews.io/api/v4/"

    def __init__(
        self,
        api_key: str,
        language: str = "zh",
        country: Optional[str] = None,
        timeout: int = 30,
    ):
        """
        åˆå§‹åŒ– GNews æ”¶é›†å™¨

        Args:
            api_key: GNews API Key
            language: è¯­è¨€ä»£ç , å¦‚ 'zh', 'en'
            country: å›½å®¶ä»£ç , å¦‚ 'cn', 'us'ï¼›å¯é€‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.api_key = api_key
        self.language = language
        self.country = country
        self.timeout = timeout
        self.session: Optional[aiohttp.ClientSession] = None
        self._connector: Optional[aiohttp.TCPConnector] = None

    async def __aenter__(self):
        if not self.session:
            self._connector = aiohttp.TCPConnector(limit=10)
            self.session = aiohttp.ClientSession(
                connector=self._connector,
                timeout=aiohttp.ClientTimeout(total=self.timeout),
            )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            self.session = None
        if self._connector:
            await self._connector.close()
            self._connector = None

    async def _ensure_session(self):
        if not self.session:
            self._connector = aiohttp.TCPConnector(limit=10)
            self.session = aiohttp.ClientSession(
                connector=self._connector,
                timeout=aiohttp.ClientTimeout(total=self.timeout),
            )

    async def _make_request(self, endpoint: str, params: Dict) -> Dict[str, Any]:
        await self._ensure_session()

        url = f"{self.BASE_URL}{endpoint}"
        # åˆ›å»ºå‚æ•°å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹å‚æ•°
        request_params = dict(params or {})
        
        # ä½¿ç”¨å½“å‰æ”¶é›†å™¨çš„API key
        request_params["apikey"] = self.api_key

        try:
            # è°ƒè¯•ï¼šæ‰“å°æœ¬æ¬¡è¯·æ±‚çš„å…³é”®ä¿¡æ¯ï¼ˆä¸æ‰“å°å®Œæ•´ keyï¼‰
            safe_params = {k: (v if k != "apikey" else "***") for k, v in request_params.items()}
            print(f"[æ•°æ®è·å–][GNews] è¯·æ±‚ {url} å‚æ•°: {safe_params}")
            async with self.session.get(url, params=request_params) as response:
                if response.status != 200:
                    text = await response.text()
                    raise Exception(f"GNews API è¯·æ±‚å¤±è´¥: {response.status} - {text}")
                    
                data = await response.json()
                print(f"[æ•°æ®è·å–][GNews] å“åº”çŠ¶æ€: {response.status}, æ–‡ç« æ•°: {len(data.get('articles', []) if isinstance(data, dict) else [])}")
                return data
        except aiohttp.ClientError as e:
            raise Exception(f"GNews ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        except json.JSONDecodeError as e:
            raise Exception(f"GNews JSON è§£æé”™è¯¯: {e}")

    async def get_top_headlines(
        self,
        category: Optional[str] = None,
        limit: int = 50,
        nullable: Optional[str] = None,
        from_: Optional[str] = None,
        to: Optional[str] = None,
        query: Optional[str] = None,
        page: Optional[int] = None,
        truncate: Optional[str] = None,
    ) -> List[Dict]:
        """
        è·å–å¤´æ¡æ–°é—»ï¼ˆTop Headlines Endpointï¼‰

        å¯¹åº” GNews å‚æ•°:
        - category: åˆ†ç±»ï¼Œå¦‚ general, world, business, technology ç­‰
        - lang:     è¯­è¨€ï¼ˆå·²ç”±å®ä¾‹å±æ€§ language å†³å®šï¼‰
        - country:  å›½å®¶ï¼ˆå·²ç”±å®ä¾‹å±æ€§ country å†³å®šï¼Œå¯é€‰ï¼‰
        - max:      è¿”å›æ¡æ•°ï¼ˆlimitï¼‰
        - nullable: å…è®¸ä¸º null çš„å­—æ®µï¼Œå¦‚ "description,content"
        - from/to:  ISO8601 æ—¶é—´èŒƒå›´
        - q:        å…³é”®å­—ï¼ˆå¯é€‰ï¼‰
        - page:     é¡µç 
        - truncate: å†…å®¹æˆªæ–­è®¾ç½®ï¼Œå¦‚ "content"
        """
        params: Dict[str, Any] = {
            "lang": self.language,
            "max": min(limit, 100),
        }
        if self.country:
            params["country"] = self.country
        if category:
            params["category"] = category
        if nullable:
            params["nullable"] = nullable
        if from_:
            params["from"] = from_
        if to:
            params["to"] = to
        if query:
            params["q"] = query
        if page is not None:
            params["page"] = page
        if truncate:
            params["truncate"] = truncate

        data = await self._make_request("top-headlines", params)
        articles = data.get("articles", []) or []

        for art in articles:
            self._process_timestamp(art)

        return articles[:limit]

    async def search(
        self,
        query: str,
        from_: Optional[str] = None,
        to: Optional[str] = None,
        limit: int = 50,
        in_fields: Optional[str] = None,
        nullable: Optional[str] = None,
        sortby: Optional[str] = None,
        page: Optional[int] = None,
        truncate: Optional[str] = None,
    ) -> List[Dict]:
        """
        ä½¿ç”¨ Search Endpoint æŒ‰å…³é”®å­—æœç´¢æ–°é—»

        å¯¹åº” GNews å‚æ•°:
        - q:       å…³é”®å­—ï¼ˆå¿…å¡«ï¼‰
        - lang:    è¯­è¨€ï¼ˆå·²ç”±å®ä¾‹å±æ€§ language å†³å®šï¼‰
        - country: å›½å®¶ï¼ˆå·²ç”±å®ä¾‹å±æ€§ country å†³å®šï¼Œå¯é€‰ï¼‰
        - max:     è¿”å›æ¡æ•°ï¼ˆlimitï¼‰
        - in:      æœç´¢å­—æ®µï¼Œå¦‚ "title,description"
        - nullable: å…è®¸ä¸º null çš„å­—æ®µï¼Œå¦‚ "description,content"
        - from / to: ISO8601 æ—¶é—´èŒƒå›´
        - sortby:  "publishedAt" | "relevance"
        - page:    é¡µç 
        - truncate: å†…å®¹æˆªæ–­è®¾ç½®ï¼Œå¦‚ "content"
        """
        
        params: Dict[str, Any] = {
            "q": query,
            "lang": self.language,
            "max": min(limit, 100),
        }

        if self.country:
            params["country"] = self.country
        if from_:
            params["from"] = from_
        if to:
            params["to"] = to
        if in_fields:
            params["in"] = in_fields
        if nullable:
            params["nullable"] = nullable
        if sortby:
            params["sortby"] = sortby
        if page is not None:
            params["page"] = page
        if truncate:
            params["truncate"] = truncate

        data = await self._make_request("search", params)
        articles = data.get("articles", []) or []

        for art in articles:
            self._process_timestamp(art)

        return articles[:limit]

    def _process_timestamp(self, article: Dict) -> None:
        """
        å¤„ç† GNews çš„ publishedAt å­—æ®µï¼Œè½¬æ¢ä¸º datetime å’Œæœ¬åœ°æ ¼å¼åŒ–æ—¶é—´
        """
        ts = article.get("publishedAt")
        if not ts:
            article["datetime"] = None
            article["formatted_time"] = "æœªçŸ¥æ—¶é—´"
            return
        try:
            # ä¾‹å¦‚: 2025-12-04T09:30:00Z
            dt = datetime.fromisoformat(ts.replace("Z", "+00:00"))
            article["datetime"] = dt
            article["formatted_time"] = dt.astimezone(timezone.utc).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
        except Exception:
            article["datetime"] = None
            article["formatted_time"] = "æœªçŸ¥æ—¶é—´"

    async def get_latest_important_news(self, limit: int = 50) -> List[Dict]:
        """
        ä¸ºäº†ä¸ BlockbeatsNewsCollector æ¥å£ç»Ÿä¸€ï¼Œè¿™é‡Œä½¿ç”¨ Top Headlines ä½œä¸ºâ€œé‡è¦æ–°é—»â€
        """
        return await self.get_top_headlines(limit=limit)

    def news_to_dataframe(self, news_list: List[Dict]) -> pd.DataFrame:
        """
        å°† GNews æ–‡ç« åˆ—è¡¨è½¬æ¢ä¸ºä¸ Agent1 å…¼å®¹çš„ DataFrame ç»“æ„
        """
        if not news_list:
            return pd.DataFrame()

        processed: List[Dict[str, Any]] = []
        source_name = "gnews"

        for article in news_list:
            url = article.get("url", "")
            title = article.get("title", "") or ""
            content = article.get("content") or article.get("description", "") or ""
            img = article.get("image", "")
            src = article.get("source", {}) or {}
            src_name = src.get("name") or source_name

            processed.append(
                {
                    # ä½¿ç”¨ URL ä½œä¸ºå…¨å±€å”¯ä¸€ IDï¼Œåç»­ Agent1 ä¼šç»„åˆä¸º "gnews:<url>"
                    "id": url or hashlib.md5(title.encode("utf-8")).hexdigest(),
                    "source": src_name,
                    "title": title,
                    "content": content,
                    "type": "article",
                    "link": url,
                    "image_url": img,
                    "create_time": article.get("formatted_time", ""),
                    "timestamp": article.get("datetime"),
                    "is_original": False,
                    "column": src_name,
                    "entities": [],
                    "event_type": None,
                    "raw_json": json.dumps(
                        article, default=_json_serializer, ensure_ascii=False
                    ),
                }
            )

        df = pd.DataFrame(processed)
        if not df.empty and "timestamp" in df.columns:
            df = df.sort_values("timestamp", ascending=False).reset_index(drop=True)
        return df
    
    async def get_news_summary(self, hours: int = 24) -> Dict[str, Any]:
        """
        è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–°é—»æ‘˜è¦
        
        Args:
            hours: æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            æ–°é—»æ‘˜è¦ç»Ÿè®¡
        """
        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(hours=hours)
        
        # è·å–æœ€è¿‘çš„é‡è¦æ–°é—»
        all_news = await self.get_latest_important_news(limit=100)
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´å†…çš„æ–°é—»
        recent_news = []
        for news in all_news:
            news_time = news.get("datetime")
            if news_time and start_time <= news_time <= end_time:
                recent_news.append(news)
        
        # ç»Ÿè®¡ä¿¡æ¯
        flash_count = sum(1 for news in recent_news if "content" in news)
        article_count = len(recent_news) - flash_count
        
        # æå–çƒ­é—¨å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰
        all_titles = " ".join([news.get("title", "") for news in recent_news])
        words = all_titles.split()
        from collections import Counter
        word_freq = Counter(words)
        top_keywords = [word for word, count in word_freq.most_common(10) if len(word) > 1]
        
        return {
            "total_news": len(recent_news),
            "flash_count": flash_count,
            "article_count": article_count,
            "time_range": f"æœ€è¿‘{hours}å°æ—¶",
            "top_keywords": top_keywords[:5],
            "latest_news": recent_news[:10]  # æœ€æ–°10æ¡æ–°é—»
        }
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        print("âœ… æ–°é—»ç¼“å­˜å·²æ¸…ç©º")


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def news_collector_demo():
    """æ–°é—»æ”¶é›†å™¨ä½¿ç”¨ç¤ºä¾‹"""
    
    async with BlockbeatsNewsCollector(language=Language.CN) as collector:
        print("ğŸš€ Blockbeatsæ–°é—»æ”¶é›†å™¨æ¼”ç¤º")
        print("=" * 50)
        
        # 1. è·å–å¿«è®¯æ–°é—»
        print("\n1. è·å–é‡è¦å¿«è®¯:")
        flash_news = await collector.get_flash_news(page=1, size=5, news_type=NewsType.IMPORTANT)
        for i, news in enumerate(flash_news, 1):
            print(f"   {i}. {news.get('title')} [{news.get('formatted_time')}]")
        
        # 2. è·å–æ–‡ç« 
        print("\n2. è·å–é‡è¦æ–‡ç« :")
        articles = await collector.get_articles(page=1, size=3, news_type=NewsType.IMPORTANT)
        for i, article in enumerate(articles, 1):
            print(f"   {i}. {article.get('title')} [{article.get('formatted_time')}]")
        
        # 3. è·å–æœ€æ–°é‡è¦æ–°é—»
        print("\n3. æœ€æ–°é‡è¦æ–°é—»:")
        important_news = await collector.get_latest_important_news(limit=5)
        for i, news in enumerate(important_news, 1):
            news_type = "å¿«è®¯" if "content" in news else "æ–‡ç« "
            print(f"   {i}. [{news_type}] {news.get('title')}")
        
        # 4. æœç´¢æ–°é—»
        print("\n4. æœç´¢'BTC'ç›¸å…³æ–°é—»:")
        btc_news = await collector.search_news_by_keyword("BTC", limit=3)
        for i, news in enumerate(btc_news, 1):
            print(f"   {i}. {news.get('title')}")
        
        # 5. è·å–æ–°é—»æ‘˜è¦
        print("\n5. 24å°æ—¶æ–°é—»æ‘˜è¦:")
        summary = await collector.get_news_summary(hours=24)
        print(f"   æ€»æ–°é—»æ•°: {summary['total_news']}")
        print(f"   å¿«è®¯æ•°: {summary['flash_count']}")
        print(f"   æ–‡ç« æ•°: {summary['article_count']}")
        print(f"   çƒ­é—¨å…³é”®è¯: {', '.join(summary['top_keywords'])}")
        
        # 6. è½¬æ¢ä¸ºDataFrame
        print("\n6. è½¬æ¢ä¸ºDataFrame:")
        df = collector.news_to_dataframe(important_news)
        if not df.empty:
            print(f"   DataFrameå½¢çŠ¶: {df.shape}")
            print(f"   åˆ—å: {list(df.columns)}")
            print(f"   å‰3æ¡æ–°é—»æ ‡é¢˜:")
            for title in df['title'].head(3):
                print(f"     - {title}")

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(news_collector_demo())