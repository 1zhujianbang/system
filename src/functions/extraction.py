from typing import List, Dict, Any, Optional, Set, Tuple
from ..core.registry import register_tool
from ..core import ConfigManager, RateLimiter, LLMAPIPool, AsyncExecutor, tools
from ..utils.data_utils import update_entities, update_abstract_map
from ..utils.json_utils import extract_json_from_llm_response
from ..utils.llm_utils import call_llm_with_retry, create_extraction_prompt
from ..utils.file_utils import ensure_dirs, safe_unlink, generate_timestamp
from ..utils.data_utils import write_jsonl_file, sanitize_datetime_fields, create_temp_file_path
import json
from pathlib import Path
import time
import threading
import asyncio
import datetime
from datetime import datetime as dt, timezone, timedelta
from collections import defaultdict

class NewsDeduplicator:
    """æ–°é—»å»é‡å™¨ï¼Œæ”¯æŒä¾èµ–æ³¨å…¥"""

    def __init__(self, threshold: int = 3):
        self.threshold = threshold
        self.seen_hashes: Set[int] = set()

    @staticmethod
    def _news_key(news: Dict) -> str:
        """æ„é€ ç”¨äºå»é‡çš„å”¯ä¸€é”®ï¼ŒåŒ…å« source å‰ç¼€ï¼Œå…¼å®¹å¤šæ•°æ®æºã€‚"""
        return f"{news.get('source', 'unknown')}:{news.get('id')}"

    def is_duplicate(self, text: str) -> bool:
        h = tools.simhash(text)
        for seen_h in self.seen_hashes:
            if tools.hamming_distance(h, seen_h) <= self.threshold:
                return True
        self.seen_hashes.add(h)
        return False

    def dedupe_file(self, input_path: Path, output_path: Path, processed_ids: Optional[Set[str]] = None):
        """
        å¯¹å•ä¸ªåŸå§‹æ–‡ä»¶åšå»é‡ï¼š
        - å…ˆç”¨ processed_idsï¼ˆå…¨å±€å·²å¤„ç† IDï¼Œå¦‚ blockbeats:323066ï¼‰è¿‡æ»¤å†å²å·²å¤„ç†æ–°é—»
        - å†ç»“åˆå·²æœ‰å»é‡æ–‡ä»¶ & simhash å»æ‰æœ¬æ‰¹å†…/è·¨æ‰¹çš„é‡å¤å†…å®¹
        """
        tools.log(f"ğŸ” å»é‡ä¸­: {input_path.name}")

        # å…ˆåŠ è½½"å…¨å±€å·²å¤„ç† ID"ï¼Œé¿å…è€æ–°é—»å†æ¬¡è¿›å…¥å»é‡ç»“æœ
        seen_ids: Set[str] = set(processed_ids or set())
        if processed_ids:
            tools.log(f"ğŸ” å·²æœ‰å†å² processed_ids æ•°é‡: {len(processed_ids)}")

        # å†åŠ è½½å·²æœ‰å»é‡ç»“æœæ–‡ä»¶ä¸­çš„ IDï¼Œå®ç°è·¨æ‰¹æ¬¡çš„æœ¬åœ°å»é‡
        if output_path.exists():
            with open(output_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        item = json.loads(line)
                        seen_ids.add(self._news_key(item))
                    except Exception as e:
                        tools.log(f"âš ï¸ è¯»å–å†å²å»é‡æ–‡ä»¶æ—¶è·³è¿‡æ— æ•ˆè¡Œ: {e}")

        kept, skipped_id, skipped_sim = 0, 0, 0
        with open(input_path, "r", encoding="utf-8") as fin, \
             open(output_path, "a", encoding="utf-8") as fout:
            for line in fin:
                try:
                    news = json.loads(line)
                    key = self._news_key(news)

                    # 1) æŒ‰å…¨å±€ ID å»é‡ï¼ˆåŒ…æ‹¬ processed_ids å’Œå·²æœ‰å»é‡æ–‡ä»¶ä¸­çš„ IDï¼‰
                    if key in seen_ids:
                        skipped_id += 1
                        continue

                    # 2) æ„é€ æ–‡æœ¬ï¼ŒæŒ‰å†…å®¹ç›¸ä¼¼åº¦å»é‡
                    raw_text = (news.get("title", "") + " " + news.get("content", "")).strip()
                    if not raw_text:
                        continue
                    if self.is_duplicate(raw_text):
                        skipped_sim += 1
                        continue
                    fout.write(line)
                    seen_ids.add(key)
                    kept += 1
                except Exception as e:
                    tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
        tools.log(f"âœ… å»é‡å®Œæˆ: ä¿ç•™ {kept} æ¡, æŒ‰ ID è·³è¿‡ {skipped_id} æ¡, æŒ‰ç›¸ä¼¼åº¦è·³è¿‡ {skipped_sim} æ¡")


def llm_extract_events(title: str, content: str, api_pool: LLMAPIPool, max_retries=2) -> List[Dict]:
    """ä½¿ç”¨LLMæå–äº‹ä»¶ï¼Œæ”¯æŒä¾èµ–æ³¨å…¥"""
    if api_pool is None:
        tools.log("[LLMè¯·æ±‚] âŒ API æ± æœªåˆå§‹åŒ–")
        return []

    # ä½¿ç”¨å·¥å…·å‡½æ•°åˆ›å»ºæç¤º
    entity_definitions = """âœ… å¿…é¡»æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š
- è‡ªç„¶äººï¼ˆå¦‚ Elon Muskã€Cathie Woodã€Warren Buffettï¼‰
- æ³¨å†Œå…¬å¸ï¼ˆå¦‚ Apple Inc.ã€Goldman Sachsã€ä¸­å›½å·¥å•†é“¶è¡Œã€Volkswagen AGï¼‰
- æ”¿åºœæœºæ„æˆ–éƒ¨é—¨ï¼ˆå¦‚ ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼šã€ä¸­å›½äººæ°‘é“¶è¡Œã€æ¬§ç›Ÿå§”å‘˜ä¼šã€æ—¥æœ¬é‡‘èå…ï¼‰
- ä¸»æƒå›½å®¶æˆ–æ˜ç¡®è¡Œæ”¿åŒºï¼ˆå¦‚ ç¾å›½ã€æ–°åŠ å¡ã€åŠ åˆ©ç¦å°¼äºšå·ã€é¦™æ¸¯ç‰¹åˆ«è¡Œæ”¿åŒºã€å¾·æ„å¿—è”é‚¦å…±å’Œå›½ï¼‰
- å›½é™…ç»„ç»‡ï¼ˆå¦‚ å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡ã€ä¸–ç•Œé“¶è¡Œã€è”åˆå›½ã€é‡‘èç¨³å®šç†äº‹ä¼šï¼‰
- é‡è¦äº§å“/å“ç‰Œ/å‹å·ï¼ˆç”±æ˜ç¡®ä¸»ä½“ç”Ÿäº§/æä¾›çš„å…·ä½“äº§å“æˆ–å“ç‰Œï¼Œå¦‚ iPhone 15 Proã€Tesla Model 3ã€ChatGPTã€Windows 11ã€Redmi 12Cï¼‰

âŒ ä»¥ä¸‹å†…å®¹**ä¸å¾—**è§†ä¸ºå®ä½“ï¼š
- åªä½œä¸ºæ–°é—»é€šè®¯æºçš„å®ä½“ä¸åº”è¯¥ä½œä¸ºå®ä½“
- æŠ½è±¡æ¦‚å¿µï¼ˆå¦‚ "å¸‚åœºæ³¢åŠ¨"ã€"ç³»ç»Ÿæ€§é£é™©"ã€"èµ„æœ¬æµåŠ¨"ï¼‰
- æŠ€æœ¯æˆ–é‡‘èæœ¯è¯­ï¼ˆå¦‚ "æœŸæƒå®šä»·"ã€"èµ„äº§è´Ÿå€ºè¡¨"ã€"é‡åŒ–å®½æ¾"ï¼‰
- é‡‘èå·¥å…·æˆ–èµ„äº§åç§°ï¼ˆå¦‚ "æ ‡æ™®500æŒ‡æ•°"ã€"10å¹´æœŸç¾å€º"ã€"é»„é‡‘æœŸè´§"ã€"BTC"ï¼‰â€”â€”é™¤éæŒ‡ä»£å…¶å‘è¡Œæ–¹ã€ç®¡ç†æ–¹æˆ–å…³è”æ³•äººï¼ˆå¦‚ "æ ‡æ™®é“ç¼æ–¯æŒ‡æ•°å…¬å¸"ï¼‰
- æ³›ç§°ï¼ˆå¦‚ "æŠ•èµ„è€…"ã€"ç›‘ç®¡æœºæ„"ã€"æŸé“¶è¡Œ"ã€"å¤§å‹ç§‘æŠ€å…¬å¸"ã€"æ™ºèƒ½æ‰‹æœº"ï¼‰
- æƒ…ç»ª/è¡Œæƒ…æè¿°ï¼ˆå¦‚ "æš´æ¶¨"ã€"æŠ›å”®æ½®"ã€"ç»æµè¡°é€€æ‹…å¿§"ï¼‰"""

    prompt = create_extraction_prompt(title, content, entity_definitions)

    # ä½¿ç”¨ç»Ÿä¸€çš„LLMè°ƒç”¨å‡½æ•°
    raw_content = call_llm_with_retry(
        llm_pool=api_pool,
        prompt=prompt,
        max_tokens=1500,
        timeout=55,
        retries=max_retries
    )

    if not raw_content:
        return []

    # ä½¿ç”¨ç»Ÿä¸€çš„JSONè§£æå‡½æ•°
    try:
        data = extract_json_from_llm_response(raw_content)
        events = data.get("events", [])
        result = []
        for item in events:
            abstract = item.get("abstract", "").strip()
            # ç¡®ä¿entitieså’Œentities_originalä¸€ä¸€å¯¹åº”ï¼Œä¸”éƒ½æœ‰æ•ˆ
            entities_raw = item.get("entities", [])
            entities_original_raw = item.get("entities_original", [])
            entities = []
            entities_original = []

            # éå†å¹¶è¿‡æ»¤ï¼Œç¡®ä¿ç´¢å¼•å¯¹åº”
            for ent, ent_original in zip(entities_raw, entities_original_raw):
                if tools.is_valid_entity(ent) and tools.is_valid_entity(ent_original):
                    entities.append(ent)
                    entities_original.append(ent_original)
            summary = item.get("event_summary", "").strip()
            if abstract and entities and summary:
                result.append({
                    "abstract": abstract,
                    "entities": entities,
                    "entities_original": entities_original,
                    "event_summary": summary
                })
        return result
    except Exception as e:
        tools.log(f"[LLMè·å–] âŒ LLM è¿”å›å†…å®¹è§£æå¤±è´¥: {e}")
        return []


@register_tool(
    name="extract_entities_events",
    description="ä½¿ç”¨ LLM ä»æ–°é—»æ ‡é¢˜å’Œå†…å®¹ä¸­æå–å®ä½“å’Œäº‹ä»¶",
    category="Information Extraction"
)
def extract_entities_events(title: str, content: str) -> List[Dict[str, Any]]:
    """
    ä»æ–°é—»ä¸­æå–å®ä½“å’Œäº‹ä»¶

    Args:
        title: æ–°é—»æ ‡é¢˜
        content: æ–°é—»å†…å®¹

    Returns:
        äº‹ä»¶åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« entities, event_summary ç­‰
    """
    api_pool = LLMAPIPool()
    return llm_extract_events(title, content, api_pool)

@register_tool(
    name="deduplicate_news_batch",
    description="å¯¹æ–°é—»åˆ—è¡¨è¿›è¡Œæ‰¹é‡å»é‡ (åŸºäº SimHash)",
    category="Data Processing"
)
def deduplicate_news_batch(news_list: List[Dict[str, Any]], threshold: int = 3) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡å»é‡
    
    Args:
        news_list: æ–°é—»å­—å…¸åˆ—è¡¨
        threshold: SimHash æ±‰æ˜è·ç¦»é˜ˆå€¼
        
    Returns:
        å»é‡åçš„æ–°é—»åˆ—è¡¨
    """
    if not news_list:
        return []
        
    deduper = NewsDeduplicator(threshold=threshold)
    unique_news = []
    
    for news in news_list:
        # æ„é€ æŒ‡çº¹æ–‡æœ¬
        text = (news.get("title", "") + " " + news.get("content", "")).strip()
        if not text: 
            continue
            
        # æ£€æŸ¥é‡å¤
        if not deduper.is_duplicate(text):
            unique_news.append(news)
            
    return unique_news

def get_unprocessed_news_files() -> List[Path]:
    """
    ä»…ä½¿ç”¨ tmp ç›®å½•çš„å»é‡ä¸å¤„ç†ã€‚
    tmp ç”¨äºæ–°æŠ“å–çš„å¾…å¤„ç†æ•°æ®ï¼Œå¤„ç†å®Œæˆåä¼šåˆ é™¤å¯¹åº” raw/dedupedã€‚
    """
    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r", encoding="utf-8", errors="ignore") as f:
            processed_ids = set(line.strip() for line in f if line.strip())

    unprocessed: List[Path] = []
    raw_dir = tools.RAW_NEWS_TMP_DIR
    dedup_dir = tools.DEDUPED_NEWS_TMP_DIR
    raw_dir.mkdir(parents=True, exist_ok=True)
    dedup_dir.mkdir(parents=True, exist_ok=True)

    for raw_file in sorted(raw_dir.glob("*.jsonl")):
        deduped_file = dedup_dir / f"{raw_file.stem}_deduped.jsonl"
        if not deduped_file.exists():
            deduper = NewsDeduplicator(threshold=tools.get_dedupe_threshold())
            deduper.dedupe_file(raw_file, deduped_file, processed_ids)
        unprocessed.append(deduped_file)
    return unprocessed


@register_tool(
    name="process_news_pipeline",
    description="[å·¥ä½œæµ] å¤„ç†æ–°é—»ç®¡é“ï¼šä»tmpæ–‡ä»¶è¯»å–ã€æå–å®ä½“äº‹ä»¶ã€æ›´æ–°å›¾è°±",
    category="Workflow"
)
async def process_news_pipeline(max_workers: int = 3, rate_limit_per_sec: float = 1.0) -> Dict[str, Any]:
    """
    ä¸»å¤„ç†æµç¨‹ï¼šå¹¶å‘å®ä½“æå–

    Args:
        max_workers: æœ€å¤§å¹¶å‘æ•°
        rate_limit_per_sec: æ¯ç§’é€Ÿç‡é™åˆ¶

    Returns:
        å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    tools.log(f"ğŸš€ å¯åŠ¨æ–°é—»å¤„ç†ç®¡é“ | workers={max_workers}, rate={rate_limit_per_sec}/s")
    files = get_unprocessed_news_files()
    if not files:
        tools.log("ğŸ“­ æ— å¯å¤„ç†æ–°é—»æ–‡ä»¶")
        return {"processed_count": 0, "files_processed": 0}

    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r", encoding="utf-8", errors="ignore") as f:
            processed_ids = set(line.strip() for line in f if line.strip())

    limiter = RateLimiter(rate_limit_per_sec)
    async_executor = AsyncExecutor()
    logger = tools.get_logger(__name__)
    api_pool = LLMAPIPool()
    total_processed = 0

    def build_published_at(ts: Optional[str]) -> Optional[str]:
        if not ts:
            return None
        try:
            return ts if isinstance(ts, str) else str(ts)
        except Exception:
            return None

    async def extract_task_async(global_id: str, title: str, content: str, source: str, published_at: Optional[str]) -> Tuple[str, str, Optional[str], List[Dict]]:
        try:
            await limiter.acquire_async()
            loop = asyncio.get_running_loop()
            extracted = await loop.run_in_executor(None, llm_extract_events, title, content, api_pool)
            return global_id, source, published_at, extracted
        except Exception as e:
            logger.error(f"ä»»åŠ¡ {global_id} æå–å¤±è´¥: {e}")
            return global_id, source, published_at, []

    with open(tools.PROCESSED_IDS_FILE, "a") as id_log:
        for file_path in files:
            logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")

            # æ”¶é›†éœ€è¦å¤„ç†çš„æ–°é—»ä»»åŠ¡
            news_tasks = []
            with open(file_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news = json.loads(line)
                        raw_id = str(news.get("id", "")).strip()
                        source = news.get("source", "unknown").strip().lower()

                        if not raw_id or not source:
                            logger.warning("âš ï¸ è·³è¿‡æ—  ID æˆ–æ—  source çš„æ–°é—»")
                            continue

                        global_id = f"{source}:{raw_id}"
                        if global_id in processed_ids:
                            continue

                        title = news.get("title", "")
                        content = news.get("content", "")
                        MAX_CONTENT_CHARS = 2000
                        if isinstance(content, str) and len(content) > MAX_CONTENT_CHARS:
                            content = content[:MAX_CONTENT_CHARS] + "â€¦â€¦ã€åæ–‡å·²æˆªæ–­ã€‘"

                        published_at = build_published_at(news.get("timestamp"))

                        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
                        news_tasks.append(
                            lambda gid=global_id, t=title, c=content, s=source, p=published_at: extract_task_async(gid, t, c, s, p)
                        )
                    except Exception as e:
                        logger.error(f"âš ï¸ è§£ææ–°é—»è¡Œå¤±è´¥: {e}")

            if news_tasks:
                logger.info(f"ğŸ”„ å¼€å§‹å¹¶å‘å¤„ç† {len(news_tasks)} ä¸ªæ–°é—»æå–ä»»åŠ¡")
                # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†å¹¶å‘æ‰§è¡Œ
                results = await async_executor.run_concurrent_tasks(
                    tasks=news_tasks,
                    concurrency=max_workers
                )

                for result in results:
                    try:
                        global_id, source, published_at, extracted = result
                        if not extracted:
                            logger.debug(f"â³ æ–°é—» {global_id}ï¼šLLM æœªè¿”å›æœ‰æ•ˆäº‹ä»¶ï¼Œä¿ç•™é‡è¯•æœºä¼š")
                            continue

                        all_entities = []
                        all_entities_original = []
                        for ev in extracted:
                            all_entities.extend(ev["entities"])
                            all_entities_original.extend(ev["entities_original"])

                        if all_entities and len(all_entities) == len(all_entities_original):
                            update_entities(all_entities, all_entities_original, source, published_at)
                            update_abstract_map(extracted, source, published_at)
                            total_processed += 1
                            id_log.write(global_id + "\n")
                            processed_ids.add(global_id)
                        else:
                            logger.debug(f"ğŸ” æ–°é—» {global_id}ï¼šLLM è¿”å›äº‹ä»¶ä½†æ— æœ‰æ•ˆå®ä½“ï¼Œæš‚ä¸æ ‡è®°")
                    except Exception as e:
                        logger.error(f"âš ï¸ å¤„ç†æå–ç»“æœå¤±è´¥: {e}")

            try:
                # å¤„ç† tmp ç›®å½•ä¸‹çš„ raw/deduped æ–‡ä»¶
                raw_dir = tools.RAW_NEWS_TMP_DIR
                raw_file_name = file_path.stem.replace("_deduped", "") + ".jsonl"
                raw_file_path = raw_dir / raw_file_name

                # ä½¿ç”¨ç»Ÿä¸€çš„åˆ é™¤å‡½æ•°
                safe_unlink(raw_file_path, "åŸå§‹æ–°é—»")
                safe_unlink(file_path, "å»é‡æ–°é—»")
            except Exception as e:
                tools.log(f"âš ï¸ åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")

    tools.log(f"âœ… å®Œæˆï¼å…±å¤„ç† {total_processed} æ¡å«æœ‰æ•ˆå®ä½“çš„æ–°é—»")
    return {"processed_count": total_processed, "files_processed": len(files)}


@register_tool(
    name="batch_process_news",
    description="[å·¥ä½œæµ] æ‰¹é‡å¤„ç†æ–°é—»ï¼šå»é‡å¹¶æå–äº‹ä»¶",
    category="Workflow"
)
async def batch_process_news(news_list: List[Dict[str, Any]], limit: int = -1) -> List[Dict[str, Any]]:
    """
    æ‰¹é‡å¤„ç†æ–°é—»ï¼š
    1. å»é‡
    2. æå–å®ä½“å’Œäº‹ä»¶
    3. é™„åŠ å…ƒæ•°æ® (source, published_at)

    Args:
        news_list: æ–°é—»åˆ—è¡¨
        limit: é™åˆ¶å¤„ç†çš„æ–°é—»æ•°é‡ï¼Œ-1 è¡¨ç¤ºä¸é™åˆ¶ã€‚ç”¨äºæµ‹è¯•/èŠ‚çœ Tokenã€‚

    Returns:
        æ‰å¹³åŒ–çš„äº‹ä»¶åˆ—è¡¨
    """
    # 1. å»é‡
    unique_news = deduplicate_news_batch(news_list)
    if limit > 0:
        unique_news = unique_news[:limit]

    # è¯»å–å¹¶å‘/é™é€Ÿé…ç½®ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ï¼‰
    config_manager = ConfigManager()
    max_workers = config_manager.get_concurrency_limit("agent1_config")
    rate_limit = config_manager.get_rate_limit("agent1_config")

    # ä½¿ç”¨ç»Ÿä¸€é™é€Ÿå™¨
    limiter = RateLimiter(rate_limit)

    def process_one(news: Dict[str, Any]) -> (List[Dict[str, Any]], Optional[str]):
        events_out = []
        processed_id = None
        try:
            title = news.get("title", "")
            content = news.get("content", "")
            source = news.get("source", "unknown")
            timestamp = news.get("datetime") or news.get("formatted_time")
            news_id = str(news.get("id", "")).strip()
            if news_id and source:
                processed_id = f"{source}:{news_id}"
            limiter.acquire()
            api_pool = LLMAPIPool()
            extracted = llm_extract_events(title, content, api_pool)
            for ev in extracted:
                ev["source"] = source
                ev["published_at"] = timestamp
                ev["news_id"] = news.get("id")
                events_out.append(ev)
        except Exception as e:
            print(f"Extraction failed for news {news.get('id', '')}: {e}")
        return events_out, processed_id

    all_events: List[Dict[str, Any]] = []
    processed_ids: List[str] = []
    if not unique_news:
        return all_events

    if max_workers <= 1:
        for n in unique_news:
            evs, pid = process_one(n)
            all_events.extend(evs)
            if pid:
                processed_ids.append(pid)
    else:
        # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†çº¿ç¨‹å¹¶å‘
        async_executor = AsyncExecutor()
        task_results = async_executor.run_threaded_tasks(
            tasks=unique_news,
            func=process_one,
            max_workers=max_workers
        )

        for evs, pid in task_results:
            all_events.extend(evs or [])
            if pid:
                processed_ids.append(pid)

    # è®°å½• processed_idsï¼Œé¿å…é‡å¤å¤„ç†
    if processed_ids:
        try:
            tools.PROCESSED_IDS_FILE.parent.mkdir(parents=True, exist_ok=True)
            with open(tools.PROCESSED_IDS_FILE, "a", encoding="utf-8") as f:
                for pid in processed_ids:
                    f.write(pid + "\n")
        except Exception:
            pass

    return all_events


@register_tool(
    name="persist_expanded_news_tmp",
    description="å°†æ‹“å±•æ–°é—»å†™å…¥ tmp/raw_news & tmp/deduped_newsï¼Œå¹¶è¿”å›è·¯å¾„",
    category="Workflow"
)
def persist_expanded_news_tmp(expanded_news: List[Dict[str, Any]]) -> Dict[str, str]:
    """
    ä¸ºå‰ç«¯è°ƒç”¨çš„åŒ…è£…ï¼šè½åœ°æ‹“å±•æ–°é—»åˆ° tmpï¼Œå¹¶è¿”å›æ–‡ä»¶è·¯å¾„ã€‚
    """
    processed_ids = set()
    if Path(tools.PROCESSED_IDS_FILE).exists():
        try:
            processed_ids = set(
                line.strip()
                for line in Path(tools.PROCESSED_IDS_FILE).read_text(encoding="utf-8").splitlines()
                if line.strip()
            )
        except Exception:
            processed_ids = set()

    deduped_path = persist_expanded_news_to_tmp(expanded_news, processed_ids)
    return {
        "deduped_path": str(deduped_path) if deduped_path else "",
        "raw_path": str(tools.RAW_NEWS_TMP_DIR) if deduped_path else "",
    }


@register_tool(
    name="save_extracted_events_tmp",
    description="å°†æå–çš„äº‹ä»¶åˆ—è¡¨å†™å…¥ data/tmp/extracted_events_*.jsonlï¼Œå¹¶è¿”å›è·¯å¾„",
    category="Data Processing"
)
def save_extracted_events_tmp(events: List[Dict[str, Any]]) -> Dict[str, str]:
    if not events:
        return {"path": ""}

    out_path = create_temp_file_path(tools.DATA_TMP_DIR, "extracted_events")
    write_jsonl_file(out_path, events, ensure_ascii=False)
    return {"path": str(out_path)}


def persist_expanded_news_to_tmp(expanded_news: List[Dict], processed_ids: Set[str]) -> Optional[Path]:
    """
    å°†æ‹“å±•æ–°é—»å†™å…¥ tmp åŸå§‹æ–‡ä»¶å¹¶åšå»é‡ï¼Œè¿”å›å»é‡åçš„æ–‡ä»¶è·¯å¾„ã€‚
    """
    if not expanded_news:
        return None

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    ensure_dirs(tools.RAW_NEWS_TMP_DIR, tools.DEDUPED_NEWS_TMP_DIR)

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
    ts = generate_timestamp()
    raw_path = tools.RAW_NEWS_TMP_DIR / f"expanded_{ts}.jsonl"
    deduped_path = tools.DEDUPED_NEWS_TMP_DIR / f"expanded_{ts}_deduped.jsonl"

    # å†™å…¥åŸå§‹æ•°æ®ï¼ˆå¤„ç†datetimeå­—æ®µï¼‰
    sanitized_news = sanitize_datetime_fields(expanded_news)
    write_jsonl_file(raw_path, sanitized_news, ensure_ascii=False)

    # å»é‡å¤„ç†
    deduper = NewsDeduplicator(threshold=tools.get_dedupe_threshold())
    deduper.dedupe_file(raw_path, deduped_path, processed_ids)
    return deduped_path
