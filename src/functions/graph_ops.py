from typing import List, Dict, Any, Optional, Set, Tuple
from ..core.registry import register_tool
from ..core import DataNormalizer, StandardEventPipeline, ConfigManager, LLMAPIPool, RateLimiter, AsyncExecutor, tools
from ..utils.data_utils import update_entities, update_abstract_map
from ..utils.json_utils import extract_json_from_llm_response
from ..utils.llm_utils import call_llm_with_retry, create_deduplication_prompt, create_event_deduplication_prompt
from ..utils.file_utils import ensure_dir
from ..utils.data_utils import write_json_file, read_json_file
from pathlib import Path
import json
from datetime import datetime, timedelta, timezone
import time
import threading
from difflib import SequenceMatcher
from collections import defaultdict

@register_tool(
    name="update_graph_data",
    description="å°†æå–çš„äº‹ä»¶æ•°æ®å†™å…¥çŸ¥è¯†å›¾è°±æ–‡ä»¶ (Entities & Events)",
    category="Knowledge Graph"
)
def update_graph_data(events_list: List[Dict[str, Any]], default_source: str = "auto_pipeline") -> Dict[str, Any]:
    """
    æ›´æ–°çŸ¥è¯†å›¾è°±æ•°æ®æ–‡ä»¶ã€‚
    
    Args:
        events_list: äº‹ä»¶åˆ—è¡¨ï¼Œæ¯é¡¹åº”åŒ…å« entities, abstract ç­‰ï¼Œä»¥åŠå¯é€‰çš„ source, published_at
        default_source: é»˜è®¤æ¥æºæ ‡è¯†
        
    Returns:
        æ›´æ–°çŠ¶æ€
    """
    count = 0
    for event in events_list:
        entities = event.get("entities", [])
        entities_original = event.get("entities_original", [])
        # å¦‚æœæ²¡æœ‰åŸå§‹å½¢å¼ï¼Œå›é€€åˆ°å®ä½“å
        if not entities_original:
            entities_original = entities
            
        source = event.get("source", default_source)
        published_at = event.get("published_at")
        
        # æ›´æ–°å®ä½“åº“
        update_entities(entities, entities_original, source, published_at)
        count += 1
        
    # æ›´æ–°äº‹ä»¶æ˜ å°„ (abstract_map)
    # update_abstract_map æœŸæœ›çš„æ˜¯ events_listï¼Œä½†å…¶ä¸­çš„ item éœ€è¦æœ‰ source å’Œ published_at
    # å¦‚æœ event å­—å…¸é‡Œå·²ç»æœ‰è¿™äº›å­—æ®µï¼Œupdate_abstract_map å†…éƒ¨æ€ä¹ˆå¤„ç†ï¼Ÿ
    # æŸ¥çœ‹ data_utils.py:
    #   def update_abstract_map(extracted_list, source, published_at):
    # å®ƒæ¥å—ä¸€ä¸ª source å’Œ published_at å‚æ•°ï¼Œç»Ÿä¸€åº”ç”¨äºæ‰€æœ‰ itemã€‚
    # è¿™å¯¹äºæ‰¹é‡å¤„ç†ä¸åŒæ¥æºçš„äº‹ä»¶ä¸å¤ªå‹å¥½ã€‚
    # æˆ‘ä»¬å¯ä»¥ç¨å¾® hack ä¸€ä¸‹ï¼šå¾ªç¯è°ƒç”¨ update_abstract_mapï¼Œæˆ–è€…ä¿®æ”¹ update_abstract_mapã€‚
    # ä¸ºäº†ä¸ä¿®æ”¹åŸæœ‰ utilï¼Œæˆ‘ä»¬æŒ‰ source åˆ†ç»„è°ƒç”¨ã€‚
    
    # ç®€å•çš„æŒ‰ä¸ªè°ƒç”¨ (æ•ˆç‡ç¨ä½ä½†å®‰å…¨)
    for event in events_list:
        src = event.get("source", default_source)
        ts = event.get("published_at")
        update_abstract_map([event], src, ts)
    
    return {"status": "success", "updated_count": count}

@register_tool(
    name="refresh_knowledge_graph",
    description="é‡å»ºå¹¶å‹ç¼©çŸ¥è¯†å›¾è°± (è§¦å‘ Agent3 é€»è¾‘)",
    category="Knowledge Graph"
)
def refresh_knowledge_graph() -> Dict[str, str]:
    """
    åˆ·æ–°çŸ¥è¯†å›¾è°±ï¼šæ„å»ºã€å‹ç¼©ã€æ›´æ–°
    """
    try:
        kg = KnowledgeGraph()
        kg.refresh_graph()
        return {"status": "refreshed"}
    except Exception as e:
        return {"status": "error", "message": str(e)}


@register_tool(
    name="generate_kg_visual_snapshots",
    description="ç”Ÿæˆè½»é‡å›¾è°±å¿«ç…§ï¼škg_visual.jsonï¼ˆå®ä½“-äº‹ä»¶è£å‰ªï¼‰ä¸ kg_visual_timeline.jsonï¼ˆäº‹ä»¶æ—¶é—´çº¿ï¼‰",
    category="Knowledge Graph"
)
def generate_kg_visual_snapshots(
    kg_path: str = "data/knowledge_graph.json",
    out_path_graph: str = "data/kg_visual.json",
    out_path_timeline: str = "data/kg_visual_timeline.json",
    top_entities: int = 500,
    top_events: int = 500,
    max_title_len: int = 80,
    days_window: Optional[int] = None
) -> Dict[str, str]:
    """
    ç”Ÿæˆè½»é‡çº§å¯è§†åŒ–å¿«ç…§ï¼Œä¾¿äºå‰ç«¯ç›´æ¥åŠ è½½ï¼š
    - kg_visual.json: nodes (id/label/type/color), edges (from/to/title)
    - kg_visual_timeline.json: events æŒ‰æ—¶é—´æ’åºï¼ŒåŒ…å«æ‘˜è¦ã€æ—¶é—´ã€å®ä½“åˆ—è¡¨ï¼ˆæˆªæ–­ï¼‰
    """
    kg_file = Path(kg_path)
    if not kg_file.exists():
        return {"status": "error", "message": f"{kg_file} not found"}

    try:
        kg = json.loads(kg_file.read_text(encoding="utf-8"))
    except Exception as e:
        return {"status": "error", "message": f"load kg failed: {e}"}

    entities = kg.get("entities") or {}
    events = kg.get("events") or {}
    edges = kg.get("edges") or []

    # æ—¶é—´çª—å£è¿‡æ»¤
    cutoff = None
    if days_window and days_window > 0:
        cutoff = datetime.utcnow() - timedelta(days=days_window)

    # è®¡ç®—åº¦
    deg = {}
    filtered_edges = []
    for e in edges:
        u, v = e.get("from"), e.get("to")
        if not u or not v:
            continue
        # äº‹ä»¶æ—¶é—´è¿‡æ»¤ï¼ˆåŸºäºäº‹ä»¶èŠ‚ç‚¹ï¼‰
        if cutoff and isinstance(v, str) and v.startswith("EVT:"):
            evt_key = v[4:]
            evt = events.get(evt_key, {})
            ts = evt.get("first_seen") or evt.get("published_at")
            try:
                dt = datetime.fromisoformat(ts.replace("Z", "+00:00")) if ts else None
            except Exception:
                dt = None
            if dt and dt < cutoff:
                continue

        deg[u] = deg.get(u, 0) + 1
        deg[v] = deg.get(v, 0) + 1
        filtered_edges.append(e)

    # å– Top èŠ‚ç‚¹
    top_ids = set()
    if deg:
        top_all = sorted(deg, key=deg.get, reverse=True)
        top_ids = set(top_all[: top_entities + top_events])

    # èŠ‚ç‚¹è¡¨
    vis_nodes = []
    node_types = {}
    for nid in top_ids:
        if isinstance(nid, str) and nid.startswith("EVT:"):
            evt = events.get(nid[4:], {})
            label = (evt.get("event_summary") or evt.get("abstract") or nid[4:])[:max_title_len]
            vis_nodes.append({"id": nid, "label": label, "type": "event", "color": "#ff7f0e"})
            node_types[nid] = "event"
        else:
            vis_nodes.append({"id": nid, "label": str(nid), "type": "entity", "color": "#1f77b4"})
            node_types[nid] = "entity"

    # è¾¹è¡¨
    vis_edges = []
    for e in filtered_edges:
        u, v = e.get("from"), e.get("to")
        if u in top_ids and v in top_ids:
            title = ""
            if isinstance(v, str) and v.startswith("EVT:"):
                evt_key = v[4:]
                title = (events.get(evt_key, {}) or {}).get("event_summary", "")[:max_title_len]
            vis_edges.append({"from": u, "to": v, "title": title})

    out_graph = Path(out_path_graph)
    ensure_dir(out_graph.parent)
    graph_data = {"nodes": vis_nodes, "edges": vis_edges}
    write_json_file(out_graph, graph_data, ensure_ascii=False, indent=None)

    # æ—¶é—´çº¿å¿«ç…§ï¼ˆå– TopN äº‹ä»¶æŒ‰æ—¶é—´æ’åºï¼‰
    tl_rows = []
    for evt_key, evt in events.items():
        ts = evt.get("first_seen") or evt.get("published_at")
        try:
            dt = datetime.fromisoformat(ts.replace("Z", "+00:00")) if ts else None
        except Exception:
            dt = None
        if not dt:
            continue
        if cutoff and dt < cutoff:
            continue
        ents = evt.get("entities", [])
        tl_rows.append({
            "time": ts,
            "event_summary": (evt.get("event_summary") or evt_key)[:max_title_len],
            "entities": ents[:20],
        })

    tl_rows = sorted(tl_rows, key=lambda x: x["time"])[: top_events]
    out_tl = Path(out_path_timeline)
    ensure_dir(out_tl.parent)
    write_json_file(out_tl, tl_rows, ensure_ascii=False, indent=None)

    return {
        "status": "ok",
        "graph_snapshot": str(out_graph),
        "timeline_snapshot": str(out_tl),
        "nodes": len(vis_nodes),
        "edges": len(vis_edges),
        "timeline_events": len(tl_rows),
    }


@register_tool(
    name="append_only_update_graph",
    description="ä»…è¿½åŠ æ–°äº‹ä»¶/å®ä½“åˆ°ç°æœ‰å›¾è°±ï¼Œä¸ä¿®æ”¹æ—§è®°å½•ï¼ˆå¯é€‰æ˜¯å¦è¿½åŠ åŸå§‹è¡¨è¿°ï¼‰",
    category="Knowledge Graph"
)
async def append_only_update_graph_tool(
    events_list: Any,
    default_source: str = "auto_pipeline",
    allow_append_original_forms: bool = True
) -> Dict[str, int]:
    """
    åŒ…è£… agent3.append_only_update_graphï¼š
    - ä»…å½“å®ä½“/äº‹ä»¶ä¸å­˜åœ¨æ—¶æ–°å¢
    - ä¸æ”¹æ—§ first_seen / sources / æ‘˜è¦ï¼›å¯é€‰æ˜¯å¦ä¸ºæ—§å®ä½“è¿½åŠ  original_forms
    - ä½¿ç”¨æ ‡å‡†äº‹ä»¶æ•°æ®ç®¡é“è¿›è¡Œå¤„ç†
    """
    # ä½¿ç”¨æ ‡å‡†äº‹ä»¶æ•°æ®ç®¡é“
    pipeline = StandardEventPipeline()

    # æ‰§è¡Œæ•°æ®ç®¡é“å¤„ç†
    pipeline_result = await pipeline.execute(events_list)

    # è·å–åºåˆ—åŒ–åçš„æ•°æ®
    processed_data = pipeline_result.get("serialization", [])
    if isinstance(processed_data, str):
        # å¦‚æœæ˜¯åºåˆ—åŒ–çš„JSONå­—ç¬¦ä¸²ï¼Œéœ€è¦è§£æå›æ¥
        import json
        processed_data = json.loads(processed_data)

    kg = KnowledgeGraph()
    return kg.append_only_update(
        events_list=processed_data,
        default_source=default_source,
        allow_append_original_forms=allow_append_original_forms
    )


@register_tool(
    name="append_tmp_extracted_events",
    description="ä» data/tmp/extracted_events_*.jsonl è¯»å–äº‹ä»¶å¹¶è¿½åŠ åˆ°å›¾è°±ï¼ˆä¸æ”¹æ—§è®°å½•ï¼‰",
    category="Knowledge Graph"
)
def append_tmp_extracted_events(
    base_dir: str = "data/tmp",
    pattern: str = "extracted_events_*.jsonl",
    max_files: int = 0,
    default_source: str = "auto_pipeline",
    allow_append_original_forms: bool = True
) -> Dict[str, Any]:
    """
    è¯»å– tmp ä¸­çš„æå–ç»“æœ jsonl æ–‡ä»¶å¹¶è°ƒç”¨ append_only_update_graph è¿½åŠ åˆ°å®ä½“/äº‹ä»¶åº“.
    max_files=0 è¡¨ç¤ºå…¨éƒ¨; >0 æ—¶æŒ‰ä¿®æ”¹æ—¶é—´å€’åºå–å‰ N ä¸ª.
    """
    base = Path(base_dir)
    if not base.exists():
        return {"status": "error", "message": f"{base} not found"}
    files = sorted(base.glob(pattern), key=lambda x: x.stat().st_mtime, reverse=True)
    if max_files and max_files > 0:
        files = files[:max_files]
    events: List[Dict[str, Any]] = []
    for fp in files:
        try:
            with fp.open("r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        if isinstance(obj, dict):
                            events.append(obj)
                    except Exception:
                        continue
        except Exception:
            continue
    kg = KnowledgeGraph()
    res = kg.append_only_update(
        events_list=events,
        default_source=default_source,
        allow_append_original_forms=allow_append_original_forms
    )
    return {
        "status": "ok",
        "files_used": [str(f) for f in files],
        "added_entities": res.get("added_entities", 0),
        "added_events": res.get("added_events", 0)
    }


class KnowledgeGraph:
    """
    å‹ç¼©çŸ¥è¯†å›¾è°±ç³»ç»Ÿï¼Œç”¨äºç®¡ç†å®ä½“å’Œäº‹ä»¶ï¼Œæ”¯æŒé‡å¤æ£€æµ‹å’Œæ›´æ–°ã€‚
    """

    def __init__(self):
        self.entities_file = tools.ENTITIES_FILE
        self.events_file = tools.EVENTS_FILE
        self.abstract_map_file = tools.ABSTRACT_MAP_FILE
        self.entities_tmp_file = tools.ENTITIES_TMP_FILE
        self.abstract_tmp_file = tools.ABSTRACT_TMP_FILE
        self.kg_file = tools.KNOWLEDGE_GRAPH_FILE
        self.merge_rules_file = tools.CONFIG_DIR / "entity_merge_rules.json" # è§„åˆ™æ–‡ä»¶è·¯å¾„
        self.merge_rules = {} # å†…å­˜ä¸­çš„è§„åˆ™ç¼“å­˜
        self.settings = self._load_agent3_settings()
        self.graph = {
            "entities": {},  # å®ä½“ID -> å®ä½“ä¿¡æ¯
            "events": {},   # äº‹ä»¶æ‘˜è¦ -> äº‹ä»¶ä¿¡æ¯
            "edges": []     # è¾¹åˆ—è¡¨ï¼Œè¿æ¥å®ä½“å’Œäº‹ä»¶
        }
        self.llm_pool = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self._tmp_loaded = []  # è®°å½•å·²åŠ è½½çš„tmpæ–‡ä»¶ï¼Œåˆ·æ–°å®Œæˆåæ¸…ç†
        self._load_merge_rules() # åˆå§‹åŒ–æ—¶åŠ è½½è§„åˆ™

    def _init_llm_pool(self):
        """åˆå§‹åŒ–LLM APIæ± """
        if self.llm_pool is None:
            try:
                self.llm_pool = LLMAPIPool()
                tools.log("[çŸ¥è¯†å›¾è°±] LLM APIæ± åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ åˆå§‹åŒ–LLM APIæ± å¤±è´¥: {e}")
                self.llm_pool = None

    def load_data(self) -> bool:
        """ä»æ–‡ä»¶åŠ è½½å®ä½“å’Œäº‹ä»¶æ•°æ®"""
        try:
            if self.entities_file.exists():
                with open(self.entities_file, 'r', encoding='utf-8') as f:
                    self.graph['entities'] = json.load(f)
            else:
                self.graph['entities'] = {}

            if self.abstract_map_file.exists():
                with open(self.abstract_map_file, 'r', encoding='utf-8') as f:
                    abstract_map = json.load(f)
                    # è½¬æ¢abstract_mapä¸ºäº‹ä»¶æ ¼å¼
                    self.graph['events'] = {
                        abstract: {
                            "abstract": abstract,
                            "entities": data["entities"],
                            "event_summary": data.get("event_summary", ""),
                            "sources": data.get("sources", []),
                            "first_seen": data.get("first_seen", "")
                        }
                        for abstract, data in abstract_map.items()
                    }
            else:
                self.graph['events'] = {}

            # é¢å¤–åŠ è½½ tmp æ–°å¢æ•°æ®ï¼ˆæœªåˆå¹¶çš„æ–°å¢å®ä½“/äº‹ä»¶ï¼‰
            self._load_tmp_entities()
            self._load_tmp_events()

            self._build_edges()
            tools.log(f"[çŸ¥è¯†å›¾è°±] æ•°æ®åŠ è½½æˆåŠŸ: {len(self.graph['entities'])} å®ä½“, {len(self.graph['events'])} äº‹ä»¶")
            return True
        except Exception as e:
            tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False

    def _build_edges(self):
        """æ„å»ºå®ä½“å’Œäº‹ä»¶ä¹‹é—´çš„è¾¹"""
        self.graph['edges'] = []
        for abstract, event in self.graph['events'].items():
            for entity in event.get('entities', []):
                if entity in self.graph['entities']:
                    self.graph['edges'].append({
                        "from": entity,
                        "to": abstract,
                        "type": "involved_in"
                    })

    def build_graph(self) -> bool:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        return self.load_data()

    def _load_merge_rules(self):
        """åŠ è½½å®ä½“åˆå¹¶è§„åˆ™"""
        if self.merge_rules_file.exists():
            try:
                with open(self.merge_rules_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.merge_rules = data.get("merge_rules", {})
                tools.log(f"[çŸ¥è¯†å›¾è°±] å·²åŠ è½½ {len(self.merge_rules)} æ¡å®ä½“åˆå¹¶è§„åˆ™")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½åˆå¹¶è§„åˆ™å¤±è´¥: {e}")
        else:
            self.merge_rules = {}

    def _load_tmp_entities(self):
        """åŠ è½½å¹¶åˆå¹¶ tmp å®ä½“æ•°æ®"""
        if self.entities_tmp_file.exists():
            try:
                with open(self.entities_tmp_file, "r", encoding="utf-8") as f:
                    tmp_entities = json.load(f)
                    for name, data in tmp_entities.items():
                        if name in self.graph['entities']:
                            self._merge_entity_record(self.graph['entities'][name], data)
                        else:
                            self.graph['entities'][name] = data
                self._tmp_loaded.append(self.entities_tmp_file)
                tools.log(f"[çŸ¥è¯†å›¾è°±] å·²åŠ è½½ tmp å®ä½“ {len(tmp_entities)} æ¡")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½ tmp å®ä½“å¤±è´¥: {e}")

    def _load_tmp_events(self):
        """åŠ è½½å¹¶åˆå¹¶ tmp äº‹ä»¶æ•°æ®"""
        if self.abstract_tmp_file.exists():
            try:
                with open(self.abstract_tmp_file, "r", encoding="utf-8") as f:
                    tmp_events = json.load(f)
                    for abstract, data in tmp_events.items():
                        if abstract in self.graph['events']:
                            self._merge_event_record(self.graph['events'][abstract], data)
                        else:
                            self.graph['events'][abstract] = {
                                "abstract": abstract,
                                "entities": data.get("entities", []),
                                "event_summary": data.get("event_summary", ""),
                                "sources": data.get("sources", []),
                                "first_seen": data.get("first_seen", "")
                            }
                self._tmp_loaded.append(self.abstract_tmp_file)
                tools.log(f"[çŸ¥è¯†å›¾è°±] å·²åŠ è½½ tmp äº‹ä»¶ {len(tmp_events)} æ¡")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ åŠ è½½ tmp äº‹ä»¶å¤±è´¥: {e}")

    def _cleanup_tmp_files(self):
        """åˆ·æ–°å®Œæˆåæ¸…ç†å·²åŠ è½½çš„ tmp æ–‡ä»¶"""
        for path in self._tmp_loaded:
            try:
                path.unlink(missing_ok=True)
                tools.log(f"[çŸ¥è¯†å›¾è°±] ğŸ—‘ï¸ å·²æ¸…ç† tmp æ–‡ä»¶: {path}")
            except Exception as e:
                tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ æ— æ³•åˆ é™¤ tmp æ–‡ä»¶ {path}: {e}")
        self._tmp_loaded = []

    def _save_merge_rules(self):
        """ä¿å­˜å®ä½“åˆå¹¶è§„åˆ™"""
        try:
            data = {
                "merge_rules": self.merge_rules,
                "last_updated": time.strftime("%Y-%m-%dT%H:%M:%S")
            }
            with open(self.merge_rules_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            tools.log(f"[çŸ¥è¯†å›¾è°±] å·²ä¿å­˜åˆå¹¶è§„åˆ™åº“ (å…± {len(self.merge_rules)} æ¡)")
        except Exception as e:
            tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ ä¿å­˜åˆå¹¶è§„åˆ™å¤±è´¥: {e}")

    def _merge_entity_record(self, target: Dict[str, Any], source: Dict[str, Any]):
        """å°†sourceå®ä½“ä¿¡æ¯åˆå¹¶åˆ°targetï¼Œä¸åˆ é™¤èŠ‚ç‚¹"""
        if not source:
            return
        # sources
        primary_sources = set()
        for s in target.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        for s in source.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        target['sources'] = list(primary_sources)

        # original_forms
        primary_forms = set()
        for f in target.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)
        for f in source.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)
        target['original_forms'] = list(primary_forms)

        # first_seen å–æœ€æ—©
        primary_first = target.get('first_seen', '')
        source_first = source.get('first_seen', '')
        if source_first and (not primary_first or source_first < primary_first):
            target['first_seen'] = source_first

    def _merge_event_record(self, target: Dict[str, Any], source: Dict[str, Any]):
        """å°†sourceäº‹ä»¶ä¿¡æ¯åˆå¹¶åˆ°targetï¼Œä¸åˆ é™¤èŠ‚ç‚¹"""
        if not source:
            return
        # sources
        primary_sources = set()
        for s in target.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        for s in source.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)
        target['sources'] = list(primary_sources)

        # entities union
        ents = set(target.get('entities', []))
        ents.update(source.get('entities', []))
        target['entities'] = list(ents)

        # first_seen å–æœ€æ—©
        primary_first = target.get('first_seen', '')
        source_first = source.get('first_seen', '')
        if source_first and (not primary_first or source_first < primary_first):
            target['first_seen'] = source_first

        # event_summary è‹¥ç¼ºå¤±åˆ™è¡¥
        if not target.get('event_summary') and source.get('event_summary'):
            target['event_summary'] = source['event_summary']

    def _load_agent3_settings(self) -> Dict[str, Any]:
        """
        åŠ è½½agent3ç›¸å…³é…ç½®ï¼Œä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ã€‚
        """
        config_manager = ConfigManager()

        # ä½¿ç”¨ConfigManagerè·å–é…ç½®ï¼Œé»˜è®¤å€¼åœ¨ConfigManagerä¸­å¤„ç†
        try:
            settings = {
                "entity_batch_size": config_manager.get_config_value("entity_batch_size", 80, "agent3_config"),
                "event_batch_size": config_manager.get_config_value("event_batch_size", 15, "agent3_config"),
                "event_bucket_days": config_manager.get_config_value("event_bucket_days", 7, "agent3_config"),
                "event_bucket_entity_overlap": config_manager.get_config_value("event_bucket_entity_overlap", 1, "agent3_config"),
                "event_bucket_max_size": config_manager.get_config_value("event_bucket_max_size", 40, "agent3_config"),
                "event_precluster_similarity": config_manager.get_config_value("event_precluster_similarity", 0.82, "agent3_config"),
                "event_precluster_limit": config_manager.get_config_value("event_precluster_limit", 300, "agent3_config"),
                "entity_precluster_similarity": config_manager.get_config_value("entity_precluster_similarity", 0.93, "agent3_config"),
                "entity_precluster_limit": config_manager.get_config_value("entity_precluster_limit", 500, "agent3_config"),
                "max_summary_chars": config_manager.get_config_value("max_summary_chars", 360, "agent3_config"),
                "entity_max_workers": config_manager.get_config_value("entity_max_workers", 3, "agent3_config"),
                "event_max_workers": config_manager.get_config_value("event_max_workers", 3, "agent3_config"),
                "rate_limit_per_sec": config_manager.get_config_value("rate_limit_per_sec", 1.0, "agent3_config"),
                "entity_evidence_per_entity": config_manager.get_config_value("entity_evidence_per_entity", 2, "agent3_config"),
                "entity_evidence_max_chars": config_manager.get_config_value("entity_evidence_max_chars", 400, "agent3_config"),
            }
            return settings
        except Exception as e:
            # è¿”å›é»˜è®¤å€¼
            return {
                "entity_batch_size": 80,
                "event_batch_size": 15,
                "event_bucket_days": 7,
                "event_bucket_entity_overlap": 1,
                "event_bucket_max_size": 40,
                "event_precluster_similarity": 0.82,
                "event_precluster_limit": 300,
                "entity_precluster_similarity": 0.93,
                "entity_precluster_limit": 500,
                "max_summary_chars": 360,
                "entity_max_workers": 3,
                "event_max_workers": 3,
                "rate_limit_per_sec": 1.0,
                "entity_evidence_per_entity": 2,
                "entity_evidence_max_chars": 400,
            }

    def _string_similarity(self, a: str, b: str) -> float:
        """å­—ç¬¦ä¸²ç›¸ä¼¼åº¦(0-1)"""
        return SequenceMatcher(None, a.lower(), b.lower()).ratio()

    def _is_chinese(self, text: str) -> bool:
        return any('\u4e00' <= ch <= '\u9fff' for ch in text)

    def _entity_type(self, name: str) -> str:
        """
        è½»é‡ç±»å‹åˆ¤å®šï¼šgeo/org/person/product/unknown
        ä»…ç”¨äºé˜»æ–­è·¨ç±»å‹åˆå¹¶ï¼Œå®å¯ unknownã€‚
        """
        n = name or ""
        geo_kw = ["å¸‚", "çœ", "å·", "å¿", "åŒº", "é•‡", "ä¹¡", "éƒ¡", "å²›", "åºœ", "é“", "è‡ªæ²»", "å…±å’Œå›½", "ç‹å›½", "ç‰¹åŒº", "è‡ªæ²»åŒº"]
        org_kw = ["å…¬å¸", "é›†å›¢", "é“¶è¡Œ", "æ”¿åºœ", "éƒ¨", "å±€", "ç½²", "é™¢", "å…", "è¡Œ", "å…š", "æœºæ„", "æ³•é™¢", "æ£€å¯Ÿé™¢", "å§”å‘˜ä¼š", "ç»„ç»‡", "è”ç›Ÿ", "ç†äº‹ä¼š", "åä¼š", "åŸºé‡‘ä¼š", "å¤§å­¦", "å­¦é™¢", "å­¦æ ¡", "å·¥å‚", "å‚", "æŠ¥", "ç”µè§†", "æ–°é—»", "æ—¥æŠ¥", "æ™šæŠ¥", "å‘¨æŠ¥", "ç¤¾"]
        product_kw = ["ç³»åˆ—", "ç‰ˆ", "å‹", "å‹å·", "Pro", "Ultra"]
        if any(k in n for k in geo_kw):
            return "geo"
        if any(k in n for k in org_kw):
            return "org"
        if " " in n or "Â·" in n:
            return "person"
        if any(k in n for k in product_kw):
            return "product"
        return "unknown"

    def _valid_entity_group(self, group: List[str]) -> bool:
        """è·¨ç±»å‹åˆå¹¶æ‹¦æˆªï¼šè‹¥æ··åˆ geo/org/person/product åˆ™æ‹’ç»"""
        types = set()
        for name in group:
            t = self._entity_type(name)
            if t != "unknown":
                types.add(t)
        # å¦‚æœæ£€æµ‹åˆ°å¤šç§å·²çŸ¥ç±»å‹åˆ™è§†ä¸ºé«˜é£é™©
        if len(types) > 1:
            tools.log(f"[çŸ¥è¯†å›¾è°±] âš ï¸ è·¨ç±»å‹åˆå¹¶è¢«é˜»æ­¢: {group} | types={types}")
            return False
        return True

    def _collect_entity_evidence(self, entities_batch: List[str]) -> Dict[str, List[str]]:
        """
        ä¸ºå®ä½“æ‰¹æ¬¡æ”¶é›†ç›¸å…³äº‹ä»¶æ‘˜è¦+å®ä½“ï¼Œå‡å°‘å¹»è§‰ã€‚
        """
        per_entity = int(self.settings.get("entity_evidence_per_entity", 2))
        max_chars = int(self.settings.get("entity_evidence_max_chars", 400))
        evidence: Dict[str, List[str]] = {e: [] for e in entities_batch}
        for abstract, event in self.graph['events'].items():
            ents = event.get('entities', [])
            summary = self._trim_text(event.get('event_summary', "") or abstract, max_chars)
            for e in ents:
                if e in evidence and len(evidence[e]) < per_entity:
                    evidence[e].append(f"{abstract} | {', '.join(ents)} | {summary}")
        return evidence

    def _trim_text(self, text: str, max_chars: int) -> str:
        """æ§åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…promptè¿‡é•¿"""
        if not text or max_chars <= 0:
            return text
        if len(text) <= max_chars:
            return text
        return text[:max_chars] + "..."

    def _parse_time(self, ts: str) -> float:
        """å°½é‡è§£ææ—¶é—´æˆ³ï¼Œå¤±è´¥è¿”å›0"""
        if not ts:
            return 0
        try:
            # æ”¯æŒISOå­—ç¬¦ä¸²
            return datetime.fromisoformat(ts.replace("Z", "+00:00")).timestamp()
        except Exception:
            try:
                return time.mktime(time.strptime(ts, "%Y-%m-%d %H:%M:%S"))
            except Exception:
                return 0

    def _bucket_events_by_time_and_entity(
        self,
        window_days: int,
        min_entity_overlap: int,
        max_bucket_size: int
    ) -> List[Dict[str, Any]]:
        """
        æŒ‰æ—¶é—´çª—å£ä¸å®ä½“äº¤é›†åˆ†æ¡¶äº‹ä»¶ï¼Œå‡å°‘è·¨æœŸ/è·¨ä¸»ä½“æ··æ‚ã€‚
        """
        events_items = list(self.graph["events"].items())
        window_sec = window_days * 86400

        # é¢„æ’åºï¼Œæ—¶é—´ç¼ºå¤±æ”¾æœ«å°¾
        def _sort_key(item):
            ts = self._parse_time(item[1].get("first_seen", ""))
            return ts if ts > 0 else float("inf")

        events_items.sort(key=_sort_key)

        buckets: List[Dict[str, Any]] = []
        for abstract, event in events_items:
            entities = set(event.get("entities", []))
            ts = self._parse_time(event.get("first_seen", ""))
            placed = False
            for bucket in buckets:
                if len(bucket["keys"]) >= max_bucket_size:
                    continue
                # æ—¶é—´çª—å£åˆ¤å®š
                if bucket["min_time"] and ts and ts < bucket["min_time"] - window_sec:
                    continue
                if bucket["max_time"] and ts and ts > bucket["max_time"] + window_sec:
                    continue
                # å®ä½“äº¤é›†åˆ¤å®š
                if min_entity_overlap > 0:
                    if not entities or len(bucket["entities"].intersection(entities)) < min_entity_overlap:
                        continue
                # å‘½ä¸­ï¼ŒåŠ å…¥æ¡¶
                bucket["keys"].append(abstract)
                bucket["entities"].update(entities)
                if ts:
                    bucket["min_time"] = min(bucket["min_time"] or ts, ts)
                    bucket["max_time"] = max(bucket["max_time"] or ts, ts)
                placed = True
                break
            if not placed:
                buckets.append({
                    "keys": [abstract],
                    "entities": set(entities),
                    "min_time": ts if ts else None,
                    "max_time": ts if ts else None
                })
        tools.log(f"[çŸ¥è¯†å›¾è°±] äº‹ä»¶åˆ†æ¡¶å®Œæˆï¼Œå…± {len(buckets)} ä¸ªæ¡¶")
        return buckets

    def _precluster_entities_by_string(self, entities: List[str], threshold: float, limit: int) -> List[List[str]]:
        """
        åŸºäºå­—ç¬¦ä¸²ç›¸ä¼¼åº¦çš„è½»é‡é¢„èšç±»ï¼Œé¿å…LLMè¿‡é‡è¾“å…¥ã€‚
        """
        if len(entities) == 0 or len(entities) > limit:
            return []
        res = []
        used = set()
        for i, ent in enumerate(entities):
            if ent in used:
                continue
            group = [ent]
            used.add(ent)
            for other in entities[i+1:]:
                if other in used:
                    continue
                if self._string_similarity(ent, other) >= threshold:
                    group.append(other)
                    used.add(other)
            if len(group) > 1:
                res.append(group)
        if res:
            tools.log(f"[çŸ¥è¯†å›¾è°±] æœ¬åœ°å®ä½“é¢„èšç±»å‘ç° {len(res)} ç»„å¯èƒ½é‡å¤")
        return res

    def _precluster_events_by_string(
        self,
        events_map: Dict[str, Dict[str, Any]],
        keys: List[str],
        threshold: float,
        limit: int,
        max_summary_chars: int
    ) -> List[List[str]]:
        """
        åŒæ¡¶äº‹ä»¶çš„å­—ç¬¦ä¸²è¿‘ä¼¼èšç±»ï¼Œå‡å°‘LLMè´Ÿæ‹…ã€‚
        """
        if len(keys) == 0 or len(keys) > limit:
            return []

        def norm_text(k: str) -> str:
            evt = events_map.get(k, {})
            summary = self._trim_text(evt.get("event_summary", "") or "", max_summary_chars)
            return (k + " " + summary).lower()

        texts = {k: norm_text(k) for k in keys}
        res = []
        used = set()
        for i, key in enumerate(keys):
            if key in used:
                continue
            base = texts.get(key, "")
            group = [key]
            used.add(key)
            for other in keys[i+1:]:
                if other in used:
                    continue
                if self._string_similarity(base, texts.get(other, "")) >= threshold:
                    group.append(other)
                    used.add(other)
            if len(group) > 1:
                res.append(group)
        if res:
            tools.log(f"[çŸ¥è¯†å›¾è°±] æœ¬åœ°äº‹ä»¶é¢„èšç±»å‘ç° {len(res)} ç»„å¯èƒ½é‡å¤")
        return res

    def _apply_merge_rules(self) -> bool:
        """
        åº”ç”¨æœ¬åœ°åˆå¹¶è§„åˆ™
        è¿”å›: æ˜¯å¦æœ‰æ›´æ–°
        """
        updated = False
        if not self.merge_rules:
            return False

        # éå†ç°æœ‰å®ä½“ï¼Œçœ‹æ˜¯å¦åŒ¹é…è§„åˆ™
        # æ³¨æ„ï¼šéœ€è¦åœ¨éå†æ—¶å¤„ç†ï¼Œé¿å…å­—å…¸å¤§å°å˜åŒ–é—®é¢˜ï¼Œé€šå¸¸æ”¶é›†åå†å¤„ç†
        to_merge = []
        for entity in list(self.graph['entities'].keys()):
            if entity in self.merge_rules:
                target = self.merge_rules[entity]
                # åªæœ‰å½“ç›®æ ‡å®ä½“ä¹Ÿå­˜åœ¨ï¼Œæˆ–è€…ç›®æ ‡å°±æ˜¯æˆ‘ä»¬æƒ³è¦ç»Ÿä¸€åˆ°çš„åç§°æ—¶ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºå¦‚æœç›®æ ‡åœ¨åº“é‡Œæˆ–æˆ‘ä»¬å†³å®šæ”¹åï¼‰
                # ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾è§„åˆ™æ˜¯ A -> Bï¼Œå¦‚æœ A å­˜åœ¨ï¼Œå°±å°è¯•åˆå¹¶åˆ° Bã€‚
                # å¦‚æœ B ä¸åœ¨åº“é‡Œï¼Œå°±æŠŠ A é‡å‘½åä¸º Bã€‚
                if target != entity:
                    to_merge.append((target, entity))

        for primary, duplicate in to_merge:
            # å¦‚æœç›®æ ‡å®ä½“ä¸å­˜åœ¨ï¼Œå…ˆé‡å‘½å
            if primary not in self.graph['entities'] and duplicate in self.graph['entities']:
                self.graph['entities'][primary] = self.graph['entities'][duplicate]
                del self.graph['entities'][duplicate]
                # æ›´æ–°äº‹ä»¶å¼•ç”¨
                for abstract, event in self.graph['events'].items():
                    entities = event.get('entities', [])
                    if duplicate in entities:
                        event['entities'] = [primary if e == duplicate else e for e in entities]
                tools.log(f"[çŸ¥è¯†å›¾è°±][è§„åˆ™] é‡å‘½åå®ä½“: {duplicate} -> {primary}")
                updated = True
            elif primary in self.graph['entities'] and duplicate in self.graph['entities']:
                # å¦‚æœéƒ½å­˜åœ¨ï¼Œåˆ™åˆå¹¶
                self._merge_entities(primary, duplicate)
                updated = True

        return updated

    def compress_with_llm(self) -> Dict[str, List[str]]:
        """
        ä½¿ç”¨LLMåˆ†æå‹ç¼©çŸ¥è¯†å›¾è°±ï¼Œè¾“å‡ºé‡å¤çš„å®ä½“å’Œäº‹ä»¶æŠ½è±¡ã€‚
        åˆ†æ‰¹å¤„ç†ä»¥é¿å…ä¸Šä¸‹æ–‡è¶…é•¿ã€‚
        é›†æˆè§„åˆ™ä¼˜å…ˆç­–ç•¥ã€‚
        """
        # é¦–å…ˆåº”ç”¨æœ¬åœ°è§„åˆ™
        rule_applied = self._apply_merge_rules()
        if rule_applied:
            self._save_data()

        self._init_llm_pool()
        if self.llm_pool is None:
            tools.log("[çŸ¥è¯†å›¾è°±] âŒ LLMä¸å¯ç”¨ï¼Œè·³è¿‡å‹ç¼©")
            return {"duplicate_entities": [], "duplicate_events": []}

        # å¹¶å‘ä¸é€Ÿç‡æ§åˆ¶
        rate_limit = float(self.settings.get("rate_limit_per_sec", 1.0))
        limiter = RateLimiter(rate_limit) if rate_limit > 0 else None

        # å¤„ç†å®ä½“å’Œäº‹ä»¶
        all_duplicate_entities = self._compress_entities_with_llm(limiter)
        all_duplicate_events = self._compress_events_with_llm(limiter)

        return {
            "duplicate_entities": all_duplicate_entities,
            "duplicate_events": all_duplicate_events
        }

    def _compress_entities_with_llm(self, limiter: Optional[RateLimiter]) -> List[List[str]]:
        """ä½¿ç”¨LLMå‹ç¼©å®ä½“"""
        all_duplicate_entities = []

        # æ’åºä»¥å¢åŠ ç›¸ä¼¼å®ä½“ç›¸é‚»çš„æ¦‚ç‡
        entities_list = sorted(list(self.graph['entities'].keys()))
        if not entities_list:
            return all_duplicate_entities

        # é¢„èšç±»
        precluster_entities = self._precluster_entities_by_string(
            entities_list,
            threshold=float(self.settings.get("entity_precluster_similarity", 0.93)),
            limit=int(self.settings.get("entity_precluster_limit", 500))
        )
        if precluster_entities:
            all_duplicate_entities.extend(precluster_entities)

        # åˆ†æ‰¹å¤„ç†
        entity_batches = [
            (idx, entities_list[i:i+int(self.settings.get("entity_batch_size", 80))])
            for idx, i in enumerate(range(0, len(entities_list), int(self.settings.get("entity_batch_size", 80))))
        ]

        if entity_batches:
            entity_results = self._process_entity_batches(entity_batches, limiter)
            all_duplicate_entities.extend(entity_results)

        return all_duplicate_entities

    def _process_entity_batches(self, entity_batches: List[Tuple[int, List[str]]],
                               limiter: Optional[RateLimiter]) -> List[List[str]]:
        """å¤„ç†å®ä½“æ‰¹æ¬¡"""
        entity_workers = int(self.settings.get("entity_max_workers", 3))
        all_results = []
        new_rules_count = 0

        def _run_entity_batch(idx: int, batch: List[str]) -> List[List[str]]:
            tools.log(f"[çŸ¥è¯†å›¾è°±] å¤„ç†å®ä½“æ‰¹æ¬¡ {idx+1}/{len(entity_batches)} (å¤§å°: {len(batch)})")
            prompt = self._prepare_entity_compression_prompt_strict(batch)
            response = self._call_llm_limited(prompt, timeout=90, limiter=limiter)
            return self._parse_entity_response(response) if response else []

        # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†çº¿ç¨‹å¹¶å‘
        async_executor = AsyncExecutor()
        entity_task_results = async_executor.run_threaded_tasks(
            tasks=[batch for idx, batch in entity_batches],
            func=lambda batch: _run_entity_batch(entity_batches.index((0, batch)), batch),
            max_workers=entity_workers
        )

        for batch_dupes in entity_task_results:
            if batch_dupes:
                for group in batch_dupes:
                    if len(group) >= 2 and self._valid_entity_group(group):
                        primary, dupes = self._choose_primary_entity(group)
                        for duplicate in dupes:
                            if duplicate not in self.merge_rules:
                                self.merge_rules[duplicate] = primary
                                new_rules_count += 1
                        all_results.append([primary] + dupes)

        if new_rules_count > 0:
            self._save_merge_rules()

        return all_results

    def _compress_events_with_llm(self, limiter: Optional[RateLimiter]) -> List[List[str]]:
        """ä½¿ç”¨LLMå‹ç¼©äº‹ä»¶"""
        all_duplicate_events = []

        events_list = sorted(list(self.graph['events'].keys()))
        if not events_list:
            return all_duplicate_events

        # äº‹ä»¶åˆ†æ¡¶
        bucket_days = int(self.settings.get("event_bucket_days", 7))
        bucket_overlap = int(self.settings.get("event_bucket_entity_overlap", 1))
        bucket_max_size = int(self.settings.get("event_bucket_max_size", 40))
        buckets = self._bucket_events_by_time_and_entity(bucket_days, bucket_overlap, bucket_max_size)

        if buckets:
            event_results = self._process_event_buckets(buckets, limiter)
            all_duplicate_events.extend(event_results)

        return all_duplicate_events

    def _process_event_buckets(self, buckets: List[Dict[str, Any]],
                              limiter: Optional[RateLimiter]) -> List[List[str]]:
        """å¤„ç†äº‹ä»¶æ¡¶"""
        event_workers = int(self.settings.get("event_max_workers", 3))
        BATCH_SIZE_EVT = int(self.settings.get("event_batch_size", 15))
        evt_similarity = float(self.settings.get("event_precluster_similarity", 0.82))
        evt_limit = int(self.settings.get("event_precluster_limit", 300))
        max_summary_chars = int(self.settings.get("max_summary_chars", 360))

        def _run_event_bucket(idx: int, bucket: Dict[str, Any]) -> List[List[str]]:
            bucket_keys = bucket.get("keys", [])
            bucket_events = {k: self.graph['events'][k] for k in bucket_keys if k in self.graph['events']}
            local_dupes: List[List[str]] = []

            # é¢„èšç±»
            pre_clusters = self._precluster_events_by_string(
                bucket_events, bucket_keys, evt_similarity, evt_limit, max_summary_chars
            )
            if pre_clusters:
                local_dupes.extend(pre_clusters)

            if len(bucket_keys) <= 1:
                tools.log(f"[çŸ¥è¯†å›¾è°±] è·³è¿‡äº‹ä»¶æ¡¶ {idx+1}/{len(buckets)}ï¼ˆä»…1æ¡ï¼Œæ— éœ€å»é‡ï¼‰")
                return local_dupes

            # LLMæ‰¹å¤„ç†
            total_batches = (len(bucket_keys) - 1) // BATCH_SIZE_EVT + 1
            for i in range(0, len(bucket_keys), BATCH_SIZE_EVT):
                batch_keys = bucket_keys[i:i+BATCH_SIZE_EVT]
                batch_events = {
                    k: {
                        **bucket_events.get(k, {}),
                        "event_summary": self._trim_text(
                            bucket_events.get(k, {}).get("event_summary", "") or "",
                            max_summary_chars
                        )
                    }
                    for k in batch_keys
                }
                tools.log(
                    f"[çŸ¥è¯†å›¾è°±] å¤„ç†äº‹ä»¶æ¡¶ {idx+1}/{len(buckets)} çš„æ‰¹æ¬¡ {i//BATCH_SIZE_EVT + 1}/{total_batches} (å¤§å°: {len(batch_keys)})"
                )
                prompt = self._prepare_event_compression_prompt(batch_events)
                response = self._call_llm_limited(prompt, timeout=120, limiter=limiter)
                if response:
                    batch_dupes = self._parse_event_response(response)
                    local_dupes.extend(batch_dupes)
            return local_dupes

        # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†çº¿ç¨‹å¹¶å‘
        async_executor = AsyncExecutor()
        event_task_results = async_executor.run_threaded_tasks(
            tasks=[bucket for bucket in buckets],
            func=lambda bucket: _run_event_bucket(buckets.index(bucket), bucket),
            max_workers=event_workers
        )

        all_results = []
        for batch_dupes in event_task_results:
            if batch_dupes:
                all_results.extend(batch_dupes)

        return all_results

    def _call_llm(self, prompt: str, timeout: int) -> Optional[str]:
        """ç»Ÿä¸€LLMè°ƒç”¨ - ä½¿ç”¨å·¥å…·å‡½æ•°"""
        return call_llm_with_retry(
            llm_pool=self.llm_pool,
            prompt=prompt,
            max_tokens=4000,
            timeout=timeout,
            retries=2
        )

    def _call_llm_limited(self, prompt: str, timeout: int, limiter: Optional[RateLimiter]) -> Optional[str]:
        """å¸¦å…¨å±€QPSé™åˆ¶çš„LLMè°ƒç”¨"""
        return call_llm_with_retry(
            llm_pool=self.llm_pool,
            prompt=prompt,
            max_tokens=4000,
            timeout=timeout,
            retries=2,
            limiter=limiter
        )

    def _choose_primary_entity(self, group: List[str]) -> (str, List[str]):
        """
        é€‰æ‹©ä¸»å®ä½“ï¼šä¼˜å…ˆä¸­æ–‡ï¼Œå…¶æ¬¡ first_seen æœ€æ—©ï¼Œå†å…¶æ¬¡åç§°é•¿åº¦ã€‚
        è¿”å› (primary, duplicates)
        """
        if not group:
            return "", []
        best = None
        best_key = None
        for name in group:
            info = self.graph['entities'].get(name, {})
            is_cn = self._is_chinese(name)
            ts = self._parse_time(info.get('first_seen', ''))
            ts_key = ts if ts > 0 else float('inf')
            key = (0 if is_cn else 1, ts_key, len(name))
            if best_key is None or key < best_key:
                best = name
                best_key = key
        duplicates = [n for n in group if n != best]
        return best, duplicates

    def _prepare_entity_compression_prompt_strict(self, entities_batch: List[str]) -> str:
        evidence_map = self._collect_entity_evidence(entities_batch)
        evidence_lines = []
        for ent, evs in evidence_map.items():
            if evs:
                for ev in evs:
                    evidence_lines.append(f"{ent} <= {ev}")

        prompt = f"""ä½ æ˜¯ä¸€åçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ä»»åŠ¡ï¼šä»…åœ¨æœ‰å……åˆ†è¯æ®æ—¶è®¤å®šå®ä½“ä¸ºåŒä¸€ä¸»ä½“ï¼ˆåˆ«å/ç¼©å†™/ä¸­è‹±æ–‡/æ³•å®šå…¨ç§°å·®å¼‚ï¼‰ã€‚ä¸è¦å› ä¸ºè¡Œä¸šç›¸ä¼¼ã€ä¸Šä¸‹çº§å…³ç³»æˆ–åœ°åŸŸç›¸ä¼¼è€Œåˆå¹¶ã€‚

ã€é«˜é£é™©è¯¯åˆ¤ç¤ºä¾‹ã€‘
- å®ä½“å…·æœ‰ç‰¹åŒ–èŒèƒ½æˆ–åŠŸæ•ˆæˆ–ç”¨é€”ï¼Œä¸å¯åˆå¹¶
- è¡Œä½¿èŒèƒ½çš„ç»„ç»‡ã€æœºæ„ä¸å…¶ä¸‹è¾–çš„æ›´å…·ä½“èŒèƒ½çš„ç»„ç»‡ã€æœºæ„ä¸å¯åˆå¹¶
- "å¤§å­¦" ä¸ "è”ç›Ÿ/åä¼š/éƒ¨é—¨/å¤®è¡Œ" ä¸æ˜¯åŒä¸€ä¸»ä½“
- ä¸åŒå›½å®¶/åœ°åŒºçš„åŒåæœºæ„ï¼Œä¸å¯åˆå¹¶
- ä¸Šå¸‚å…¬å¸ vs å­å…¬å¸/æ§è‚¡è‚¡ä¸œï¼Œä¸å¯åˆå¹¶
- æ”¿åºœéƒ¨é—¨ vs ä¸Šçº§æ”¿åºœï¼Œä¸å¯åˆå¹¶
- å›½å®¶/çœå·/åŸå¸‚/åŒºå¿ ä¹‹é—´ä¸å¾—äº’å¹¶ï¼Œä¹Ÿä¸å¾—è·¨å›½åˆå¹¶
- å…¬å¸/æœºæ„ â‰  äº§å“/å“ç‰Œ/å‹å·ï¼Œä¸å¾—äº’å¹¶
- äººå â‰  å…¬å¸/æœºæ„/åœ°ç†/äº§å“ï¼Œä¸å¾—äº’å¹¶
- åª’ä½“åç§° â‰  åœ°ç‚¹/æ”¿åºœ/ä¼ä¸š/äººå
- ä½“è‚²ä¿±ä¹éƒ¨/èµ›äº‹ â‰  åŸå¸‚/å›½å®¶/æ”¿åºœ/ä¸ªäºº

ã€å®ä½“åˆ—è¡¨ã€‘
{json.dumps(entities_batch, ensure_ascii=False, indent=2)}

ã€è¯æ®ï¼ˆéƒ¨åˆ†ç›¸å…³äº‹ä»¶æ‘˜è¦ï¼Œä¾›å‚è€ƒï¼Œé¿å…å¹»è§‰ï¼‰ã€‘
æ ¼å¼: å®ä½“ <= æ‘˜è¦ | å‚ä¸å®ä½“ | æè¿°
{chr(10).join(evidence_lines) if evidence_lines else "ï¼ˆæ— å¯ç”¨äº‹ä»¶ï¼Œè°¨æ…åˆå¹¶ï¼‰"}

ã€è¦æ±‚ã€‘
- ä¸»å®ä½“ä¼˜å…ˆæ›´é€šç”¨ï¼ˆæˆ–æ›´å€¾å‘ä¸­å›½äººè¡¨è¾¾ï¼‰ã€æ›´è¯¦ç»†ã€æ›´ç²¾ç¡®ã€æ›´å­¦æœ¯ï¼ˆ"æ›´XX"æŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰
- åªè¾“å‡ºç¡®å®šä¸ºåŒä¸€ä¸»ä½“çš„ç»„åˆï¼›ä¸ç¡®å®šå°±è¿”å›ç©ºã€‚
- ä¼˜å…ˆä¸¥æ ¼åŒ¹é…ï¼šåŒåã€æ˜æ˜¾è¯‘åã€ç¼©å†™å±•å¼€ã€‚
- ä¸è¦æ”¹å†™åç§°æ ¼å¼ï¼ˆä¸è¦æ·»åŠ ä¹¦åå·/å¼•å·/æ‹¬å·ç­‰æ ‡ç‚¹ï¼‰ã€‚
- åœ°ç†/æœºæ„ç±»åˆå¹¶éœ€åŒä¸€å›½å®¶/è¡Œæ”¿å±‚çº§ï¼›äººåä¸å¾—ä¸éäººååˆå¹¶ï¼›å…¬å¸ä¸å¾—ä¸äº§å“/å“ç‰Œåˆå¹¶ã€‚
- å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼š
{{
  "duplicate_entities": [
    ["ä¸»å®ä½“", "åˆ«åæˆ–é‡å¤"],
    ["ä¸»å®ä½“2", "åˆ«å2", "åˆ«å3"]
  ]
}}
å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å› {{ "duplicate_entities": [] }}ã€‚åªè¾“å‡ºJSONã€‚
"""
        return prompt

    def _prepare_event_compression_prompt(self, events_batch: Dict) -> str:
        prompt = f"""ä½ æ˜¯ä¸€åçŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚ä»»åŠ¡ï¼šä»…åœ¨æè¿°"åŒä¸€å…·ä½“äº‹å®"æ—¶æ‰è§†ä¸ºé‡å¤äº‹ä»¶ã€‚ä¸è¦åˆå¹¶ä¸åŒä¸»ä½“ã€ä¸Šä¸‹æ¸¸å…³è”æˆ–æ—¶é—´ä¸åŒçš„ç›¸ä¼¼äº‹ä»¶ã€‚

ã€æ‹’ç»åˆå¹¶çš„æƒ…å†µç¤ºä¾‹ã€‘
- è¡Œä¸šç›¸ä¼¼ä½†ä¸»ä½“ä¸åŒçš„äº‹ä»¶
- ä¸Šæ¸¸/ä¸‹æ¸¸/ç›‘ç®¡/è”ç›Ÿå…³ç³» â‰  åŒä¸€äº‹ä»¶
- æ—¶é—´é—´éš”æ˜æ˜¾ä¸åŒçš„å¤šæ¬¡äº‹ä»¶
- äº‹ä»¶å…·æœ‰è¿ç»­å‘ç”Ÿæˆ–ä¸€å‰ä¸€åçš„å…³ç³»
- å¯¹äºç›¸åŒå®ä½“ï¼Œä¸åŒæ—¶é—´ç‚¹å‘ç”Ÿçš„äº‹ä»¶

ã€äº‹ä»¶åˆ—è¡¨ã€‘
æ ¼å¼: æ‘˜è¦ | å‚ä¸å®ä½“ | äº‹ä»¶æè¿°
"""
        for abstract, event in events_batch.items():
            entities = event.get('entities', [])
            summary = event.get('event_summary', '')
            prompt += f"{abstract} | {', '.join(entities)} | {summary}\n"

        prompt += """
ã€ä»»åŠ¡ã€‘
æ‰¾å‡ºè¯­ä¹‰ä¸Šé«˜åº¦é‡å ã€æè¿°åŒä¸€äº‹å®çš„äº‹ä»¶ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
ä¸¥æ ¼è¿”å› JSONï¼š
{
  "duplicate_events": [
    ["äº‹ä»¶æ‘˜è¦1", "äº‹ä»¶æ‘˜è¦2"],
    ["äº‹ä»¶æ‘˜è¦3", "äº‹ä»¶æ‘˜è¦4", "äº‹ä»¶æ‘˜è¦5"]
  ]
}
å¦‚æœæ²¡æœ‰é‡å¤ï¼Œè¿”å› { "duplicate_events": [] }ã€‚åªè¾“å‡ºJSONã€‚
"""
        return prompt

    def _parse_entity_response(self, raw_content: str) -> List[List[str]]:
        try:
            data = self._extract_json(raw_content)
            res = data.get("duplicate_entities", [])
            return res if isinstance(res, list) else []
        except Exception:
            return []

    def _parse_event_response(self, raw_content: str) -> List[List[str]]:
        try:
            data = self._extract_json(raw_content)
            res = data.get("duplicate_events", [])
            return res if isinstance(res, list) else []
        except Exception:
            return []

    def _extract_json(self, text: str) -> Dict:
        """ä½¿ç”¨ç»Ÿä¸€çš„JSONæå–å‡½æ•°"""
        return extract_json_from_llm_response(text)

    def update_entities_and_events(self, duplicates: Dict[str, List[List[str]]]):
        """æ ¹æ®é‡å¤æ£€æµ‹ç»“æœæ›´æ–°å®ä½“åº“å’Œäº‹ä»¶åº“"""
        updated = False

        # åˆå¹¶é‡å¤å®ä½“
        for group in duplicates.get("duplicate_entities", []):
            if len(group) < 2:
                continue
            if not self._valid_entity_group(group):
                continue
            primary, dupes = self._choose_primary_entity(group)
            # ç¡®ä¿ä¸»å®ä½“å­˜åœ¨ï¼›è‹¥ä¸å­˜åœ¨ä½†æœ‰é‡å¤å®ä½“å­˜åœ¨ï¼Œå¯äº¤æ¢
            if primary not in self.graph['entities'] and dupes:
                for d in dupes:
                    if d in self.graph['entities']:
                        primary, dupes = d, [x for x in group if x != d]
                        break
            for duplicate in dupes:
                if duplicate in self.graph['entities'] and primary in self.graph['entities']:
                    self._merge_entities(primary, duplicate)
                    updated = True

        # åˆå¹¶é‡å¤äº‹ä»¶
        for group in duplicates.get("duplicate_events", []):
            if len(group) < 2:
                continue
            primary = group[0]
            for duplicate in group[1:]:
                if duplicate in self.graph['events']:
                    self._merge_events(primary, duplicate)
                    updated = True

        if updated:
            self._save_data()
            tools.log("[çŸ¥è¯†å›¾è°±] å®ä½“å’Œäº‹ä»¶æ›´æ–°å®Œæˆ")
        else:
            tools.log("[çŸ¥è¯†å›¾è°±] æ— é‡å¤é¡¹éœ€è¦æ›´æ–°")

    def _merge_entities(self, primary: str, duplicate: str):
        """åˆå¹¶é‡å¤å®ä½“"""
        if primary not in self.graph['entities'] or duplicate not in self.graph['entities']:
            return

        primary_data = self.graph['entities'][primary]
        duplicate_data = self.graph['entities'][duplicate]

        # åˆå¹¶sources (ç¡®ä¿è½¬æ¢ä¸ºå¯å“ˆå¸Œçš„tupleæˆ–ç›´æ¥åˆ—è¡¨å¤„ç†)
        primary_sources = set()
        for s in primary_data.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue # æš‚æ—¶å¿½ç•¥å¤æ‚ç»“æ„
            else: primary_sources.add(s)

        for s in duplicate_data.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)

        # è½¬å›list
        primary_data['sources'] = list(primary_sources)

        # åˆå¹¶original_forms
        primary_forms = set()
        for f in primary_data.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)

        for f in duplicate_data.get('original_forms', []):
            if isinstance(f, list): primary_forms.add(tuple(f))
            elif isinstance(f, dict): continue
            else: primary_forms.add(f)

        # å°†é‡å¤å®ä½“åä¹Ÿä½œä¸ºä¸»å®ä½“çš„å…¶ä»–è¡¨è¿°è®°å½•ï¼Œé˜²æ­¢ä¸¢å¤±åˆ«å
        primary_forms.add(duplicate)
        primary_forms.add(primary)

        primary_data['original_forms'] = list(primary_forms)

        # æ›´æ–°first_seenä¸ºæ›´æ—©çš„æ—¶é—´
        primary_first = primary_data.get('first_seen', '')
        duplicate_first = duplicate_data.get('first_seen', '')
        if duplicate_first and (not primary_first or duplicate_first < primary_first):
            primary_data['first_seen'] = duplicate_first

        # åˆ é™¤é‡å¤å®ä½“
        del self.graph['entities'][duplicate]

        # æ›´æ–°äº‹ä»¶ä¸­çš„å®ä½“å¼•ç”¨
        for abstract, event in self.graph['events'].items():
            entities = event.get('entities', [])
            if duplicate in entities:
                # æ›¿æ¢ä¸ºprimaryï¼Œå¹¶å»é‡
                new_entities = [primary if ent == duplicate else ent for ent in entities]
                # å»é‡
                unique_entities = []
                seen = set()
                for ent in new_entities:
                    if ent not in seen:
                        seen.add(ent)
                        unique_entities.append(ent)
                event['entities'] = unique_entities

        tools.log(f"[çŸ¥è¯†å›¾è°±] åˆå¹¶å®ä½“: {duplicate} -> {primary}")

    def _merge_events(self, primary: str, duplicate: str):
        """åˆå¹¶é‡å¤äº‹ä»¶"""
        if primary not in self.graph['events'] or duplicate not in self.graph['events']:
            return

        primary_event = self.graph['events'][primary]
        duplicate_event = self.graph['events'][duplicate]

        # åˆå¹¶sources
        primary_sources = set()
        for s in primary_event.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)

        for s in duplicate_event.get('sources', []):
            if isinstance(s, list): primary_sources.add(tuple(s))
            elif isinstance(s, dict): continue
            else: primary_sources.add(s)

        primary_event['sources'] = list(primary_sources)

        # åˆå¹¶entities
        primary_entities = set(primary_event.get('entities', []))
        duplicate_entities = set(duplicate_event.get('entities', []))
        primary_event['entities'] = list(primary_entities.union(duplicate_entities))

        # æ›´æ–°first_seen
        primary_first = primary_event.get('first_seen', '')
        duplicate_first = duplicate_event.get('first_seen', '')
        if duplicate_first and (not primary_first or duplicate_first < primary_first):
            primary_event['first_seen'] = duplicate_first

        # äº‹ä»¶æè¿°åˆå¹¶ï¼šä¿ç•™æ›´è¯¦ç»†çš„
        if not primary_event.get('event_summary') and duplicate_event.get('event_summary'):
            primary_event['event_summary'] = duplicate_event['event_summary']

        # åˆ é™¤é‡å¤äº‹ä»¶
        del self.graph['events'][duplicate]

        tools.log(f"[çŸ¥è¯†å›¾è°±] åˆå¹¶äº‹ä»¶: {duplicate} -> {primary}")

    def _save_data(self):
        """ä¿å­˜æ›´æ–°åçš„æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            # ä¿å­˜å®ä½“
            write_json_file(self.entities_file, self.graph['entities'], ensure_ascii=False, indent=2)

            # ä¿å­˜äº‹ä»¶ï¼ˆabstract_mapæ ¼å¼ï¼‰
            abstract_map = {}
            for abstract, event in self.graph['events'].items():
                abstract_map[abstract] = {
                    "entities": event.get('entities', []),
                    "event_summary": event.get('event_summary', ''),
                    "sources": event.get('sources', []),
                    "first_seen": event.get('first_seen', '')
                }

            write_json_file(self.abstract_map_file, abstract_map, ensure_ascii=False, indent=2)

            # ä¿å­˜çŸ¥è¯†å›¾è°±çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            write_json_file(self.kg_file, self.graph, ensure_ascii=False, indent=2)

            tools.log("[çŸ¥è¯†å›¾è°±] æ•°æ®ä¿å­˜å®Œæˆ")
        except Exception as e:
            tools.log(f"[çŸ¥è¯†å›¾è°±] âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")

    def append_only_update(self, events_list: List[Dict[str, Any]], default_source: str = "auto_pipeline", allow_append_original_forms: bool = True) -> Dict[str, int]:
        """
        åªè¿½åŠ æ–°æ•°æ®ï¼Œä¸æ”¹åŠ¨å·²æœ‰å®ä½“/äº‹ä»¶çš„æ—§å­—æ®µã€‚
        - å®ä½“å·²å­˜åœ¨ï¼šä¸æ”¹ first_seen/sourcesï¼Œé»˜è®¤ä»…å¯é€‰åœ°è¿½åŠ åŸå§‹è¡¨è¿°
        - äº‹ä»¶å·²å­˜åœ¨ï¼ˆåŒ abstractï¼‰ï¼šè·³è¿‡ï¼Œä¸æ”¹æ—§äº‹ä»¶
        """
        if not events_list:
            return {"added_entities": 0, "added_events": 0}

        if not self.build_graph():
            return {"added_entities": 0, "added_events": 0}

        # å‡†å¤‡ LLM å»é‡æ˜ å°„ï¼ˆä»…æ˜ å°„"æ–°å®ä½“"åˆ°"å·²æœ‰å®ä½“"ï¼Œä¸æ”¹æ—§å®ä½“å­—æ®µï¼‰
        self._init_llm_pool()
        merge_rules = self.merge_rules or {}
        existing_entities = set(self.graph["entities"].keys())

        # æ”¶é›†æ–°å®ä½“é›†åˆ
        new_entities_set = set()
        for ev in events_list:
            ents = ev.get("entities", []) or []
            ents = [merge_rules.get(e, e) for e in ents if e]
            for e in ents:
                if e not in existing_entities:
                    new_entities_set.add(e)

        # æ„å»ºæ˜ å°„ï¼šæ–°å®ä½“ -> å·²æœ‰å®ä½“ï¼ˆLLM åˆ¤æ–­å¯èƒ½åŒåï¼‰
        llm_merge_map: Dict[str, str] = {}
        if self.llm_pool and new_entities_set:
            # åˆ†æ¡¶é™ä½ä¸Šä¸‹æ–‡é•¿åº¦ï¼šæŒ‰é¦–å­—æ¯/å­—ç¬¦åˆ†æ¡¶
            from collections import defaultdict
            # from concurrent.futures import ThreadPoolExecutor, as_completed  # å·²è¿ç§»åˆ°AsyncExecutor
            buckets = defaultdict(list)
            for ent in new_entities_set:
                prefix = ent[0] if ent else "#"
                buckets[prefix].append(ent)

            llm_lock = threading.Lock()
            max_workers = int(self.settings.get("entity_max_workers", 3)) or 1
            async_executor = AsyncExecutor()

            def handle_bucket(prefix: str, bucket_new: List[str]):
                local_map = {}
                # å–åŒå‰ç¼€çš„éƒ¨åˆ†å·²æœ‰å®ä½“åšå¯¹æ¯”ï¼Œé¿å…è¿‡é•¿
                existing_subset = [e for e in existing_entities if e.startswith(prefix)]
                existing_subset = existing_subset[: max(10, min(80, len(existing_subset)))]
                candidates = list(existing_subset) + list(bucket_new)
                if len(candidates) < 2:
                    return local_map
                try:
                    prompt = self._prepare_entity_compression_prompt_strict(candidates)
                    resp = self._call_llm_limited(prompt, timeout=60, limiter=None)
                    groups = self._parse_entity_response(resp) if resp else []
                    for g in groups:
                        if len(g) < 2:
                            continue
                        primary, dupes = self._choose_primary_entity(g)
                        if primary in existing_entities:
                            for d in dupes:
                                if d in new_entities_set:
                                    local_map[d] = primary
                except Exception as e:
                    tools.log(f"[çŸ¥è¯†å›¾è°±] è¿½åŠ æ¨¡å¼ LLM å»é‡å¤±è´¥: {e}")
                return local_map

            # ä½¿ç”¨AsyncExecutorç»Ÿä¸€ç®¡ç†çº¿ç¨‹å¹¶å‘
            bucket_results = async_executor.run_threaded_tasks(
                tasks=[(p, b) for p, b in buckets.items()],
                func=lambda pb: handle_bucket(pb[0], pb[1]),
                max_workers=max_workers
            )

            for res_map in bucket_results:
                if res_map:
                    with llm_lock:
                        llm_merge_map.update(res_map)

        added_entities = 0
        added_events = 0

        def normalize_entity(name: str) -> str:
            if not name:
                return name
            name = merge_rules.get(name, name)
            return llm_merge_map.get(name, name)

        for ev in events_list:
            ents = ev.get("entities", []) or []
            ents = [normalize_entity(e) for e in ents]
            ents_original = ev.get("entities_original") or ents
            src = ev.get("source", default_source)
            published_at = ev.get("published_at") or ev.get("datetime") or ""

            # è¿½åŠ å®ä½“ï¼ˆä»…å½“ä¸å­˜åœ¨ï¼‰
            for ent, ent_orig in zip(ents, ents_original):
                if not ent:
                    continue
                if ent not in self.graph["entities"]:
                    self.graph["entities"][ent] = {
                        "first_seen": published_at or datetime.utcnow().isoformat(),
                        "sources": [src] if src else [],
                        "original_forms": [ent_orig] if ent_orig else []
                    }
                    added_entities += 1
                else:
                    # ä¸æ”¹æ—§å­—æ®µï¼Œä»…å¯é€‰è¿½åŠ åŸå§‹è¡¨è¿°
                    if allow_append_original_forms and ent_orig:
                        forms = self.graph["entities"][ent].get("original_forms", [])
                        if ent_orig not in forms:
                            forms.append(ent_orig)
                            self.graph["entities"][ent]["original_forms"] = forms

            # è¿½åŠ äº‹ä»¶ï¼ˆä»…å½“æ‘˜è¦ä¸å­˜åœ¨ï¼‰
            abstract = ev.get("abstract")
            if abstract and abstract not in self.graph["events"]:
                self.graph["events"][abstract] = {
                    "abstract": abstract,
                    "entities": ents,
                    "event_summary": ev.get("event_summary", ""),
                    "sources": [src] if src else [],
                    "first_seen": published_at
                }
                added_events += 1

        # é‡å»ºè¾¹å¹¶ä¿å­˜ï¼ˆåªåœ¨æœ‰æ–°å¢æ—¶ï¼‰
        if added_entities or added_events:
            self._build_edges()
            self._save_data()
            tools.log(f"[çŸ¥è¯†å›¾è°±] è¿½åŠ æ¨¡å¼å®Œæˆï¼šæ–°å¢å®ä½“ {added_entities}ï¼Œæ–°å¢äº‹ä»¶ {added_events}")
        else:
            tools.log("[çŸ¥è¯†å›¾è°±] è¿½åŠ æ¨¡å¼ï¼šæ²¡æœ‰æ–°å¢å®ä½“/äº‹ä»¶")

        return {"added_entities": added_entities, "added_events": added_events}

    def refresh_graph(self):
        """åˆ·æ–°çŸ¥è¯†å›¾è°±ï¼šæ„å»ºã€å‹ç¼©ã€æ›´æ–°"""
        tools.log("[çŸ¥è¯†å›¾è°±] å¼€å§‹åˆ·æ–°çŸ¥è¯†å›¾è°±")

        # æ„å»ºå›¾
        if not self.build_graph():
            tools.log("[çŸ¥è¯†å›¾è°±] âŒ æ„å»ºå›¾å¤±è´¥")
            return

        # å‹ç¼©ï¼šä½¿ç”¨LLMæ£€æµ‹é‡å¤
        duplicates = self.compress_with_llm()

        # æ›´æ–°å®ä½“å’Œäº‹ä»¶
        self.update_entities_and_events(duplicates)

        tools.log("[çŸ¥è¯†å›¾è°±] çŸ¥è¯†å›¾è°±åˆ·æ–°å®Œæˆ")
        # æ¸…ç†å·²åŠ è½½çš„ä¸´æ—¶æ–‡ä»¶
        self._cleanup_tmp_files()

