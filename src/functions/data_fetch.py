from typing import List, Dict, Any, Optional, Set
import pandas as pd
from ..core.registry import register_tool
from ..data import news_collector
from ..utils.tool_function import tools as Tools
from ..core import ConfigManager, AsyncExecutor, RateLimiter
from ..utils.data_utils import update_entities, update_abstract_map
from ..functions.extraction import llm_extract_events, NewsDeduplicator, persist_expanded_news_to_tmp
from ..utils.file_utils import safe_unlink_multiple, safe_unlink
from ..utils.data_utils import sanitize_datetime_fields, write_jsonl_file
from pathlib import Path
import json
import asyncio
import time
from datetime import datetime, timedelta, timezone

@register_tool(
    name="fetch_news_stream",
    description="ä»æ‰€æœ‰é…ç½®çš„æ•°æ®æºï¼ˆå½“å‰ä»… GNewsï¼‰è·å–æœ€æ–°æ–°é—»",
    category="Data Fetch"
)
async def fetch_news_stream(
    limit: int = 50,
    sources: Optional[List[str]] = None,
    # GNews å¯é€‰å‚æ•°
    category: Optional[str] = None,
    query: Optional[str] = None,
    from_: Optional[str] = None,
    to: Optional[str] = None,
    nullable: Optional[str] = None,
    truncate: Optional[str] = None,
    sortby: Optional[str] = None,
    in_fields: Optional[str] = None,
    page: Optional[int] = None,
) -> List[Dict[str, Any]]:
    """
    è·å–å…¨æ¸ é“æ–°é—»æ•°æ®ã€‚
    
    Args:
        limit: æ¯ä¸ªæºè·å–çš„æœ€å¤§æ¡æ•°
        sources: æŒ‡å®šæºåˆ—è¡¨ (å¦‚ ["GNews-cn"]), é»˜è®¤ä¸ºæ‰€æœ‰å¯ç”¨æº
        
    Returns:
        æ–°é—»åˆ—è¡¨ (List[Dict])
    """
    tools = Tools()
    # é…ç½®é©±åŠ¨çš„å¹¶å‘ä¸Šé™ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ï¼‰
    config_manager = ConfigManager()
    concurrency = config_manager.get_concurrency_limit("agent1_config")
    
    # åˆå§‹åŒ– API Pool
    news_collector.init_api_pool()
    if news_collector.API_POOL is None:
        raise RuntimeError("API Pool failed to initialize")

    available_sources = news_collector.API_POOL.list_available_sources()
    if sources:
        target_sources = [s for s in sources if s in available_sources]
    else:
        target_sources = available_sources
    
    if not target_sources:
        tools.log("Warning: No valid sources to fetch from.")
        return []

    # ä½¿ç”¨ç»Ÿä¸€å¼‚æ­¥æ‰§è¡Œå™¨
    async_executor = AsyncExecutor()

    async def fetch_one(source_name: str) -> List[Dict[str, Any]]:
        try:
            collector = news_collector.API_POOL.get_collector(source_name)
            async with collector:
                if query:
                    news = await collector.search(
                        query=query,
                        from_=from_,
                        to=to,
                        limit=limit,
                        in_fields=in_fields,
                        nullable=nullable,
                        sortby=sortby,
                        page=page,
                        truncate=truncate,
                    )
                else:
                    news = await collector.get_top_headlines(
                        category=category,
                        limit=limit,
                        nullable=nullable,
                        from_=from_,
                        to=to,
                        query=query,
                        page=page,
                        truncate=truncate,
                    )
                for item in news:
                    if "source" not in item:
                        item["source"] = source_name
                    if "datetime" in item and hasattr(item["datetime"], "isoformat"):
                        item["datetime"] = item["datetime"].isoformat()
                tools.log(f"Fetched {len(news)} items from {source_name}")
                return news
        except Exception as e:
            tools.log(f"Error fetching from {source_name}: {e}")
            return []

    # ä½¿ç”¨AsyncExecutorè¿›è¡Œå¹¶å‘æ‰§è¡Œ
    results = await async_executor.run_concurrent_tasks(
        tasks=[lambda src=src: fetch_one(src) for src in target_sources],
        concurrency=concurrency
    )

    all_news = []
    for news in results:
        all_news.extend(news)

    # æŒ‰æ—¶é—´å€’åºæ’åº
    all_news.sort(key=lambda x: x.get("datetime") or "", reverse=True)
    return all_news


def _load_entity_equivs() -> Dict[str, Set[str]]:
    """
    æ„å»ºå®ä½“åŠåŒä¹‰è¯ç´¢å¼•ï¼šå®ä½“åº“ original_forms + åˆå¹¶è§„åˆ™çš„åˆ«åã€‚
    è¿”å› dict: è¯ -> åŒä¹‰é›†åˆï¼ˆåŒ…å«è‡ªèº«ï¼‰ã€‚
    """
    tools = Tools()
    idx: Dict[str, Set[str]] = {}

    def add_forms(key: str, forms: List[str]):
        if not key:
            return
        bucket = idx.setdefault(key, set())
        for f in forms:
            if f:
                bucket.add(f)
        bucket.add(key)

    # å®ä½“åº“ original_forms
    try:
        if tools.ENTITIES_FILE.exists():
            with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
                ents = json.load(f)
            for name, data in ents.items():
                forms = data.get("original_forms", []) if isinstance(data, dict) else []
                if isinstance(forms, list):
                    flat = []
                    for x in forms:
                        if isinstance(x, str):
                            flat.append(x)
                        elif isinstance(x, list):
                            flat.extend([str(i) for i in x])
                    add_forms(name, flat)
                else:
                    add_forms(name, [])
    except Exception:
        pass

    # åˆå¹¶è§„åˆ™ï¼ˆalias -> primaryï¼‰ï¼ŒåŒå‘åŠ å…¥
    try:
        merge_rules_file = tools.CONFIG_DIR / "entity_merge_rules.json"
        if merge_rules_file.exists():
            with open(merge_rules_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            rules = data.get("merge_rules", {}) if isinstance(data, dict) else {}
            inv: Dict[str, List[str]] = {}
            for alias, primary in rules.items():
                add_forms(alias, [primary])
                inv.setdefault(primary, []).append(alias)
            for primary, aliases in inv.items():
                add_forms(primary, aliases)
    except Exception:
        pass

    # å±•å¼€ï¼šç¡®ä¿äº’ç›¸åŒ…å«
    for k, forms in list(idx.items()):
        for f in list(forms):
            if f in idx:
                forms.update(idx[f])
    return idx


def _expand_keywords(keywords: List[str]) -> List[List[str]]:
    """
    å°†æ¯ä¸ªå…³é”®è¯æ‰©å±•ä¸ºåŒä¹‰é›†åˆåˆ—è¡¨ï¼ˆæŒ‰è¾“å…¥é¡ºåºä¿ç•™ç»„ï¼‰ã€‚
    """
    idx = _load_entity_equivs()
    groups: List[List[str]] = []
    for kw in keywords:
        kw_norm = (kw or "").strip()
        if not kw_norm:
            continue
        forms = set([kw_norm])
        if kw_norm in idx:
            forms.update(idx[kw_norm])
        groups.append(list(forms))
    return groups


def _build_boolean_query(groups: List[List[str]]) -> str:
    """
    æ ¹æ®åˆ†ç»„æ„é€  (A1 OR A2) AND (B1 OR B2) å½¢å¼çš„æŸ¥è¯¢ã€‚
    """
    clauses = []
    for g in groups:
        ors = []
        for term in g:
            t = str(term).strip()
            if not t:
                continue
            # å»é™¤å†…éƒ¨å¼•å·ï¼Œå¤–å±‚åŠ å¼•å·ä¿è¯çŸ­è¯­/ç‰¹æ®Šå­—ç¬¦å®‰å…¨
            t = t.replace('"', "")
            ors.append(f'"{t}"')
        if ors:
            clauses.append("(" + " OR ".join(ors) + ")")
    return " AND ".join(clauses)


@register_tool(
    name="search_news_by_keywords",
    description="æŒ‰å…³é”®è¯æœç´¢æ–°é—»ï¼ˆå½“å‰ä»… GNewsï¼‰ï¼Œæ”¯æŒå¯é€‰æ—¶é—´èŒƒå›´ä¸æ’åº",
    category="Data Fetch"
)
async def search_news_by_keywords(
    keywords: List[str],
    apis: Optional[List[str]] = None,
    limit: int = 50,
    category: Optional[str] = None,
    from_: Optional[str] = None,
    to: Optional[str] = None,
    nullable: Optional[str] = None,
    truncate: Optional[str] = None,
    sortby: Optional[str] = None,
    in_fields: Optional[str] = None,
    page: Optional[int] = None,
) -> List[Dict[str, Any]]:
    """
    æ ¹æ®å…³é”®è¯åˆ—è¡¨æœç´¢æ–°é—»ï¼ˆGNewsï¼‰ï¼Œè¿”å›åˆå¹¶åçš„ç»“æœã€‚
    """
    tools = Tools()
    if not keywords:
        return []

    # æ„é€  (A OR A2) AND (B OR B2) æŸ¥è¯¢ä¸²
    groups = _expand_keywords(keywords)
    query_str = _build_boolean_query(groups)
    if not query_str:
        return []

    news_collector.init_api_pool()
    if news_collector.API_POOL is None:
        raise RuntimeError("API Pool failed to initialize")

    # å¹¶å‘ä¸Šé™ï¼ˆä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ï¼‰
    config_manager = ConfigManager()
    concurrency = config_manager.get_concurrency_limit("agent2_config")

    # ä½¿ç”¨ç»Ÿä¸€å¼‚æ­¥æ‰§è¡Œå™¨
    async_executor = AsyncExecutor()

    available_sources = news_collector.API_POOL.list_available_sources()
    if apis:
        target_sources = [s for s in apis if s in available_sources]
    else:
        target_sources = available_sources

    if not target_sources:
        tools.log("Warning: No valid sources to search from.")
        return []

    async def search_one(source_name: str) -> List[Dict[str, Any]]:
        try:
            collector = news_collector.API_POOL.get_collector(source_name)
            async with collector:
                news = await collector.search(
                    query=query_str,
                    from_=from_,
                    to=to,
                    limit=limit,
                    in_fields=in_fields,
                    nullable=nullable,
                    sortby=sortby,
                    page=page,
                    truncate=truncate,
                )
                if not news and not query_str:
                    news = await collector.get_top_headlines(
                        category=category,
                        limit=limit,
                        nullable=nullable,
                        from_=from_,
                        to=to,
                        query=None,
                        page=page,
                        truncate=truncate,
                    )
                for item in news:
                    if "source" not in item:
                        item["source"] = source_name
                    if "datetime" in item and hasattr(item["datetime"], "isoformat"):
                        item["datetime"] = item["datetime"].isoformat()
                tools.log(f"Fetched {len(news)} items from {source_name} with query: {query_str}")
                return news
        except Exception as e:
            tools.log(f"Error searching from {source_name}: {e}")
            return []

    # ä½¿ç”¨AsyncExecutorè¿›è¡Œå¹¶å‘æ‰§è¡Œ
    results = await async_executor.run_concurrent_tasks(
        tasks=[lambda src=src: search_one(src) for src in target_sources],
        concurrency=concurrency
    )
    all_news: List[Dict[str, Any]] = []
    for news in results:
        all_news.extend(news)
    all_news.sort(key=lambda x: x.get("datetime") or "", reverse=True)
    return all_news


async def expand_news_by_entities(entities: List[Dict], limit_per_entity: int = 10, time_window_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    æ ¹æ®å®ä½“åˆ—è¡¨æœç´¢ç›¸å…³æ–°é—»ï¼Œæ”¯æŒä½¿ç”¨åŸå§‹è¯è¿›è¡Œæ£€ç´¢

    Args:
        entities: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«nameå’Œoriginal_formså­—æ®µ
        limit_per_entity: æ¯ä¸ªå®ä½“æœç´¢çš„æ–°é—»æ•°é‡é™åˆ¶
        time_window_days: é»˜è®¤æ£€ç´¢æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢ï¼Œå¦‚æœä¸ºTrueåˆ™ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´

    Returns:
        æœç´¢åˆ°çš„ç›¸å…³æ–°é—»åˆ—è¡¨
    """
    expanded_news = []
    news_id_set = set()  # ç”¨äºå»é‡

    # è·å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨
    news_collectors = []
    available_sources = news_collector.API_POOL.list_available_sources()

    # ä¸æ›´æ–°åçš„APIæ± å…¼å®¹ï¼Œç§»é™¤å¯èƒ½çš„å¤‡ç”¨é€»è¾‘
    for source_name in available_sources:
        try:
            collector = news_collector.API_POOL.get_collector(source_name)
            news_collectors.append(collector)
        except Exception as e:
            Tools().log(f"âš ï¸ æ— æ³•åˆ›å»ºæ–°é—»æ”¶é›†å™¨ {source_name}: {e}")

    if not news_collectors:
        Tools().log("âŒ æœªæ‰¾åˆ°å¯ç”¨çš„æ–°é—»æ”¶é›†å™¨")
        return expanded_news

    # ä¸ºæ¯ä¸ªå®ä½“æœç´¢ç›¸å…³æ–°é—»
    for entity in entities:
        entity_name = entity['name']
        original_forms = entity.get('original_forms', [])

        # æ„å»ºä½¿ç”¨ORæ“ä½œç¬¦è¿æ¥çš„æœç´¢æŸ¥è¯¢ï¼šå®ä½“åç§° + æ‰€æœ‰åŸå§‹è¯
        all_terms = [entity_name] + original_forms

        # ç”ŸæˆæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶
        query_batches = []
        current_batch = []
        current_length = 0

        for term in all_terms:
            quoted_term = f'"{term}"'
            term_length = len(quoted_term)

            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªè¯ï¼Œç›´æ¥æ·»åŠ ï¼›å¦åˆ™éœ€è¦è€ƒè™‘ORæ“ä½œç¬¦çš„é•¿åº¦
            if current_batch:
                required_length = current_length + 4 + term_length  # 4æ˜¯" OR "çš„é•¿åº¦
                if required_length > 200:
                    # å¦‚æœæ·»åŠ å½“å‰è¯ä¼šè¶…è¿‡é™åˆ¶ï¼Œä¿å­˜å½“å‰æ‰¹æ¬¡å¹¶å¼€å§‹æ–°æ‰¹æ¬¡
                    query_batches.append(" OR ".join(current_batch))
                    current_batch = [quoted_term]
                    current_length = term_length
                else:
                    current_batch.append(quoted_term)
                    current_length = required_length
            else:
                current_batch.append(quoted_term)
                current_length = term_length

        # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
        if current_batch:
            query_batches.append(" OR ".join(current_batch))

        Tools().log(f"ğŸ” ä¸ºå®ä½“ '{entity_name}' æœç´¢ç›¸å…³æ–°é—»ï¼Œä½¿ç”¨ORæŸ¥è¯¢è¿æ¥ {len(original_forms)} ä¸ªåŸå§‹è¯...")
        Tools().log(f"   ğŸ“ ç”Ÿæˆäº† {len(query_batches)} ä¸ªæŸ¥è¯¢æ‰¹æ¬¡ä»¥é¿å…è¶…è¿‡200å­—ç¬¦é™åˆ¶")

        # è·å–æ—¶é—´èŒƒå›´
        time_ranges = get_time_ranges(time_window_days, full_search)

        for collector in news_collectors:
            try:
                # å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰¹æ¬¡å’Œæ—¶é—´èŒƒå›´è¿›è¡Œæœç´¢
                for time_range in time_ranges:
                    start_date = time_range['start']
                    end_date = time_range['end']

                    Tools().log(f"   ğŸ“… æœç´¢æ—¶é—´èŒƒå›´: {start_date} è‡³ {end_date}")

                    for batch_index, batch_query in enumerate(query_batches):
                        Tools().log(f"   ğŸ“ æ‰§è¡ŒæŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1}/{len(query_batches)}: '{batch_query}'")

                        try:
                            # ä½¿ç”¨æœç´¢åŠŸèƒ½è·å–ç›¸å…³æ–°é—»ï¼Œä¼ å…¥æ—¶é—´èŒƒå›´å‚æ•°
                            search_params = {
                                'keyword' if hasattr(collector, 'search_news_by_keyword') else 'query': batch_query,
                                'limit': limit_per_entity // (len(query_batches) * len(time_ranges)) + 1  # å¹³å‡åˆ†é…é™åˆ¶
                            }

                            # å¦‚æœæ”¶é›†å™¨æ”¯æŒæ—¶é—´èŒƒå›´å‚æ•°ï¼Œåˆ™æ·»åŠ 
                            if hasattr(collector, 'search_news_by_keyword'):
                                if 'start_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search_news_by_keyword.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date
                            elif hasattr(collector, 'search'):
                                if 'start_date' in collector.search.__code__.co_varnames:
                                    search_params['start_date'] = start_date
                                    search_params['end_date'] = end_date
                                elif 'from_date' in collector.search.__code__.co_varnames:
                                    search_params['from_date'] = start_date
                                    search_params['to_date'] = end_date

                            # ä½¿ç”¨æ›´æ–°åçš„APIè°ƒç”¨æ–¹æ³•
                            # ä¼˜å…ˆä½¿ç”¨search_news_by_keywordæ–¹æ³•ï¼Œç¡®ä¿ä¸æ›´æ–°åçš„APIæ± å…¼å®¹
                            if hasattr(collector, 'search_news_by_keyword'):
                                news_list = await collector.search_news_by_keyword(**search_params)
                            elif hasattr(collector, 'search'):
                                news_list = await collector.search(**search_params)
                            else:
                                Tools().log(f"âš ï¸ æ”¶é›†å™¨ {collector.__class__.__name__} æ²¡æœ‰æ”¯æŒçš„æœç´¢æ–¹æ³•")
                                continue

                            # ä¸ºæ¯æ¡æ–°é—»æ·»åŠ å®ä½“æ ‡ç­¾å¹¶å»é‡
                            for news in news_list:
                                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦ç”¨äºå»é‡
                                news_id = f"{news.get('url', '')}_{news.get('publishedAt', '')}"
                                if news_id not in news_id_set:
                                    news_id_set.add(news_id)
                                    news['expanded_from_entity'] = entity_name
                                    news['search_term'] = batch_query  # è®°å½•ä½¿ç”¨çš„æœç´¢è¯
                                    news['source'] = collector.__class__.__name__.replace('Collector', '').lower()
                                    news['query_batch'] = batch_index + 1  # è®°å½•æŸ¥è¯¢æ‰¹æ¬¡
                                    news['search_time_range'] = f"{start_date} to {end_date}"  # è®°å½•æœç´¢æ—¶é—´èŒƒå›´
                                    expanded_news.append(news)
                        except Exception as batch_error:
                            Tools().log(f"âš ï¸ æŸ¥è¯¢æ‰¹æ¬¡ {batch_index + 1} æ‰§è¡Œå¤±è´¥: {batch_error}")
            except Exception as e:
                Tools().log(f"âš ï¸ ä» {collector.__class__.__name__} æœç´¢å¤±è´¥: {e}")

    return expanded_news


def get_time_ranges(default_days: int = 30, full_search: bool = False) -> List[Dict]:
    """
    è·å–æœç´¢çš„æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
    1. æ—¶é—´èŒƒå›´åªèƒ½ä»2020å¹´è‡³ä»Š
    2. é»˜è®¤æ£€ç´¢å‰30å¤©å†…çš„æ–°é—»
    3. å…¨é¢æ£€ç´¢æ—¶ä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´

    Args:
        default_days: é»˜è®¤çš„æ—¶é—´çª—å£å¤©æ•°ï¼Œé»˜è®¤ä¸º30å¤©
        full_search: æ˜¯å¦è¿›è¡Œå…¨é¢æ£€ç´¢

    Returns:
        æ—¶é—´èŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«startå’Œendæ—¥æœŸå­—ç¬¦ä¸²
    """
    time_ranges = []
    now = datetime.now(timezone.utc)

    # å®šä¹‰2020å¹´1æœˆ1æ—¥ä½œä¸ºèµ·å§‹æ—¶é—´
    start_date_2020 = datetime(2020, 1, 1, tzinfo=timezone.utc)

    if not full_search:
        # éå…¨é¢æ£€ç´¢ï¼Œåªè¿”å›é»˜è®¤æ—¶é—´çª—å£
        end_date = now
        start_date = max(start_date_2020, now - timedelta(days=default_days))
        time_ranges.append({
            'start': start_date.strftime('%Y-%m-%d'),
            'end': end_date.strftime('%Y-%m-%d')
        })
    else:
        # å…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æˆ–æ›´å°çš„å¤©æ•°ç›´åˆ°2020å¹´
        Tools().log("ğŸ”„ æ‰§è¡Œå…¨é¢æ£€ç´¢ï¼Œä»å½“å‰æ—¶é—´å‘å‰æ£€ç´¢å¤šä¸ª30å¤©æ‰¹æ¬¡ç›´åˆ°2020å¹´...")

        end_date = now
        batch_count = 0

        while end_date > start_date_2020:
            start_date = max(start_date_2020, end_date - timedelta(days=default_days))

            # ç¡®ä¿ä¸é‡å¤æ·»åŠ ç›¸åŒçš„æ—¶é—´èŒƒå›´
            if not time_ranges or time_ranges[-1]['start'] != start_date.strftime('%Y-%m-%d'):
                time_ranges.append({
                    'start': start_date.strftime('%Y-%m-%d'),
                    'end': end_date.strftime('%Y-%m-%d')
                })

            batch_count += 1
            end_date = start_date - timedelta(days=1)  # é¿å…æ—¥æœŸé‡å 

        Tools().log(f"âœ… ç”Ÿæˆäº† {batch_count} ä¸ªæ—¶é—´èŒƒå›´æ‰¹æ¬¡")

    return time_ranges


def get_recent_entities(time_window_days: int = 30, limit: int = 50) -> List[Dict]:
    """
    è·å–æœ€è¿‘æ—¶é—´çª—å£å†…çš„å®ä½“åˆ—è¡¨ï¼ŒåŒ…å«åŸå§‹è¯ä¿¡æ¯

    Args:
        time_window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        limit: è¿”å›çš„å®ä½“æ•°é‡é™åˆ¶

    Returns:
        æœ€è¿‘çš„å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«åç§°å’ŒåŸå§‹è¯ä¿¡æ¯
    """
    entities = []
    tools = Tools()

    if not tools.ENTITIES_FILE.exists():
        tools.log("âš ï¸ å®ä½“åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return entities

    # è¯»å–å®ä½“åº“
    with open(tools.ENTITIES_FILE, "r", encoding="utf-8") as f:
        entity_data = json.load(f)

    # æ ¹æ® first_seen æ’åºï¼Œè·å–æœ€è¿‘çš„å®ä½“
    sorted_entities = sorted(
        entity_data.items(),
        key=lambda x: x[1].get('first_seen', ''),
        reverse=True
    )

    # è¿‡æ»¤æ—¶é—´çª—å£å†…çš„å®ä½“
    now = datetime.now(timezone.utc)
    time_window = timedelta(days=time_window_days)

    for entity_name, entity_info in sorted_entities:
        first_seen = entity_info.get('first_seen')
        if first_seen:
            try:
                # è§£ææ—¶é—´å­—ç¬¦ä¸²
                if 'T' in first_seen:
                    # ISOæ ¼å¼æ—¶é—´
                    seen_time = datetime.fromisoformat(first_seen.replace('Z', '+00:00'))
                else:
                    # æ™®é€šæ ¼å¼æ—¶é—´
                    seen_time = datetime.strptime(first_seen, '%Y-%m-%d %H:%M:%S')
                    seen_time = seen_time.replace(tzinfo=timezone.utc)

                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
                if now - seen_time <= time_window:
                    entity_info = {
                        'name': entity_name,
                        'original_forms': entity_data[entity_name].get('original_forms', [])
                    }
                    entities.append(entity_info)
                    if len(entities) >= limit:
                        break
            except Exception as e:
                tools.log(f"âš ï¸ è§£æå®ä½“ '{entity_name}' çš„æ—¶é—´æˆ³å¤±è´¥: {e}")

    tools.log(f"âœ… è·å–äº† {len(entities)} ä¸ªæœ€è¿‘å®ä½“")
    return entities


async def process_expanded_news(expanded_news: List[Dict], rate_limit: float = 1.0, max_workers: int = 3) -> int:
    """
    å¤„ç†æ‹“å±•çš„æ–°é—»ï¼Œæå–å®ä½“å’Œäº‹ä»¶

    Args:
        expanded_news: æ‹“å±•çš„æ–°é—»åˆ—è¡¨
        rate_limit: æ¯ç§’é€Ÿç‡é™åˆ¶
        max_workers: æœ€å¤§å¹¶å‘æ•°

    Returns:
        å¤„ç†çš„æ–°é—»æ•°é‡
    """
    processed_count = 0
    tools = Tools()

    # åˆå§‹åŒ–æ–°é—»å»é‡å™¨
    deduplicator = NewsDeduplicator(threshold=tools.get_dedupe_threshold())

    # åˆ›å»ºå»é‡é›†åˆï¼ˆIDå»é‡ï¼‰
    seen_news = set()

    # ä½¿ç”¨ç»Ÿä¸€å¼‚æ­¥æ‰§è¡Œå™¨å’Œé™é€Ÿå™¨
    async_executor = AsyncExecutor()
    limiter = RateLimiter(rate_limit) if rate_limit > 0 else None

    async def handle_one(news: Dict) -> int:
        nonlocal processed_count
        try:
            news_id = news.get('id')
            source = news.get('source', 'unknown')
            if news_id:
                news_key = f"{source}:{news_id}"
                if news_key in seen_news:
                    return 0
                seen_news.add(news_key)

            title = news.get('title', '')
            content = news.get('content', '')
            if not title:
                return 0

            news_text = f"{title} {content}".strip()
            if deduplicator.is_duplicate(news_text):
                return 0

            # åº”ç”¨é™é€Ÿ
            if limiter:
                await limiter.acquire_async()

            loop = asyncio.get_running_loop()
            api_pool = news_collector.API_POOL.get_llm_api_pool() if hasattr(news_collector.API_POOL, 'get_llm_api_pool') else None
            if api_pool is None:
                from ..core import LLMAPIPool
                api_pool = LLMAPIPool()
            extracted = await loop.run_in_executor(None, llm_extract_events, title, content, api_pool)

            if extracted:
                all_entities = []
                for ev in extracted:
                    all_entities.extend(ev['entities'])

                if all_entities:
                    published_at = news.get('datetime')
                    if published_at and isinstance(published_at, datetime):
                        published_at = published_at.isoformat()

                    all_entities_original = all_entities
                    update_entities(all_entities, all_entities_original, source, published_at)
                    update_abstract_map(extracted, source, published_at)
                    return 1
        except Exception as e:
            tools.log(f"âš ï¸ å¤„ç†æ‹“å±•æ–°é—»å¤±è´¥: {e}")
        return 0

    tasks = [handle_one(news) for news in expanded_news]
    # ä½¿ç”¨AsyncExecutorè¿›è¡Œå¹¶å‘æ‰§è¡Œ
    results = await async_executor.run_concurrent_tasks(
        tasks=tasks,
        concurrency=max_workers
    )
    processed_count = sum(results)

    return processed_count


@register_tool(
    name="expand_news_by_recent_entities",
    description="[å·¥ä½œæµ] æ ¹æ®æœ€è¿‘å®ä½“æœç´¢ç›¸å…³æ–°é—»å¹¶æå–äº‹ä»¶",
    category="Workflow"
)
async def expand_news_by_recent_entities(
    entity_limit: int = 1,
    time_window_days: int = 30,
    limit_per_entity: int = 120,
    full_search: bool = False,
    rate_limit: float = 1.0,
    max_workers: int = 3
) -> Dict[str, Any]:
    """
    æ ¹æ®æœ€è¿‘å®ä½“æœç´¢ç›¸å…³æ–°é—»çš„å·¥ä½œæµ

    Args:
        entity_limit: å®ä½“æ•°é‡é™åˆ¶
        time_window_days: æ—¶é—´çª—å£ï¼ˆå¤©ï¼‰
        limit_per_entity: æ¯ä¸ªå®ä½“æœç´¢æ–°é—»æ•°é‡
        full_search: æ˜¯å¦å…¨é¢æ£€ç´¢
        rate_limit: é€Ÿç‡é™åˆ¶
        max_workers: æœ€å¤§å¹¶å‘æ•°

    Returns:
        å¤„ç†ç»“æœç»Ÿè®¡
    """
    tools = Tools()
    processed_ids = set()
    if tools.PROCESSED_IDS_FILE.exists():
        with open(tools.PROCESSED_IDS_FILE, "r", encoding="utf-8", errors="ignore") as f:
            processed_ids = set(line.strip() for line in f if line.strip())

    # è·å–æœ€è¿‘å®ä½“
    recent_entities = get_recent_entities(time_window_days=time_window_days, limit=entity_limit)
    if not recent_entities:
        tools.log("ğŸ“­ æ²¡æœ‰å¯ç”¨çš„å®ä½“è¿›è¡Œæ–°é—»æ‹“å±•")
        return {"processed_count": 0, "expanded_news_count": 0}

    # æœç´¢ç›¸å…³æ–°é—»
    tools.log(f"ğŸ” å¼€å§‹æœç´¢ {len(recent_entities)} ä¸ªå®ä½“çš„ç›¸å…³æ–°é—»...")
    expanded_news = await expand_news_by_entities(
        recent_entities,
        limit_per_entity=limit_per_entity,
        time_window_days=time_window_days,
        full_search=full_search
    )
    tools.log(f"âœ… å…±æœç´¢åˆ° {len(expanded_news)} æ¡ç›¸å…³æ–°é—»")

    # å¤„ç†æœç´¢åˆ°çš„æ–°é—»
    if expanded_news:
        deduped_path = persist_expanded_news_to_tmp(expanded_news, processed_ids)
        processed_count = 0
        if deduped_path and deduped_path.exists():
            tools.log(f"ğŸ“„ å¼€å§‹å¤„ç†æ‹“å±•çš„æ–°é—» (deduped: {deduped_path.name}) ...")
            news_list = []
            with open(deduped_path, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        news_list.append(json.loads(line))
                    except Exception as e:
                        tools.log(f"âš ï¸ è·³è¿‡æ— æ•ˆè¡Œ: {e}")
            processed_count = await process_expanded_news(news_list, rate_limit=rate_limit, max_workers=max_workers)
            # æ¸…ç† tmp æ–‡ä»¶
            try:
                raw_file = tools.RAW_NEWS_TMP_DIR / deduped_path.name.replace("_deduped", "")
                file_paths = [raw_file, deduped_path]
                safe_unlink_multiple(file_paths, "ä¸´æ—¶")
            except Exception as e:
                tools.log(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        tools.log(f"âœ… æˆåŠŸå¤„ç† {processed_count} æ¡æ‹“å±•æ–°é—»")
        return {"processed_count": processed_count, "expanded_news_count": len(expanded_news)}

    return {"processed_count": 0, "expanded_news_count": 0}

